"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4277],{3905:function(e,t,r){r.d(t,{Zo:function(){return u},kt:function(){return d}});var a=r(7294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function l(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var s=a.createContext({}),p=function(e){var t=a.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},u=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),m=p(r),d=n,k=m["".concat(s,".").concat(d)]||m[d]||c[d]||o;return r?a.createElement(k,i(i({ref:t},u),{},{components:r})):a.createElement(k,i({ref:t},u))}));function d(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,i=new Array(o);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:n,i[1]=l;for(var p=2;p<o;p++)i[p]=r[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,r)}m.displayName="MDXCreateElement"},5063:function(e,t,r){r.r(t),r.d(t,{assets:function(){return u},contentTitle:function(){return s},default:function(){return d},frontMatter:function(){return l},metadata:function(){return p},toc:function(){return c}});var a=r(7462),n=r(3366),o=(r(7294),r(3905)),i=["components"],l={sidebar_position:2},s="Ray",p={unversionedId:"shakudo-platform-features/api/ray",id:"shakudo-platform-features/api/ray",title:"Ray",description:"Ray is an open source project that distributed frameworks that has a more support for deep learning and reinforcement learning. It has a rich set of libraries and integrations built on a flexible distributed execution framework, is ideal choice for parallelizing model training and hyper-parameter tuning.",source:"@site/docs/shakudo-platform-features/api/ray.md",sourceDirName:"shakudo-platform-features/api",slug:"/shakudo-platform-features/api/ray",permalink:"/shakudo-platform-features/api/ray",draft:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Dask",permalink:"/shakudo-platform-features/api/dask"},next:{title:"Pipeline Jobs",permalink:"/shakudo-platform-features/api/pipelinejobs"}},u={},c=[{value:"Ray_Common",id:"ray_common",level:2},{value:"<code>quickstart_ray()</code>",id:"quickstart_ray",level:2},{value:"<code>initialize_ray_cluster()</code>",id:"initialize_ray_cluster",level:2},{value:"<code>stop_ray_cluster()</code>",id:"stop_ray_cluster",level:2},{value:"<code>get_ray_cluster()</code>",id:"get_ray_cluster",level:2},{value:"<code>find_ray_workers</code>",id:"find_ray_workers",level:2}],m={toc:c};function d(e){var t=e.components,r=(0,n.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},m,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"ray"},"Ray"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://ray.io/"},"Ray")," is an open source project that distributed frameworks that has a more support for deep learning and reinforcement learning. It has a rich set of libraries and integrations built on a flexible distributed execution framework, is ideal choice for parallelizing model training and hyper-parameter tuning."),(0,o.kt)("h2",{id:"ray_common"},"Ray_Common"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"ray_common")," is part of the Shakudo Platform Hyperplane API that contains convenience functions to manage Ray clusters. We support extensions to the basic Ray framework by supporting Ray Tune, Ray Spark, Ray with RAPIDS, and more."),(0,o.kt)("h2",{id:"quickstart_ray"},(0,o.kt)("inlineCode",{parentName:"h2"},"quickstart_ray()")),(0,o.kt)("p",null,"Use ",(0,o.kt)("inlineCode",{parentName:"p"},"quickstart_ray")," to quickly spin up a Ray cluster using ",(0,o.kt)("a",{parentName:"p",href:"/shakudo-platform-features/api/dask/#worker-pools"},"t-shirt sizes")," (Sizes are the same as quick start for Dask clusters)."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from hyperplane import ray_common as rc\nray_cluster = rc.quickstart_ray(\n    num_workers = 4, \n    size = 'hyperplane-med-high-mem'\n)\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Parameters")),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:"left"},"Name"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Type"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"num_workers   ",(0,o.kt)("div",{class:"label basic required"},"Required")),(0,o.kt)("td",{parentName:"tr",align:"left"},"integer"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Number of workers")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"size          ",(0,o.kt)("div",{class:"label basic required"},"Required")),(0,o.kt)("td",{parentName:"tr",align:"left"},"object"),(0,o.kt)("td",{parentName:"tr",align:"left"},(0,o.kt)("a",{parentName:"td",href:"#worker-pools"},"Pre-configured worker pools"))))),(0,o.kt)("hr",null),(0,o.kt)("a",{name:"initialize-ray-cluster"}),(0,o.kt)("h2",{id:"initialize_ray_cluster"},(0,o.kt)("inlineCode",{parentName:"h2"},"initialize_ray_cluster()")),(0,o.kt)("p",null,"Initialize a distributed Ray cluster with ease and more customizability. You can also run this function to clean up the Ray nodes and re-initialize."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from hyperplane import ray_common as rc\nray_cluster = rc.initialize_ray_cluster(\n    num_workers = 4, \n    cpu_core_per_worker = 4, \n    ram_gb_per_worker = 4, \n    n_gpus = 0\n    )\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"}," Parameters ")),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:"left"},"Name"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Type"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"num_workers"),(0,o.kt)("td",{parentName:"tr",align:"left"},"integer"),(0,o.kt)("td",{parentName:"tr",align:"left"},"(Default value: 2) Number of Ray nodes to be initialized")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"cpu_core_per_worker"),(0,o.kt)("td",{parentName:"tr",align:"left"},"integer"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Number of CPU cores in each Ray node")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"ram_gb_per_worker"),(0,o.kt)("td",{parentName:"tr",align:"left"},"float"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Memory size in GB for each Ray node")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"n_gpus"),(0,o.kt)("td",{parentName:"tr",align:"left"},"integer"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Number of Nvidia GPUs in each Ray node (if ",(0,o.kt)("inlineCode",{parentName:"td"},"n_gpus > 0"),", ",(0,o.kt)("inlineCode",{parentName:"td"},"cpu_core_per_worker")," and ",(0,o.kt)("inlineCode",{parentName:"td"},"ram_gb_per_worker")," are ignored)")),(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"use_existing"),(0,o.kt)("td",{parentName:"tr",align:"left"},"boolean"),(0,o.kt)("td",{parentName:"tr",align:"left"},"(Default: ",(0,o.kt)("inlineCode",{parentName:"td"},"use_existing = False"),") Whether to connect to/ reinitialize existing Ray cluster or spin up a new one")))),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"If you are aiming for a specific pool, ensure your ",(0,o.kt)("inlineCode",{parentName:"p"},"cpu_core_per_worker")," = the number of allocatable cores and ",(0,o.kt)("inlineCode",{parentName:"p"},"ram_gb_per_worker")," = the allocatable ram. For example, if you would like to use a POOL_16_16 worker, you may want to use the following cluster initalization"),(0,o.kt)("pre",{parentName:"div"},(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from hyperplane import ray_common as rc\nray_cluster = rc.initialize_ray_cluster(\n    num_workers = 4, \n    cpu_core_per_worker = 15, \n    ram_gb_per_worker = 12\n    )\n")))),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"stop_ray_cluster"},(0,o.kt)("inlineCode",{parentName:"h2"},"stop_ray_cluster()")),(0,o.kt)("p",null,"Use ",(0,o.kt)("inlineCode",{parentName:"p"},"stop_ray_cluster")," to shutdown a Ray cluster. After computation is finished, it's a good idea to shutdown the distributed cluster and release the resources back to the node pool. If any Ray nodes are left hanging, Shakudo Platform's garbage collection function will also automatically shutdown the Ray workers when the Session or job is finished. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from hyperplane import ray_common as rc\nrc.stop_ray_cluster(ray_cluster)\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"}," Parameters ")),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:"left"},"Name"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Type"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"ray_cluster ",(0,o.kt)("div",{class:"label basic required"},"Required")),(0,o.kt)("td",{parentName:"tr",align:"left"},"object"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Ray cluster to shutdown")))),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"get_ray_cluster"},(0,o.kt)("inlineCode",{parentName:"h2"},"get_ray_cluster()")),(0,o.kt)("p",null,"Reconnect to a Ray cluster by using the ",(0,o.kt)("inlineCode",{parentName:"p"},"get_ray_cluster")," to retrieve the cluster. You can use this function if you've already spun up a Ray cluster and want to connect to the same cluster (for example: in another notebook in the same session). This function will connect to an existing cluster. There are two ways to reconnect to Ray clusters.  "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from hyperplane import ray_common as rc\nrc.get_ray_cluster(extra_workers = 1)\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"}," Parameters ")),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:"left"},"Name"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Type"),(0,o.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:"left"},"extra_workers"),(0,o.kt)("td",{parentName:"tr",align:"left"},"integer"),(0,o.kt)("td",{parentName:"tr",align:"left"},"Adds nodes to your existing cluster (Default: ",(0,o.kt)("inlineCode",{parentName:"td"},"extra_workers = 0"),") The nodes that are added to the cluster will be of the same specification as the original cluster.")))),(0,o.kt)("p",null,"There are two ways to reconnect to Ray clusters. The method using the function ",(0,o.kt)("inlineCode",{parentName:"p"},"get_ray_cluster()")," is the simpler and recommended way. "),(0,o.kt)("p",null,"You can also use the ",(0,o.kt)("inlineCode",{parentName:"p"},"initialize_ray_cluster()")," to accomplish the same. Note, the arguments for ",(0,o.kt)("inlineCode",{parentName:"p"},"cpu_core_per_worker")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"ram_gb_per_worker")," must be the same as when you initialized the cluster originally."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from hyperplane import ray_common as rc\nray_cluster = rc.initialize_ray_cluster(\n    num_workers = 0, \n    cpu_core_per_worker = 15, \n    ram_gb_per_worker = 12,\n    use_existing = True\n    )\n")),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"find_ray_workers"},(0,o.kt)("inlineCode",{parentName:"h2"},"find_ray_workers")),(0,o.kt)("p",null,"Use ",(0,o.kt)("inlineCode",{parentName:"p"},"find_ray_workers()")," function to see if there are any Ray workers already spun up. Returns a list of Ray workers running."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from hyperplane import ray_common as rc\nrc.find_ray_workers()\n")))}d.isMDXComponent=!0}}]);