"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3710],{3905:function(e,t,o){o.d(t,{Zo:function(){return u},kt:function(){return h}});var n=o(7294);function a(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function r(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,n)}return o}function i(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?r(Object(o),!0).forEach((function(t){a(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):r(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function s(e,t){if(null==e)return{};var o,n,a=function(e,t){if(null==e)return{};var o,n,a={},r=Object.keys(e);for(n=0;n<r.length;n++)o=r[n],t.indexOf(o)>=0||(a[o]=e[o]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)o=r[n],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(a[o]=e[o])}return a}var l=n.createContext({}),p=function(e){var t=n.useContext(l),o=t;return e&&(o="function"==typeof e?e(t):i(i({},t),e)),o},u=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var o=e.components,a=e.mdxType,r=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),c=p(o),h=a,m=c["".concat(l,".").concat(h)]||c[h]||d[h]||r;return o?n.createElement(m,i(i({ref:t},u),{},{components:o})):n.createElement(m,i({ref:t},u))}));function h(e,t){var o=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=o.length,i=new Array(r);i[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,i[1]=s;for(var p=2;p<r;p++)i[p]=o[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,o)}c.displayName="MDXCreateElement"},5693:function(e,t,o){o.r(t),o.d(t,{assets:function(){return u},contentTitle:function(){return l},default:function(){return h},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return d}});var n=o(7462),a=o(3366),r=(o(7294),o(3905)),i=["components"],s={sidebar_position:1},l="Build & Deploy in 5min",p={unversionedId:"tutorials/getstarted",id:"tutorials/getstarted",title:"Build & Deploy in 5min",description:"Shakudo Platform enables you to build pipelines from VSCode notebooks, python files, or Jupyter notebooks.",source:"@site/docs/tutorials/getstarted.md",sourceDirName:"tutorials",slug:"/tutorials/getstarted",permalink:"/tutorials/getstarted",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Tutorial",permalink:"/category/tutorial"},next:{title:"Create a React App",permalink:"/tutorials/buildareactapp"}},u={},d=[{value:"1. Start a Session",id:"1-start-a-session",level:2},{value:"2. Open a notebook",id:"2-open-a-notebook",level:2},{value:"3. Process data with Pandas",id:"3-process-data-with-pandas",level:2},{value:"4. Spin up a Dask Cluster",id:"4-spin-up-a-dask-cluster",level:2},{value:"5. Process data in Dask",id:"5-process-data-in-dask",level:2},{value:"6. Automate the job with Shakudo Platform Jobs",id:"6-automate-the-job-with-shakudo-platform-jobs",level:2},{value:"7. Productionize the notebook by launching a job on Shakudo Platform",id:"7-productionize-the-notebook-by-launching-a-job-on-shakudo-platform",level:2},{value:"8. Additional Steps",id:"8-additional-steps",level:2}],c={toc:d};function h(e){var t=e.components,s=(0,a.Z)(e,i);return(0,r.kt)("wrapper",(0,n.Z)({},c,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"build--deploy-in-5min"},"Build & Deploy in 5min"),(0,r.kt)("p",null,"Shakudo Platform enables you to build pipelines from VSCode notebooks, python files, or Jupyter notebooks."),(0,r.kt)("p",null,"In this tutorial we'll walk you through an end to end example of going from development to deployment of a simple data processing pipeline."),(0,r.kt)("h2",{id:"1-start-a-session"},"1. Start a Session"),(0,r.kt)("p",null,"Start a Session by navigating to Sessions tab on the dashboard and clicking on the ",(0,r.kt)("strong",{parentName:"p"},"+")," button.\nYou will see a popup to Start a Session. Choose ",(0,r.kt)("inlineCode",{parentName:"p"},"Basic")," from the Session Type drop down. For more information on Session Types and other configurations check out the ",(0,r.kt)("a",{parentName:"p",href:"/shakudo-platform-features/sessions"},"Guide on Sessions")),(0,r.kt)("h2",{id:"2-open-a-notebook"},"2. Open a notebook"),(0,r.kt)("p",null,"Once the Session is ready you'll see an expand icon click on it to open a Jupter lab browser Session."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"launchsession",src:o(6504).Z,width:"987",height:"447"})),(0,r.kt)("p",null,"In your browser Session choose the Python 3 Notebook option."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"pythonnotebook",src:o(6797).Z,width:"640",height:"290"})),(0,r.kt)("h2",{id:"3-process-data-with-pandas"},"3. Process data with Pandas"),(0,r.kt)("p",null,"The example we are going to use will use be a ",(0,r.kt)("strong",{parentName:"p"},"group-by")," task on a public flight dataset in AWS S3 bucket. The dataset has 22 CSV files and is 11GB in total."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import pandas as pd\ndf_pd = pd.read_csv(\"s3://dask-data/airline-data/1990.csv\", \n          usecols = ['DepTime','FlightNum','DepDelay','Origin', 'Dest','Distance'])\ndf_sort_pd = df_pd.groupby('Origin').apply(lambda x : x.nlargest(n = 10, columns = 'Distance'))\n")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"1990.csv")," data shape is ",(0,r.kt)("inlineCode",{parentName:"p"},"(5270893, 6)")," and the processing time of the above operation is 21s on a 16 CPU 16G RAM machine. Now we want to scale up the above computation for ",(0,r.kt)("strong",{parentName:"p"},"10")," CSV files for 1990 to 1999. On one 16 CPU 16G RAM machine it will cause an ",(0,r.kt)("inlineCode",{parentName:"p"},"out of memory error")," and crash the kernel. To process the files one by one, in total the operation is estimated to take ",(0,r.kt)("strong",{parentName:"p"},"3.5 minutes"),". As a workaround and to speed up computation we'll be using Dask."),(0,r.kt)("h2",{id:"4-spin-up-a-dask-cluster"},"4. Spin up a Dask Cluster"),(0,r.kt)("p",null,"Now let's spin up a distributed Dask cluster and speed up the computation!"),(0,r.kt)("p",null,"The easiest way to get started is by using the ",(0,r.kt)("a",{parentName:"p",href:"/shakudo-platform-features/api/dask"},(0,r.kt)("inlineCode",{parentName:"a"},"notebook_common"))," function to spin up a pre-configured Dask cluster. You can specify the number of workers with argument ",(0,r.kt)("inlineCode",{parentName:"p"},"num_workers")," or specify more specs to better fit your computation. Shakudo Platform will automatically choose a cluster configuration for you and provides a Dask dashboard link to monitor progress."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from hyperplane import notebook_common as nc\nclient, cluster = nc.initialize_cluster(\n        nprocs=5,\n        nthreads=3,\n        ram_gb_per_proc=2.4,\n        cores_per_worker=15,\n        scheduler_deploy_mode="remote",\n        num_workers = 3\n    )\n')),(0,r.kt)("p",null,"You will be able to see the spinning up logs of the Dask cluster and the link to the Dask dashboard."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"\ud83d\udc49 Shakudo Platform: selecting worker node pool\n\ud83d\udc49 Shakudo Platform: selecting scheduler node pool\nCreating scheduler pod on cluster. This may take some time.\n\ud83d\udc49 Shakudo Platform: spinning up a dask cluster with a scheduler as a standalone container.\n\ud83d\udc49 Shakudo Platform: In a few minutes you'll be able to access the dashboard at https://ds.hyperplane.dev/dask-cluster-e002f3d0-b18d-4027-81c5-bed613eb63a4/status\n\ud83d\udc49 Shakudo Platform: to get logs from all workers, do `cluster.get_logs()`\n")),(0,r.kt)("h2",{id:"5-process-data-in-dask"},"5. Process data in Dask"),(0,r.kt)("p",null,"To run the code from step 3 on a Dask cluster, we just need to swap the Pandas API to Dask API, which is very similar. Dask does lazy computation, the last line ",(0,r.kt)("inlineCode",{parentName:"p"},"df_sort.compute")," triggers the computation. You can find information on the Dask concepts and Dask best practices page. Check out the ",(0,r.kt)("a",{parentName:"p",href:"https://docs.dask.org/"},"Dask offcial documentation")," for more."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"df = dd.read_csv(data_url, \n                 storage_options = {'anon': True},\n                 usecols = ['DepTime','FlightNum','DepDelay','Origin', 'Dest','Distance'],\n                 dtype={'Distance': 'float64',\n                    'DepTime':'float64',\n                    'FlightNum':'int64',\n                    'DepDelay':'float64',\n                    'Dest':'object',\n                    }, \n                encoding = \"ISO-8859-1\")\ndf_sort = df.groupby('Origin').apply(lambda x : x.nlargest(n = 10, columns = 'Distance'))\ndf_sort.compute()\n")),(0,r.kt)("p",null,"The above Dask operation took ",(0,r.kt)("strong",{parentName:"p"},"18.8 seconds")," using 3 remote 16 CPU 16G RAM Dask nodes."),(0,r.kt)("p",null,"After using Dask it's good practice to close the cluster atfer use. Add the line below at the end of your notebook:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"cluster.close()\n")),(0,r.kt)("h2",{id:"6-automate-the-job-with-shakudo-platform-jobs"},"6. Automate the job with Shakudo Platform Jobs"),(0,r.kt)("p",null,"Now the data processing notebook is developed and tested, to automatically run this notebook on a schedule as in most production setups, we can simply add a pipeline.yaml file to build a pipeline. To read more on pipeline YAML files please visit the ",(0,r.kt)("a",{parentName:"p",href:"/shakudo-platform-features/jobs/#getstartedwithjobs"},"create a pipeline job page"),"."),(0,r.kt)("p",null,"Open a text file on your browser Session by clicking on the blue ",(0,r.kt)("strong",{parentName:"p"},"+")," button on the top left of the side bar. "),(0,r.kt)("p",null,"Copy and paste the yaml below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'pipeline:\n  name: "data prep pipeline"\n  tasks:\n  - name: "Dask groupby data"\n    type: "jupyter notebook"\n    notebook_path: "pipeline_job/dask_group_sort.ipynb" #Edit this path to where the notebook is stored\n    notebook_output_path: "dask_group_sort_output.ipynb" \n')),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"Once you are happy with the pipeline yaml and the notebook, make sure to commit and push them to the synced git repo. "))),(0,r.kt)("h2",{id:"7-productionize-the-notebook-by-launching-a-job-on-shakudo-platform"},"7. Productionize the notebook by launching a job on Shakudo Platform"),(0,r.kt)("p",null,"Now we are one step away to put the job in production! To launch a pipeline job, we can go to the Shakudo Platform dashboard's Jobs tab and click ",(0,r.kt)("strong",{parentName:"p"},"Create"),"."),(0,r.kt)("p",null,"Once the job creation dialogue pops up we paste in the path to where the pipeline .yaml file is located in the corresponding Pipeline YAML Path field. "),(0,r.kt)("p",null,"Click the ",(0,r.kt)("strong",{parentName:"p"},"CREATE")," button on the top right corner."),(0,r.kt)("p",null,"Once the job is created, we are officially alive in production!"),(0,r.kt)("h2",{id:"8-additional-steps"},"8. Additional Steps"),(0,r.kt)("p",null,"Shakudo Platform offers a variety of other functionalities for more advanced workflows. Some additional uses include the following:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"/shakudo-platform-features/jobs/#scheduleajob"},"Run pipelines on a schedule"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"/shakudo-platform-features/jobs/#triggerajob"},"Submitting/ triggering pipeline jobs")))))}h.isMDXComponent=!0},6504:function(e,t,o){t.Z=o.p+"assets/images/launchsession-96df24d359128f33770e23611d4db8aa.png"},6797:function(e,t,o){t.Z=o.p+"assets/images/pythonnotebook-bfed9a602c362567217bf0a670221df2.png"}}]);