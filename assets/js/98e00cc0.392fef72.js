"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[142],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return h}});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},s=Object.keys(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,s=e.originalType,l=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),c=p(a),h=o,m=c["".concat(l,".").concat(h)]||c[h]||u[h]||s;return a?n.createElement(m,r(r({ref:t},d),{},{components:a})):n.createElement(m,r({ref:t},d))}));function h(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var s=a.length,r=new Array(s);r[0]=c;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:o,r[1]=i;for(var p=2;p<s;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},8862:function(e,t,a){a.r(t),a.d(t,{assets:function(){return d},contentTitle:function(){return l},default:function(){return h},frontMatter:function(){return i},metadata:function(){return p},toc:function(){return u}});var n=a(7462),o=a(3366),s=(a(7294),a(3905)),r=["components"],i={sidebar_position:3},l="Quick Start",p={unversionedId:"legacy/quickstart",id:"legacy/quickstart",title:"Quick Start",description:"Shakudo Platform enables you to develop in a pre-config environments and deploy without docker built. The development environment are accessible through Jupyter notebooks, Code-server and local IDE like VSCode.",source:"@site/docs/legacy/quickstart.md",sourceDirName:"legacy",slug:"/legacy/quickstart",permalink:"/legacy/quickstart",draft:!1,tags:[],version:"current",lastUpdatedBy:"YiranWang",lastUpdatedAt:1758666396,formattedLastUpdatedAt:"9/23/2025",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Shakudo API",permalink:"/legacy/shakudoApi"},next:{title:"FAQ",permalink:"/legacy/faq"}},d={},u=[{value:"1. Start a Session",id:"1-start-a-session",level:2},{value:"2. Access the Session",id:"2-access-the-session",level:2},{value:"3. Process with Pandas",id:"3-process-with-pandas",level:2},{value:"4. Spin up a Dask cluster",id:"4-spin-up-a-dask-cluster",level:2},{value:"5. Process data in Dask",id:"5-process-data-in-dask",level:2},{value:"6. Creating the YAML file for deployment",id:"6-creating-the-yaml-file-for-deployment",level:2},{value:"7. Deploy to a pipeline job",id:"7-deploy-to-a-pipeline-job",level:2},{value:"8. Check job status",id:"8-check-job-status",level:2},{value:"9. Additional Steps",id:"9-additional-steps",level:2}],c={toc:u};function h(e){var t=e.components,i=(0,o.Z)(e,r);return(0,s.kt)("wrapper",(0,n.Z)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"quick-start"},"Quick Start"),(0,s.kt)("p",null,"Shakudo Platform enables you to develop in a pre-config environments and deploy without docker built. The development environment are accessible through Jupyter notebooks, Code-server and local IDE like VSCode. "),(0,s.kt)("p",null,"In this tutorial we'll walk you through a basic example of going from development to deployment of a simple data processing pipeline. "),(0,s.kt)("h2",{id:"1-start-a-session"},"1. Start a Session"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"/shakudo-platform-core/sessions/"},"Sessions")," are pre-configured virtual machines (VMs) with connections to all the tools and resources necessary for development. To begin development, navigate to the Sessions tab on the dashboard and click the ",(0,s.kt)("strong",{parentName:"p"},"+"),"Start a Session button. You will see a dialog window to start a session like the image below."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"start_session",src:a(8143).Z,width:"1306",height:"1012"})),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Image"),": Choose the image type to use in the session. To add new image to the dropdown permanently, you can add a new ",(0,s.kt)("inlineCode",{parentName:"p"},"Environment Config")," under the ",(0,s.kt)("inlineCode",{parentName:"p"},"admin settings"),". Please checkout ",(0,s.kt)("a",{parentName:"p",href:"/shakudo-platform-core/adminSettings/podspecs"},"EC")," for more details. In this example, we are going to use the ",(0,s.kt)("inlineCode",{parentName:"p"},"Basic")," image in the dropdown. For more information on Session Types and other configurations check out the ",(0,s.kt)("a",{parentName:"p",href:"/shakudo-platform-core/sessions/"},"Guide on Sessions"))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"ImageURL"),": You can paste any image URL in the Image Url field. This will overwrite the ",(0,s.kt)("inlineCode",{parentName:"p"},"Image")," above field that we have chosen and use the ImageURL instead. This is useful for quick testing, we need to make sure the image registry credentials are added to the shakudo environment. This is usually setup at installation time. If you'd like to add more image registry access, please contact your Kubernets cluster admin or Shakudo support.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Timeout"),": Choose the idle timeout for the session. Idle timeout is defined as the number of seconds from which the session has been continuously idling. It's default to 15 minutes. ")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},(0,s.kt)("strong",{parentName:"p"},"Drive"),": Drive is the persistent volume that this session will use. ",(0,s.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/storage/persistent-volumes/"},"Persistent volumes")," is a Kubernetes term, imaging it as a hard drive in a laptop. You can have multiple drives and manage your drives by clicking on the icon to the right of the ",(0,s.kt)("inlineCode",{parentName:"p"},"Drive")," field.   "))),(0,s.kt)("h2",{id:"2-access-the-session"},"2. Access the Session"),(0,s.kt)("p",null,"Once the Session is ready, you'll see a Jupyterlab icon and a SSH icon. If your image has CodeServer, you will also see a VsCode icon. To see the different ways of accessing the session, checkout the ",(0,s.kt)("a",{parentName:"p",href:"/shakudo-platform-core/sessions/"},"Guide on Sessions"),". In this example, we'll use the Jupyterlab option."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"access_session",src:a(9311).Z,width:"2356",height:"240"})),(0,s.kt)("h2",{id:"3-process-with-pandas"},"3. Process with Pandas"),(0,s.kt)("p",null,"The dataset we are using is the ",(0,s.kt)("a",{parentName:"p",href:"https://www.kaggle.com/datasets/wenxingdi/data-expo-2009-airline-on-time-data"},"flight dataset")," in a public AWS S3 bucket. The dataset has 22 CSV files and has a total size of 11 GB.\nThe objective is to identify the airport with the most frequently delayed arriving flights. This can be achieved through a few simple Pandas DataFrame transformations. Your session should already be set up to connect to your internal data source if this is a local storage bucket."),(0,s.kt)("p",null,"Let's first use the ",(0,s.kt)("a",{parentName:"p",href:"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"},(0,s.kt)("inlineCode",{parentName:"a"},"groupby"))," and ",(0,s.kt)("a",{parentName:"p",href:"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nlargest.html"},(0,s.kt)("inlineCode",{parentName:"a"},"nlargest"))," function in Pandas to benchmark. "),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"## loop through 22 files and groupby one by one, on the 16vCPU 16 GB RAM node that the session is on  \n\nfrom tqdm.notebook import tqdm\nresults = []\nfor file in tqdm(files):\n    df = pd.read_csv(\n        f\"s3://{file}\", \n        encoding = \"ISO-8859-1\",\n        usecols = ['DepTime','FlightNum','DepDelay','Origin', 'Dest','Distance']\n                    )\n    df['isDelayed'] = df.DepDelay.fillna(16) > 15 \n    df_delayed = df.groupby(['Origin','Dest']).isDelayed.mean()\n    results.append(df_delayed)\n    \ndf_results = pd.concat(results).reset_index()\ndf_results = df_results.groupby(['Origin','Dest']).mean()\ndf_results = df_results[df_results.isDelayed==1].reset_index()\nprint(f'found {len(df_results)} most delayed flights Origin and destination pairs')\n")),(0,s.kt)("p",null,"The processing time of the above operation is 6 minutes and 26 seconds on a 16 vCPU 16GB RAM machine. There are many ways to speed things up, here we use distributed Dask."),(0,s.kt)("h2",{id:"4-spin-up-a-dask-cluster"},"4. Spin up a Dask cluster"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://www.dask.org/"},"Dask")," is a powerful distributed computing framework and can scale from multi-core local machine to large distributed clusters. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, Scikit-learn and NumPy, leading to a shallow learning curve. "),(0,s.kt)("p",null,"We can use the Shakudo package ",(0,s.kt)("a",{parentName:"p",href:"/Shakudo-stack/distributedComputing/dask"},(0,s.kt)("inlineCode",{parentName:"a"},"notebook_common"))," to spin up a fully configured Dask cluster with ",(0,s.kt)("a",{parentName:"p",href:"https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms"},"preemptible nodes"),". You can specify the number of workers with argument ",(0,s.kt)("inlineCode",{parentName:"p"},"num_workers")," or specify more specs to better fit the computation. Shakudo will automatically choose a cluster configuration for you and provides a Dask dashboard link to monitor progress."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"from hyperplane import notebook_common as nc\nnum_workers = 2   ## number of worker nodes \ntotal_memory = 12   ## total memory size for the worker nodes in GB \ncors_per_worker = 15  ## total number of cores for the worker nodes\nnprocs = 3  ## number of processes for each worker node\nram_gb_per_proc = total_memory/nprocs  ## calculated memory size per processes in GB \nnthreads = int(cors_per_worker/nprocs) ## calculated number of threads per processes\n\nclient, cluster = nc.initialize_cluster(\n        num_workers = num_workers,\n        nprocs = nprocs,\n        nthreads = nthreads,\n        ram_gb_per_proc = ram_gb_per_proc,\n        cores_per_worker = cors_per_worker,\n        node_selector = {}\n    )\n")),(0,s.kt)("p",null,"You will be able to see the spinning up logs of the Dask cluster and the link to the Dask dashboard."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"\ud83d\udc49 Shakudo Platform: selecting worker node pool\n\ud83d\udc49 Shakudo Platform: selecting scheduler node pool\nCreating scheduler pod on cluster. This may take some time.\n\ud83d\udc49 Shakudo Platform: spinning up a dask cluster with a scheduler as a standalone container.\n\ud83d\udc49 Shakudo Platform: In a few minutes you'll be able to access the dashboard at https://ds.hyperplane.dev/dask-cluster-e002f3d0-b18d-4027-81c5-bed613eb63a4/status\n\ud83d\udc49 Shakudo Platform: to get logs from all workers, do `cluster.get_logs()`\n")),(0,s.kt)("p",null,"By clicking on the link above you'll see the unique distributed Dask dashboard. "),(0,s.kt)("h2",{id:"5-process-data-in-dask"},"5. Process data in Dask"),(0,s.kt)("p",null,"To run the code from step 3 on a Dask cluster, we just need to swap the Pandas API to the Dask API. Dask does lazy computation, the last line ",(0,s.kt)("inlineCode",{parentName:"p"},".compute()")," function triggers the actual computation. You can find information on the Dask concepts and Dask best practices page. Check out the ",(0,s.kt)("a",{parentName:"p",href:"https://docs.dask.org/"},"Dask official documentation")," for more."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import dask.dataframe as dd\ndf = dd.read_csv([f\"s3://{file}\" for file in files],  \n                 encoding = \"ISO-8859-1\",\n                 usecols = ['DepTime','FlightNum','DepDelay','Origin', 'Dest','Distance'],\n                 dtype={'Distance': 'float64'}\n                )\ndf['isDelayed'] = df.DepDelay.fillna(16) > 15 \ndf_delayed = df.groupby(['Origin','Dest']).isDelayed.mean()\ndf_results = df_delayed[df_delayed==1].compute().reset_index()\nprint(f'found {len(df_results)} most delayed flights Origin and destination pairs')\n")),(0,s.kt)("p",null,"The Dask operation took ",(0,s.kt)("strong",{parentName:"p"},"20 seconds")," using 2 remote 16 vCPU 16GB RAM Dask nodes. Comparing to Pandas, that's a ",(0,s.kt)("strong",{parentName:"p"},"~20x")," speed up with only 2 extra nodes!"),(0,s.kt)("p",null,"After using Dask it's good practice to close the cluster after the computation. Add the line below at the end of your notebook:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"client.shutdown()\n")),(0,s.kt)("h2",{id:"6-creating-the-yaml-file-for-deployment"},"6. Creating the YAML file for deployment"),(0,s.kt)("p",null,"Now the data processing notebook is developed and tested, to automatically run this notebook on a schedule as in most production setups, we can simply add a ",(0,s.kt)("inlineCode",{parentName:"p"},"pipeline.yaml")," file to build a pipeline. To read more on pipeline YAML files please visit the ",(0,s.kt)("a",{parentName:"p",href:"/shakudo-platform-core/jobs/#getstartedwithjobs"},"create a pipeline job page"),"."),(0,s.kt)("p",null,"Open a text file on your browser Session by clicking on the blue ",(0,s.kt)("strong",{parentName:"p"},"+")," button on the top left of the side bar. "),(0,s.kt)("p",null,"Copy and paste the content below:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'pipeline:\n  name: "data prep pipeline"\n  tasks:\n  - name: "dask processing data"\n    type: "jupyter notebook"\n    notebook_path: "example_notebooks/doc_demo/quick_start/dask.ipynb"  # path to the script to run\n    notebook_output_path: "dask_output.ipynb" \n')),(0,s.kt)("p",null,"In this YAML file, we'll need to change the ",(0,s.kt)("inlineCode",{parentName:"p"},"notebook_path")," to the actual path in your repository. "),(0,s.kt)("p",null,"This YAML is all we need to deploy the pipeline. Now let's commit the notebook and the YAML file to a GIT repository and run the job. Shakudo maintains live syncs of the repositories that are connect and make the code available for deployment immediately after code is synced.\nYou can check the status of the git sync at Admin Settings."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"git_sync_status",src:a(9799).Z,width:"2824",height:"1036"})),(0,s.kt)("h2",{id:"7-deploy-to-a-pipeline-job"},"7. Deploy to a pipeline job"),(0,s.kt)("p",null,"Now we are one step away to put the job in production! To launch a pipeline job, we can go to the Shakudo Platform dashboard's Jobs tab and click ",(0,s.kt)("strong",{parentName:"p"},"Create"),"."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Create an immediate job",src:a(3653).Z,width:"1598",height:"220"})),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Create an immediate job dialog",src:a(5970).Z,width:"1587",height:"381"})),(0,s.kt)("p",null,"In the job dialogue, we need to fill the following:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Name: Job name or use the randomized name "),(0,s.kt)("li",{parentName:"ul"},"Image: Choose the image that we developed the code on to maintain environment consistency "),(0,s.kt)("li",{parentName:"ul"},"YAML path: Paste the path of the YAML file that we created above, from the root of the repository   ")),(0,s.kt)("p",null,"Click the ",(0,s.kt)("strong",{parentName:"p"},"Create Immediate Job")," button on the top right corner to create the job."),(0,s.kt)("h2",{id:"8-check-job-status"},"8. Check job status"),(0,s.kt)("p",null,"Once the job is submitted, you are redirected to the immediate jobs dashboard where our job is at the top of the job list. To see the live log of the job, click on the file button. You can pin the job to the top with the pin button, or checkout more functions in side the three dots."),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"Job_submitted",src:a(9102).Z,width:"1591",height:"309"})),(0,s.kt)("p",null,"Congratulations on developed and deployed your first Shakudo job pipeline with distributed computing!"),(0,s.kt)("h2",{id:"9-additional-steps"},"9. Additional Steps"),(0,s.kt)("p",null,"Shakudo Platform offers a variety of other functionalities for more advanced workflows. Some additional uses include the following:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"/shakudo-platform-core/jobs/#scheduleajob"},"Run pipelines on a schedule")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"/shakudo-platform-core/jobs/#triggerajob"},"Triggering pipeline jobs programmatically"))))}h.isMDXComponent=!0},9311:function(e,t,a){t.Z=a.p+"assets/images/access_sessions-5514524f54338d4b389d2e7999be717f.png"},3653:function(e,t,a){t.Z=a.p+"assets/images/create_immediate_job-cb648e6a20b5a99e2fdbf0737fa078f1.png"},9799:function(e,t,a){t.Z=a.p+"assets/images/git_sync_status-70fbb7255aaa524500f9e6bda9a0f095.png"},5970:function(e,t,a){t.Z=a.p+"assets/images/immediate_job_dialog-8ba2a555ca048d985489f00d431ca23a.png"},9102:function(e,t,a){t.Z=a.p+"assets/images/job_submitted-70e4852c6ee9e4079dd6c1ed5c1b6f50.png"},8143:function(e,t,a){t.Z=a.p+"assets/images/start_session-a2cec1e0a2dcab9c1a593a3fe91c6a83.png"}}]);