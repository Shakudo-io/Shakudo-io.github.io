"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3227],{3905:function(e,t,r){r.d(t,{Zo:function(){return u},kt:function(){return m}});var n=r(7294);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function a(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function l(e,t){if(null==e)return{};var r,n,o=function(e,t){if(null==e)return{};var r,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||(o[r]=e[r]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var c=n.createContext({}),s=function(e){var t=n.useContext(c),r=t;return e&&(r="function"==typeof e?e(t):a(a({},t),e)),r},u=function(e){var t=s(e.components);return n.createElement(c.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var r=e.components,o=e.mdxType,i=e.originalType,c=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=s(r),m=o,f=d["".concat(c,".").concat(m)]||d[m]||p[m]||i;return r?n.createElement(f,a(a({ref:t},u),{},{components:r})):n.createElement(f,a({ref:t},u))}));function m(e,t){var r=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=r.length,a=new Array(i);a[0]=d;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:o,a[1]=l;for(var s=2;s<i;s++)a[s]=r[s];return n.createElement.apply(null,a)}return n.createElement.apply(null,r)}d.displayName="MDXCreateElement"},6186:function(e,t,r){r.r(t),r.d(t,{assets:function(){return u},contentTitle:function(){return c},default:function(){return m},frontMatter:function(){return l},metadata:function(){return s},toc:function(){return p}});var n=r(7462),o=r(3366),i=(r(7294),r(3905)),a=["components"],l={sidebar_position:1},c="NVIDIA Triton",s={unversionedId:"Shakudo-stack/modelServing/triton",id:"Shakudo-stack/modelServing/triton",title:"NVIDIA Triton",description:"The Shakudo Platform comes with a build-in NVIDIA Triton Inference Server that simplifies the deployment of AI models at scale in production. Triton is an open-source inference serving software that lets teams deploy trained AI models from any framework (TensorFlow, NVIDIA\xae TensorRT\xae, PyTorch, ONNX Runtime, or custom) from local storage or cloud platform on any GPU- or CPU-based infrastructure (cloud, data center, or edge).",source:"@site/docs/Shakudo-stack/modelServing/triton.md",sourceDirName:"Shakudo-stack/modelServing",slug:"/Shakudo-stack/modelServing/triton",permalink:"/Shakudo-stack/modelServing/triton",draft:!1,tags:[],version:"current",lastUpdatedBy:"yevgeniy-ds",lastUpdatedAt:1759413731,formattedLastUpdatedAt:"10/2/2025",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Model Serving",permalink:"/category/model-serving"},next:{title:"TorchServe",permalink:"/Shakudo-stack/modelServing/torchserve"}},u={},p=[],d={toc:p};function m(e){var t=e.components,r=(0,o.Z)(e,a);return(0,i.kt)("wrapper",(0,n.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"nvidia-triton"},"NVIDIA Triton"),(0,i.kt)("p",null,"The Shakudo Platform comes with a build-in NVIDIA Triton Inference Server that simplifies the deployment of AI models at scale in production. Triton is an open-source inference serving software that lets teams deploy trained AI models from any framework (TensorFlow, NVIDIA\xae TensorRT\xae, PyTorch, ONNX Runtime, or custom) from local storage or cloud platform on any GPU- or CPU-based infrastructure (cloud, data center, or edge)."),(0,i.kt)("p",null,"To serve your model with the Triton server, you need to upload your model to the triton server model repository and write a client file. The default path of the triton model repository is ",(0,i.kt)("inlineCode",{parentName:"p"},"{your_cloud_bucket}/triton-server/model-repository/"),"."),(0,i.kt)("p",null,"The official ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/triton-inference-server/client/tree/main/src/python/examples"},"Triton client examples")," will help you with different client files for popular machine learning tasks such as image recognition and NLP. "),(0,i.kt)("p",null,"Please find a ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/devsentient/examples/tree/main/example_notebooks/serving/triton"},"simple App")," in the Shakudo example repository that serves an image recognition model."))}m.isMDXComponent=!0}}]);