"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4779],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return m}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=p(a),m=r,f=c["".concat(s,".").concat(m)]||c[m]||u[m]||o;return a?n.createElement(f,i(i({ref:t},d),{},{components:a})):n.createElement(f,i({ref:t},d))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},115:function(e,t,a){a.r(t),a.d(t,{assets:function(){return d},contentTitle:function(){return s},default:function(){return m},frontMatter:function(){return l},metadata:function(){return p},toc:function(){return u}});var n=a(7462),r=a(3366),o=(a(7294),a(3905)),i=["components"],l={sidebar_position:1},s="Overview",p={unversionedId:"Shakudo-stack/distributedComputing/overview",id:"Shakudo-stack/distributedComputing/overview",title:"Overview",description:"The Shakudo Platform supports many distributed computing methods.",source:"@site/docs/Shakudo-stack/distributedComputing/overview.md",sourceDirName:"Shakudo-stack/distributedComputing",slug:"/Shakudo-stack/distributedComputing/overview",permalink:"/Shakudo-stack/distributedComputing/overview",draft:!1,tags:[],version:"current",lastUpdatedBy:"yevgeniy-ds",lastUpdatedAt:1759422702,formattedLastUpdatedAt:"10/2/2025",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Distributed Computing",permalink:"/category/distributed-computing"},next:{title:"Dask",permalink:"/Shakudo-stack/distributedComputing/dask"}},d={},u=[{value:"Dask",id:"dask",level:2},{value:"Dask Collections",id:"dask-collections",level:3},{value:"Dask bags",id:"dask-bags",level:4},{value:"Dask dataframes",id:"dask-dataframes",level:4},{value:"Lazy calculations",id:"lazy-calculations",level:3},{value:"Repartitioning",id:"repartitioning",level:3},{value:"Split out",id:"split-out",level:3},{value:"Cheap vs. expensive computations",id:"cheap-vs-expensive-computations",level:3},{value:"File types",id:"file-types",level:3},{value:"parquet files",id:"parquet-files",level:4},{value:"avro files",id:"avro-files",level:4},{value:"Saving to cloud storage",id:"saving-to-cloud-storage",level:3},{value:"Choosing Dask workers and specs",id:"choosing-dask-workers-and-specs",level:3},{value:"Worker freeze policies",id:"worker-freeze-policies",level:4},{value:"Examples",id:"examples",level:3},{value:"Pandas to Dask.dataframe",id:"pandas-to-daskdataframe",level:4},{value:"Column to_datetime",id:"column-to_datetime",level:4},{value:"Dataframe groupby",id:"dataframe-groupby",level:4},{value:"Get dummies",id:"get-dummies",level:4},{value:"Dask map",id:"dask-map",level:4},{value:"Parallel training and preprocessing on dask",id:"parallel-training-and-preprocessing-on-dask",level:4},{value:"Ray",id:"ray",level:2},{value:"Spark",id:"spark",level:2},{value:"Initializing a distributed Ray cluster for Spark",id:"initializing-a-distributed-ray-cluster-for-spark",level:3},{value:"Start a Spark session",id:"start-a-spark-session",level:3},{value:"Use PySpark",id:"use-pyspark",level:3},{value:"Shutdown a Ray cluster",id:"shutdown-a-ray-cluster",level:3},{value:"RAPIDS",id:"rapids",level:2},{value:"Get started",id:"get-started",level:3}],c={toc:u};function m(e){var t=e.components,a=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"overview"},"Overview"),(0,o.kt)("p",null,"The Shakudo Platform supports many distributed computing methods."),(0,o.kt)("p",null,"With more data collected and streamed for machine learning and data science, every machine learning team is looking at ways to scale up computation with more machines while cut down latency. Distributed computing frameworks like Dask, Ray, and Spark is designed to accomplish exactly this. In addition Rapids as an extension to CPU tools that further leverages GPUs to gain speed."),(0,o.kt)("h2",{id:"dask"},"Dask"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://docs.dask.org/"},"Dask"),"\xa0is a flexible open source distributed framework for parallel computing. It has similar APIs to NumPy and Pandas, is an ideal choice for parallelizing NumPy, Pandas and List based code."),(0,o.kt)("p",null,"Shakudo Platform comes with a number of useful APIs to make using Dask easy. See the ",(0,o.kt)("a",{parentName:"p",href:"/legacy/shakudoApi"},"Hyperplane API page")," for a full list."),(0,o.kt)("h3",{id:"dask-collections"},"Dask Collections"),(0,o.kt)("p",null,"Dask collections are useful for large datasets because they support delayed tasks. We will explore three types","\u2014"," Dask bags, Dask dataframes, and Dask arrays. "),(0,o.kt)("h4",{id:"dask-bags"},"Dask bags"),(0,o.kt)("p",null,"Dask bags (synonymous with multisets) are unordered collections of immutable objects. Below are some common operations:"),(0,o.kt)("p",null,"Select records where:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"b.filter(lambda record: record['num_clicks'] > 2).take(2) \n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-output"},"({'id': '01mz489cnkd',  \n  'area': 'Aerial Alaska',  \n  'num_clicks': 3,\n  'info': {'a_field': 0}},  \n {'id': '25z48t9cfaf',  \n  'area': 'Bustling Birktown',  \n  'num_clicks': 5,\n  'info': {'a_field': 1}})  \n")),(0,o.kt)("p",null,"Select one field:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"b.map(lambda record: record['area']).take(2)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-output"},"('Aerial Alaska', 'Bustling Birktown')\n")),(0,o.kt)("p",null,"Aggregate the number of records in your bag:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"b.count().compute()\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-output"},"100000\n")),(0,o.kt)("p",null,"Note that the ",(0,o.kt)("inlineCode",{parentName:"p"},".take(n)")," function will return the first ",(0,o.kt)("strong",{parentName:"p"},"n")," records from the bag, only in the first partition."),(0,o.kt)("p",null,"For more info, see ",(0,o.kt)("a",{parentName:"p",href:"https://examples.dask.org/bag.html"},"https://examples.dask.org/bag.html")),(0,o.kt)("h4",{id:"dask-dataframes"},"Dask dataframes"),(0,o.kt)("p",null,"Dask dataframes are collections of pandas dataframes. It can be used in cases where one pandas dataframe is too large to fit in memory and to speed up expensive computations by using multiple cores. "),(0,o.kt)("p",null,"To read multiple csvs, use the * or a list of files. Each file will be read into a separate partition."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"import dask.dataframe as dd\ndf = dd.read_csv('2014-*.csv')\n")),(0,o.kt)("p",null,"A common workflow is the following:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Load large datasets from files"),(0,o.kt)("li",{parentName:"ol"},"Filter to a subset of data"),(0,o.kt)("li",{parentName:"ol"},"Shuffle data to get an intelligent index"),(0,o.kt)("li",{parentName:"ol"},"Perform queries or aggregations using the indices")),(0,o.kt)("p",null,"For more information on Dask dataframes, see ",(0,o.kt)("a",{parentName:"p",href:"https://docs.dask.org/en/latest/dataframe.html"},"https://docs.dask.org/en/latest/dataframe.html"),"."),(0,o.kt)("h3",{id:"lazy-calculations"},"Lazy calculations"),(0,o.kt)("p",null,"Dask operates with ",(0,o.kt)("strong",{parentName:"p"},"lazy")," collections, meaning operations on a collection are simply scheduled by the scheduler, but the actual calculation will not be triggered until explicitly called. At first, you will notice that dataframe operations or functions seem to happen almost instantaneously, but nothing will be calculated until one of the following is used:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},".persist()"),(0,o.kt)("li",{parentName:"ul"},".compute()"),(0,o.kt)("li",{parentName:"ul"},".load()")),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},".compute()")," will trigger a computation on the Dask cluster without returning anything. You can use this if some of your functions include saving to a location."),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},".persist()")," will trigger a computation on the Dask cluster and store the results in ram. Use this sparingly, only if you need to use an intermediate collection, or after a computationally expensive operation such as index, groupby, etc."),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},".load()")," will trigger a computation on the Dask cluster when you are working with Dask xarrays."),(0,o.kt)("p",null,"For example, you can trigger all computations on your dataframe like the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"df = dask.df.read_parquet('file*.csv')\ndf = df[['col_a', 'col_b']]\ndf = df.drop_duplicates()\ndf = client.persist(df)\n")),(0,o.kt)("h3",{id:"repartitioning"},"Repartitioning"),(0,o.kt)("p",null,"After a few computations, your Dask df may need to be repartitioned, due to the partition size-number tradeoff. Partitions that are too large will cause out of memory errors, while too many partitions will incure a larger overhead time for the schedule to process. See more on best practices at ",(0,o.kt)("a",{parentName:"p",href:"https://docs.dask.org/en/latest/dataframe-best-practices.html"},"https://docs.dask.org/en/latest/dataframe-best-practices.html"),"."),(0,o.kt)("h3",{id:"split-out"},"Split out"),(0,o.kt)("p",null,"Dataframe aggregation operations can get slow. Try to use ",(0,o.kt)("inlineCode",{parentName:"p"},"split_out")," in aggregation operationg like groupbys to spread the aggregation work."),(0,o.kt)("h3",{id:"cheap-vs-expensive-computations"},"Cheap vs. expensive computations"),(0,o.kt)("p",null,"Examples of fast and cheap computations:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Element-wise ops (addition, multiplication)"),(0,o.kt)("li",{parentName:"ul"},"Row-wise operations and filtering: ",(0,o.kt)("inlineCode",{parentName:"li"},"df[df.x > 0]")),(0,o.kt)("li",{parentName:"ul"},"Joining Dask dfs along indexed fields, or joining with a one-partition Dask df"),(0,o.kt)("li",{parentName:"ul"},"Max, min, count, common aggregations (",(0,o.kt)("inlineCode",{parentName:"li"},"df.groupby(df.x).y.max()"),")"),(0,o.kt)("li",{parentName:"ul"},"isin: ",(0,o.kt)("inlineCode",{parentName:"li"},"df[df.x.isin([1, 2, 3])]")),(0,o.kt)("li",{parentName:"ul"},"drop_duplicates"),(0,o.kt)("li",{parentName:"ul"},"groupby-apply on an index: ",(0,o.kt)("inlineCode",{parentName:"li"},"df.groupby(['idx', 'x']).apply(myfunc)"))),(0,o.kt)("p",null,"Examples of slow and expensive computations (for this reason, it is often recommended to use persist your data after these steps for stability):"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"setting an index: ",(0,o.kt)("inlineCode",{parentName:"li"},"df.set_index(df.x)")),(0,o.kt)("li",{parentName:"ul"},"groupby-apply on non-index fields: ",(0,o.kt)("inlineCode",{parentName:"li"},"df.groupby(df.x).apply(myfunc)")),(0,o.kt)("li",{parentName:"ul"},"joining two dataframes along non-index columns")),(0,o.kt)("h3",{id:"file-types"},"File types"),(0,o.kt)("h4",{id:"parquet-files"},"parquet files"),(0,o.kt)("p",null,"Parquet is a columnar storage format for Hadoop, which enables parallel reading and writing, and is most useful for efficiently filtering a subset of fields in a Dask df. "),(0,o.kt)("h4",{id:"avro-files"},"avro files"),(0,o.kt)("p",null,"Avro is a row-based storage format for Hadoop, which is most efficient if you intend to retrieve and use all fields or columns in the dataset. "),(0,o.kt)("h3",{id:"saving-to-cloud-storage"},"Saving to cloud storage"),(0,o.kt)("p",null,"Remember that new workers are spun up when use ",(0,o.kt)("inlineCode",{parentName:"p"},".initialize_cluster()"),", and they are destroyed on ",(0,o.kt)("inlineCode",{parentName:"p"},"cluster.close()"),". This means you should ensure your intermediate and output files are saved in a cloud storage location that can be accessed outside of each node. "),(0,o.kt)("p",null,"This can be achieved through the following code example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"gcp_project = YOUR_GCP_PROJECT\ngcs_client = storage.Client(project=gcp_project)\nbucket = gcs_client.get_bucket(bucket)\nblob = bucket.blob(yourfile)\nblob.upload_from_string(filename, content_type='application/x-www-form-urlencoded;charset=UTF-8')\n")),(0,o.kt)("h3",{id:"choosing-dask-workers-and-specs"},"Choosing Dask workers and specs"),(0,o.kt)("p",null,"If you are aiming for a specific pool, ensure that"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"nprocs")," x ",(0,o.kt)("inlineCode",{parentName:"p"},"nthreads")," \u2264 ",(0,o.kt)("inlineCode",{parentName:"p"},"cores_per_worker")," \u2264 the number of allocatable cores "),(0,o.kt)("p",null,"and"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"nprocs")," x ",(0,o.kt)("inlineCode",{parentName:"p"},"ram_gb_per_proc")," \u2264 allocatable ram."),(0,o.kt)("p",null,"For example, if you would like to use a ",(0,o.kt)("inlineCode",{parentName:"p"},"DASK_POOL_16_16")," worker, you may want to choose the following cluster initialization"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"nprocs = 5\nnthreads = 3\nram_gb_proc = 2.4\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Rules of Thumb")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"For a job with aggregation (data transfer) choose a setup with a minimum of 10x the data size. Worker memory usage is about 10% initially. If at any point any worker\u2019s memory usage exceeds 75%, the job is very likely to fail (see ",(0,o.kt)("a",{parentName:"p",href:"#worker-freeze-policies"},"Worker freeze policies"),").")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"For a job that consists of only parallel-friendly operations (no sorting, shuffling, or moving large chunks of data), use more CPU (for example ",(0,o.kt)("inlineCode",{parentName:"p"},"16_16"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"32_32"),"). Otherwise use more memory (for example ",(0,o.kt)("inlineCode",{parentName:"p"},"16_128"),").")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"For a job that requires both huge data and a large number of tasks, split it into multiple jobs to avoid errors. For example, an optimized setting for 1TB group-by job is to split into 10 pieces (100GB each) and use 24 of ",(0,o.kt)("inlineCode",{parentName:"p"},"32_32")," nodes. You can further convert the piece indicator to a parameter like ",(0,o.kt)("inlineCode",{parentName:"p"},"chunk_id")," and convert the code into a pipeline job, then run 10 pipeline jobs concurrently to save more time."))),(0,o.kt)("p",null,"Use the steps below to estimate how many nodes and workers you will need:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Check data size (uncompressed). For example, 100GB"),(0,o.kt)("li",{parentName:"ol"},"Choose operation type to find a multiplier of memory: light (x4), medium (x8), heavy(x48). Multiply your data size from step 1 by this multiplier. For example, a group-by will be medium, which requires 100G x 8 ~ 800GB total memory"),(0,o.kt)("li",{parentName:"ol"},"Use the number of tasks (heavy vs. light) to determine the number of nodes. If the sequence of operations has many tasks, (computationally heavy), use ",(0,o.kt)("inlineCode",{parentName:"li"},"32_32"),". Otherwise use ",(0,o.kt)("inlineCode",{parentName:"li"},"16_128"),". Multiply your required memory from step 2 by 32 or 128 depending on computation load. For example, 800GB/32GB = 25 nodes, or 800GB/128GB = 8 of ",(0,o.kt)("inlineCode",{parentName:"li"},"16_128")," nodes. ")),(0,o.kt)("p",null,"At this point, you should have an approximate Dask pool spec and number of workers. "),(0,o.kt)("p",null,"Add-on step: Setup automatic retry if in pipeline mode. Sometimes pipelines error out when spinning up nodes, or HTTP error, canceled error. These can be fixed by retrying."),(0,o.kt)("h4",{id:"worker-freeze-policies"},"Worker freeze policies"),(0,o.kt)("p",null,"Below are the defaults for worker memory limits and actions to avoid memory blowup."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"distributed:\n  worker:\n    memory:\n      target: 0.60  # target fraction to stay below\n      spill: 0.70  # fraction at which we spill to disk\n      pause: 0.80  # fraction at which we pause worker threads\n      terminate: 0.95  # fraction at which we terminate the worker\n")),(0,o.kt)("h3",{id:"examples"},"Examples"),(0,o.kt)("h4",{id:"pandas-to-daskdataframe"},"Pandas to Dask.dataframe"),(0,o.kt)("p",null,"Below are some common examples of converting pandas operations to dask-friendly code."),(0,o.kt)("h4",{id:"column-to_datetime"},"Column to_datetime"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# pandas to_datetime\nimport pandas as pd\ndf1['date'] = pd.to_datetime(df1['date'])\n\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# dask to_datetime\nfrom dask import dataframe as dd\ndf1['date'] = dd.to_datetime(df1['date'])\n")),(0,o.kt)("h4",{id:"dataframe-groupby"},"Dataframe groupby"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# pandas groupby\ndf_metrics = df_metrics.groupby(pd.Grouper(timeframe='1day',\n                                           closed='right',label='right')\n                               ).agg({'id':pd.Series.nunique,\n                                      'num_entries':'sum',\n                                      'total_runs':'sum')\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# dask is exactly the same \ndf_metrics = df_metrics.groupby(pd.Grouper(timeframe='1day',\n                                           closed='right',label='right')\n                               ).agg({'id':pd.Series.nunique,\n                                      'num_entries':'sum',\n                                      'total_runs':'sum')\n")),(0,o.kt)("h4",{id:"get-dummies"},"Get dummies"),(0,o.kt)("p",null,"The following example features a more complicated groupby; get_dummies will explode in dimension with large amount of data (i.e. possibly explode the data into many columns)"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# pandas version do dummies first then groupby time interval to get aggregation per time interval\ndfr= pd.get_dummies(df,['col_a','col_b','col_c'])\ndfr = dfr.merge(df['date'],right_index=True,left_index=True)\ndfr = dfr.sort_values(by='date')\ndfr = dfr.groupby(pd.Grouper(timeframe='1day',closed='right',label='right')).sum()\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# dask version do value counts instead of getting dummies, and do pivot after groupby\ndef agg_func(df: pd.DataFrame, timeframe: str, ts_col: str, sec_id: str, target_col:str) -> pd.DataFrame:\n    \"\"\"\n    function that group data by required timeframe for one target column\n    df: dataframe to be aggregated  \n    timeframe: aggregation timeframe\n    ts_col: column name of the index\n    sec_id: column name of a secondary id\n    target_col: target column name for processing e.g. col_a\n    \"\"\"\n    df = df.groupby([sec_id, ts_col,target_col]).size().reset_index().set_index(ts_col)\n    df.columns = [sec_id, target_col,'count']\n    df_agg = df.groupby(pd.Grouper(timeframe=timeframe, closed='right', label='right')).apply(\n        lambda x: x.groupby(target_col).agg({'count': 'sum'})).reset_index()\n    return df_agg\n\n\nmeta_df = agg_func(df.head(10), timeframe, ts_col, sec_id, target_col).dtypes.to_dict()\ndf = df.map_partitions(agg_func, timeframe, ts_col, sec_id, target_col,  meta = meta_df)\ndf.columns = [ts_col, target_col, 'count']\n\n# further groupby session_ts and event as there will be duplicates among partition\ndf = df.groupby([ts_col, target_col]).agg({'count': 'sum'})\n\n# create pivot table for end results\ndf = df.reset_index()\ndf = df.pivot_table(values=\"count\", index=ts_col, columns=target_col)\ndf.columns = [target_col+'_'+i for i in list(df.columns)]\n")),(0,o.kt)("h4",{id:"dask-map"},"Dask map"),(0,o.kt)("p",null,"The following example optimizes a function that reads a list of files one by one."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"## this snippet reads a list of files one by one\n\nimport xarray as xr\nimport gcsfs\n\nfs = gcsfs.GCSFileSystem(project='myproject', token=None)\nfiles_list = ['file1', 'file2', 'file3', 'file4']\n\ngcsmap = gcsfs.mapping.GCSMap(f'gs://my-bucket/{files_list[0]}', gcs=fs)\nGlob = xr.open_zarr(store=gcsmap).load()\n\n## add other datasets sequentially\nfor filepath in files_list[1:]:\n    gcsmap = gcsfs.mapping.GCSMap(f'gs://my-bucket/{filepath}', gcs=fs)\n    ds = xr.open_zarr(store=gcsmap).load()\n    Glob = xr.merge([Glob, ds], compat=\"no_conflicts\", combine_attrs = \"no_conflicts\") \n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"## in dask, create a function to read one file, then use client.map the function and list of files\nimport xarray as xr\nimport gcsfs\n\nfs = gcsfs.GCSFileSystem(project='myproject', token=None)\nfiles_list = ['file1', 'file2', 'file3', 'file4']\n\ndef read_files(gsfilepath):\n    gcsmap = gcsfs.mapping.GCSMap(f\"gs://my-bucket/{gsfilepath}\", gcs=fs)\n    ds = xr.open_zarr(store=gcsmap).load().persist()\n    return ds\n\ndss = client.map(read_files, files_list)\nds_list = client.gather(dss)\nprint(len(ds_list)) # output: 4\nGlob = xr.merge(ds_list, compat=\"no_conflicts\", combine_attrs = \"no_conflicts\")\n")),(0,o.kt)("h4",{id:"parallel-training-and-preprocessing-on-dask"},"Parallel training and preprocessing on dask"),(0,o.kt)("p",null,"Sklearn training can be easily converted to distributed training with dask using joblib. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import joblib\n\nwith joblib.parallel_backend('dask'):\n    grid_search.fit(X, y)\n")),(0,o.kt)("p",null,"Many sklearn preprocessing modules (e.g. OneHotEncoder, Categorize, StandardScaler, etc.), models (NaiveBayes, xgboost, clustering, etc.), and model selection utilities (KFold, train_test_split, etc.) have dask equivalents. "),(0,o.kt)("p",null,"See ",(0,o.kt)("a",{parentName:"p",href:"https://ml.dask.org/index.html"},"https://ml.dask.org/index.html")," for full list of equivalents."),(0,o.kt)("h2",{id:"ray"},"Ray"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"http://ray.io/"},"Ray"),"\xa0is an distributed frameworks open source project that has a more support for deep learning and reinforcement learning. It has a rich set of libraries and integrations built on a flexible distributed execution framework, is ideal choice for parallelizing model training and hyper-parameter tuning."),(0,o.kt)("h2",{id:"spark"},"Spark"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://spark.apache.org/"},"Apache Spark"),"\xa0is an open source platform for large-scale SQL, batch processing, stream processing, and machine learning. PySpark is the python API for Spark and in the recent releases PySpark adopted more Pandas like APIs. Spark is great for data processing especially for the computations that involves shuffling joining type of operations."),(0,o.kt)("p",null,"Shakudo Platform provides simple APIs to use Spark on distributed Ray clusters using RayDP. RayDP combines your Spark and Ray clusters, making it easy to do large scale data processing using the PySpark API and seamlessly use that data to train your models using TensorFlow and PyTorch."),(0,o.kt)("h3",{id:"initializing-a-distributed-ray-cluster-for-spark"},"Initializing a distributed Ray cluster for Spark"),(0,o.kt)("p",null,"Initialize a distributed Ray cluster as usual using the following:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from hyperplane.ray_common import initialize_ray_cluster\nray_cluster = initialize_ray_cluster(\n    num_workers = 4, \n    cpu_core_per_worker = 15, \n    ram_gb_per_worker = 12\n    )\n")),(0,o.kt)("p",null,"num_workers (int) is the number of Ray nodes to be initialized cpu_core_per_worker (int) is the number of CPU cores in each Ray node ram_gb_per_worker (float) is the memory size in GB for each Ray node Read more about Ray and Ray on Shakudo Platform."),(0,o.kt)("h3",{id:"start-a-spark-session"},"Start a Spark session"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"spark = raydp.init_spark(\n    'example', \n    num_executors=2, \n    executor_cores=4, \n    executor_memory='4G'\n    )\n")),(0,o.kt)("h3",{id:"use-pyspark"},"Use PySpark"),(0,o.kt)("p",null,"Once the Spark session is initialized, you can use pyspark as ususal from here on. The latest RayDP supports PySpark 3.2.0+, which provides simple Pandas-like APIs."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import pyspark.pandas as pd\ndf = pd.read_csv("data.csv")\n')),(0,o.kt)("h3",{id:"shutdown-a-ray-cluster"},"Shutdown a Ray cluster"),(0,o.kt)("p",null,"After computation is finished, it's a good idea to shutdown the distributed cluster and release the resources back to the node pool. If any Ray nodes are left hanging, Shakudo Platform's garbage collection function will also automatically shutdown the Ray workers when the Jupyter session or job is finished. You can add this line to the end of your code to shutdown the Ray nodes."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"stop_ray_cluster(ray_cluster)\n")),(0,o.kt)("h2",{id:"rapids"},"RAPIDS"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://rapids.ai/"},"Rapids"),"\xa0is a suite of open source libraries and APIs for doing data science on GPUs. Rapids can speed up common computation by 50x and has similar APIs to Pandas, NumPy and Scikit-learn and support multi-GPU scale up. They are very useful in significantly speed up long-running preprocessing loads. "),(0,o.kt)("h3",{id:"get-started"},"Get started"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Start by spinning up a ",(0,o.kt)("a",{parentName:"p",href:"/shakudo-platform-core/sessions/"},"Session")," with the GPU session type.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Initialize cluster with GPUs"))),(0,o.kt)("p",null,"On the Sessions GPU image, you can scale up a Dask cluster with GPUs by adding ",(0,o.kt)("inlineCode",{parentName:"p"},"ngpus=1")," to the cluster initialization. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'client, cluster = nc.initialize_cluster(\n    nprocs=1,\n    nthreads=8,\n    ram_gb_per_proc=7,\n    cores_per_worker=2,\n    num_workers = 2,\n    ngpus = 1,\n    scheduler_deploy_mode="local"\n)\n')),(0,o.kt)("ol",{start:3},(0,o.kt)("li",{parentName:"ol"},"Once the Dask cluster is spun up use the RAPIDS library by import importing relevant packages. For example ",(0,o.kt)("inlineCode",{parentName:"li"},"dask_cudf"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"df = dask_cudf.read_csv(file_path, assume_missing=True)\n")))}m.isMDXComponent=!0}}]);