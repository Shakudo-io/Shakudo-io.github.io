{"searchDocs":[{"title":"First Blog Post","type":0,"sectionRef":"#","url":"/blog/first-blog-post","content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","keywords":"","version":null},{"title":"Long Blog Post","type":0,"sectionRef":"#","url":"/blog/long-blog-post","content":"This is the summary of a very long blog post, Use a &lt;!-- truncate --&gt; comment to limit blog post size in the list view. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","keywords":"","version":null},{"title":"MDX Blog Post","type":0,"sectionRef":"#","url":"/blog/mdx-blog-post","content":"Blog posts support Docusaurus Markdown features, such as MDX. tip Use the power of React to create interactive blog posts. &lt;button onClick={() =&gt; alert('button clicked!')}&gt;Click me!&lt;/button&gt; Click me!","keywords":"","version":null},{"title":"Welcome","type":0,"sectionRef":"#","url":"/blog/welcome","content":"Docusaurus blogging features are powered by the blog plugin. Simply add Markdown files (or folders) to the blog directory. Regular blog authors can be added to authors.yml. The blog post date can be extracted from filenames, such as: 2019-05-30-welcome.md2019-05-30-welcome/index.md A blog post folder can be convenient to co-locate blog post images: The blog supports tags as well! And if you don't want a blog: just delete this directory, and use blog: false in your Docusaurus config.","keywords":"","version":null},{"title":"Apps Catalogue (Stack Components Directory)","type":0,"sectionRef":"#","url":"/developer-docs/features/apps-catalogue","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#overview","content":"The Apps Catalogue (also known as Stack Components Directory) is a browsable directory of pre-configured data tools and applications available for one-click installation in the Shakudo platform. This feature provides administrators with a visual catalogue of stack components (like databases, analytics tools, workflow orchestrators, etc.) that can be automatically deployed to the platform with minimal configuration. Each component is presented as a card with its logo, category, installation status, and an install action button. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#access--location","content":"Route: ?panel=apps-catalogueNavigation: Admin → Stack Components CatalogueAccess Requirements: dashboard-admin roleFeature Flags: selfInstalledAppsEnabled must be enabled ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Browse Available Stack Components​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#browse-available-stack-components","content":"Users can explore a visual catalogue of pre-configured data science and analytics tools, each represented by a card showing the tool's name, logo, category, and current installation status. The catalogue includes components ranging from databases to workflow orchestrators to visualization tools. ","version":"Next","tagName":"h3"},{"title":"One-Click Installation​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#one-click-installation","content":"For components marked as &quot;CATALOGUED&quot; (ready to install), administrators can install them with a single click. The system automatically creates a dedicated namespace (hyperplane-auto-{component-name}), configures necessary metadata, and initiates the Helm chart deployment without requiring manual configuration. ","version":"Next","tagName":"h3"},{"title":"Search and Filter by Category​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#search-and-filter-by-category","content":"The interface provides a search bar for finding specific stack components by name, and a category filter menu that allows filtering components by their functional category (e.g., databases, workflows, visualization, etc.). This makes it easy to discover relevant tools in large catalogues. ","version":"Next","tagName":"h3"},{"title":"Monitor Installation Status​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#monitor-installation-status","content":"Each component card displays its current installation status (CATALOGUED, SCALING, ACTIVE, PAUSED, ARCHIVED, etc.), allowing administrators to track which components are installed, which are being deployed, and which are available for installation. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#main-view","content":"The panel features a two-column layout: Left Sidebar (Filter Panel): Search text field with search icon for filtering components by nameCategory filter menu listing all available categories with an &quot;All&quot; optionCategories are dynamically populated based on available apps Main Content Area: Title: &quot;Stack Component Directory&quot;Grid of app cards (responsive: 3 cards per row on desktop, 1 on mobile)Each card displays: Component logo/icon with background colorComponent nameCategory labelInstall/status button in bottom-right corner Loading indicator appears during data fetches ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#dialogs--modals","content":"Installation Confirmation Dialog Purpose: Confirm installation of a stack component before proceedingTitle: &quot;Install {App Name}&quot;Prompt: &quot;Are you sure you want to install '{App Name}'?&quot;Actions: Cancel or InstallAutomatically displays success notification with namespace information after installation ","version":"Next","tagName":"h3"},{"title":"Installation States & Actions​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#installation-states--actions","content":"Component Install States: CATALOGUED - Ready to install, shows &quot;Install&quot; button (blue, contained style)SCALING - Currently installing, shows &quot;Scaling&quot; button (disabled, loading spinner)ACTIVE - Installed and running, shows &quot;Pause&quot; button (outlined style)PAUSED - Installed but scaled down, shows &quot;Activate&quot; button (blue, contained style)PAUSING - Currently scaling down, shows &quot;Pausing&quot; button (disabled, loading spinner)ARCHIVED - Not auto-installable, requires manual configuration (disabled)UNKNOWN - Status cannot be determined (disabled)UNAVAILABLE - Deprecated status, component not available (disabled) Special Cases: Components in category &quot;Shakudo Core&quot; cannot be installed or uninstalled through this interfaceInstall buttons are disabled during state transitions (SCALING, PAUSING)Hover tooltips provide detailed status explanations and namespace information ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#graphql-operations","content":"Queries: getCatalogue - Retrieves the list of all available stack components from the catalogue with their current installation status, category, description, Helm chart info, license details, and image metadata. Supports filtering by name and category. Uses polling interval of 5000ms for real-time status updates. Mutations: installOnePlatformApp - Creates and installs a new platform app from the catalogue. Takes PlatformAppCreateInput which includes name, namespace, component label, category, tooltip description, image details, and license information. Automatically generates namespace as hyperplane-auto-{app-name}. Subscriptions:None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#component-structure","content":"Main Component: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformAppsCatalogue/PlatformAppsCataloguePanel.tsxGrid View: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformAppsCatalogue/PlatformAppsCatalogueGrid.tsxApp Cards: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformAppsCatalogue/PlatformAppCatalogueCard.tsxInstall Component: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformAppsCatalogue/PlatformAppsCatalogueInstall.tsxCategory Filter: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformAppsCatalogue/PlatformCatalogueCategoryFilterMenu.tsxUtility Functions: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformAppsCatalogue/getInstallStateDescription.ts ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#state-management","content":"Uses Jotai atom PlatformAppPageAtom for managing selected platform app stateLocal component state for search query and selected category filterApollo Client cache for GraphQL query data with 5-second polling interval ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Installing a New Stack Component​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#installing-a-new-stack-component","content":"Navigate to Admin → Stack Components CatalogueBrowse available components or use search/filter to find desired toolLocate component card showing &quot;Install&quot; button (must be in CATALOGUED state)Click &quot;Install&quot; button on the component cardConfirm installation in the dialog that appearsSystem displays success notification showing the installation namespaceComponent status changes to &quot;SCALING&quot; with loading spinnerWait for installation to complete (status will change to &quot;ACTIVE&quot;)Component becomes available in the main Stack Components panel for configuration and management ","version":"Next","tagName":"h3"},{"title":"Finding Components by Category​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#finding-components-by-category","content":"Open the Apps Catalogue panelLook at the category filter menu in the left sidebarClick on a category name (e.g., &quot;Databases&quot;, &quot;Workflows&quot;, &quot;Visualization&quot;)The grid refreshes to show only components in that categoryClick &quot;All&quot; to remove category filter and see all components again ","version":"Next","tagName":"h3"},{"title":"Searching for Specific Tools​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#searching-for-specific-tools","content":"Open the Apps Catalogue panelType component name in the search field at top-leftResults filter in real-time as you typeCombine with category filter for more refined searchesClear search field to see all results again ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#related-features","content":"Stack Components - Manage installed stack components (view details, pause/activate, configure, uninstall)Platform Apps Management - Configure installed applications with Helm values and custom settingsNamespace Management - View and manage Kubernetes namespaces where components are deployed ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Apps Catalogue (Stack Components Directory)","url":"/developer-docs/features/apps-catalogue#notes--tips","content":"Installation Behavior: All catalogue installations automatically create namespaces with the pattern hyperplane-auto-{component-name}Component labels are automatically set as hyperplane.dev/app-name={component-name}Installation success messages include the target namespace for referenceComponents marked as &quot;Shakudo Core&quot; cannot be uninstalled as they are essential platform services Status Monitoring: The catalogue polls for updates every 5 seconds to keep installation states currentDuring installation (SCALING state), the install button shows a loading spinner and is disabledACTIVE components can be paused/scaled down from the main Stack Components panelIf a component shows UNKNOWN or UNAVAILABLE status, contact Shakudo support Search &amp; Filter Best Practices: Use search for specific tool names (e.g., &quot;PostgreSQL&quot;, &quot;Airflow&quot;)Use category filter to discover tools by function (e.g., all databases, all ML tools)Combine search and filter for precise results (e.g., search &quot;flow&quot; within &quot;Workflows&quot; category)Category list is dynamic and only shows categories present in the current catalogue Permissions: Only users with dashboard-admin role can access the Apps CatalogueThe selfInstalledAppsEnabled feature flag must be enabled at the platform levelInstallation permissions are checked at the component level (some may require additional permissions)Components in &quot;Archived&quot; state require manual intervention from Shakudo support to install Component Lifecycle: CATALOGUED → SCALING (installation in progress) → ACTIVE (running)ACTIVE → PAUSING (scaling down) → PAUSED (scaled to zero)PAUSED → SCALING (scaling up) → ACTIVE (running again)Archived components cannot be installed through this interface (require manual configuration) ","version":"Next","tagName":"h2"},{"title":"Authorization Policies","type":0,"sectionRef":"#","url":"/developer-docs/features/authorization-policies","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#overview","content":"The Authorization Policies panel provides a centralized view of all Kubernetes authorization policies configured in the Shakudo platform. This feature allows administrators to review and understand the access control rules that govern how users and services interact with platform resources. The panel displays AI-generated descriptions for each policy, making it easier to understand complex authorization configurations without diving into raw Kubernetes manifests. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#access--location","content":"Route: ?panel=authorization-policiesNavigation: Admin → Authorization PoliciesAccess Requirements: dashboard-admin role (Keycloak RBAC) Feature Flags: authPoliciesEnabled (environment variable: HYPERPLANE_DASHBOARD_AUTH_POLICIES_ENABLED) ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"View Authorization Policies​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#view-authorization-policies","content":"Browse all authorization policies configured in the Kubernetes cluster with AI-generated human-readable descriptions that explain what each policy does and which resources it affects. ","version":"Next","tagName":"h3"},{"title":"Refresh Policy Descriptions​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#refresh-policy-descriptions","content":"Manually trigger a refresh of authorization policy data to ensure you're viewing the most up-to-date information from the cluster. The system uses AI to regenerate descriptions when policies are updated. ","version":"Next","tagName":"h3"},{"title":"Review Policy Details​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#review-policy-details","content":"Expand individual policies to view detailed markdown-formatted descriptions that explain the policy's purpose, scope, and affected resources in natural language. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#main-view","content":"The main view displays a summary card showing: Last Modified: The timestamp of the most recently updated authorization policyPolicy Count: Total number of authorization policies in the systemRefresh Button: Manually refresh policy data and regenerate AI descriptions Below the summary, policies are displayed as expandable accordion items, each showing: Policy nameCollapsible description section with markdown-formatted details ","version":"Next","tagName":"h3"},{"title":"Summary Card​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#summary-card","content":"A prominent card at the top of the panel displays: Last modification timestamp in the format: &quot;ddd, MMMM D YYYY HH:mm:ss&quot;Total count of authorization policiesA refresh button to reload policy data ","version":"Next","tagName":"h3"},{"title":"Policy List​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#policy-list","content":"Authorization policies are displayed as accordion items that can be expanded to reveal: Policy name with an icon indicatorFull markdown description explaining the policy's purpose and scopeDetails about resources affected by the policy ","version":"Next","tagName":"h3"},{"title":"Loading States​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#loading-states","content":"The interface provides clear feedback during data loading: Skeleton loaders for summary card metrics during initial loadLoading spinner with message: &quot;AI is generating descriptions. It might take a while&quot;Empty state message: &quot;There are no Authorization Policies to display&quot;Error alert if policies fail to load ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#dialogs--modals","content":"This panel does not include any dialogs or modals. All functionality is accessible from the main view. ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#tables--data-grids","content":"This panel does not use traditional data tables. Instead, it displays policies as expandable accordion items for better readability of complex policy descriptions. ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#graphql-operations","content":"Queries: getAuthorizationPolicies - Retrieves all authorization policies with their names, AI-generated descriptions, and timestamps Mutations:None - This is a read-only panel Subscriptions:None - Data is fetched on demand ","version":"Next","tagName":"h3"},{"title":"Data Model​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#data-model","content":"Each AuthorizationPolicy contains: name (String) - The name of the authorization policydescription (String, optional) - AI-generated markdown description of what the policy doestimestamp (DateTime) - When the policy was last modified ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#component-structure","content":"Main Component: components/AuthPolicies/AuthPoliciesPanel.tsxGrid Component: components/AuthPolicies/AuthPoliciesGrid.tsxDetails Component: components/AuthPolicies/AuthPoliciesDetails.tsxGraphQL Query: graphql/authorizationPolicies/getAuthorizationPolicies.query.graphql ","version":"Next","tagName":"h3"},{"title":"Implementation Details​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#implementation-details","content":"The panel uses Apollo Client's useQuery hook with notifyOnNetworkStatusChange to track loading statesPolicy descriptions are rendered using the MarkdownDisplay component for rich formattingThe component is memoized for performance optimizationTimestamps use dayjs for formatting with timezone supportThe last modified date is computed from the maximum timestamp across all policies ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Reviewing Authorization Policies​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#reviewing-authorization-policies","content":"Navigate to Admin → Authorization PoliciesView the summary card to see total policy count and last update timeScroll through the list of policiesClick on any policy accordion to expand and read its AI-generated descriptionUse the refresh button if you've recently made changes to policies ","version":"Next","tagName":"h3"},{"title":"Understanding a Specific Policy​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#understanding-a-specific-policy","content":"Locate the policy by name in the accordion listClick on the policy to expand its descriptionReview the markdown-formatted explanation of: What the policy controlsWhich resources are affectedWhich users or services the policy applies to ","version":"Next","tagName":"h3"},{"title":"Checking for Policy Updates​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#checking-for-policy-updates","content":"Note the &quot;Last Modified&quot; timestamp in the summary cardIf you've recently modified Kubernetes authorization policies, click the refresh buttonWait for the AI to regenerate descriptions (this may take a few moments)Review the updated policy information ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#related-features","content":"Users - Overall cluster configuration and managementService Accounts - Related security features in the platformSecOps Panel - Security operations and compliance monitoring ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"AI-Generated Descriptions​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#ai-generated-descriptions","content":"Descriptions are generated by AI to make complex Kubernetes authorization policies more understandableThe AI analyzes the policy configuration and creates natural language explanationsDescription generation may take some time, especially when refreshing many policiesIf descriptions seem outdated after making changes, use the refresh button ","version":"Next","tagName":"h3"},{"title":"Performance Considerations​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#performance-considerations","content":"The panel fetches all policies at once, so loading time may increase with many policiesPolicy descriptions are cached after initial generationThe refresh operation regenerates all descriptions, which can be time-consuming ","version":"Next","tagName":"h3"},{"title":"When to Use This Panel​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#when-to-use-this-panel","content":"During security audits to review all authorization policiesWhen onboarding new administrators who need to understand access controlsAfter making changes to Kubernetes authorization policiesWhen troubleshooting permission issues to understand what policies are in effect ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#troubleshooting","content":"If you see an error message, try refreshing the pageContact platform support if the issue persistsCheck that the HYPERPLANE_DASHBOARD_AUTH_POLICIES_ENABLED environment variable is set to trueVerify you have dashboard-admin role assigned in Keycloak ","version":"Next","tagName":"h3"},{"title":"Feature Flag Configuration​","type":1,"pageTitle":"Authorization Policies","url":"/developer-docs/features/authorization-policies#feature-flag-configuration","content":"This feature must be explicitly enabled via environment variable: HYPERPLANE_DASHBOARD_AUTH_POLICIES_ENABLED=true  Without this flag, the panel will not appear in the admin navigation menu. ","version":"Next","tagName":"h3"},{"title":"Alert Targets","type":0,"sectionRef":"#","url":"/developer-docs/features/alert-targets","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#overview","content":"Alert Targets is a notification management system that allows users to configure and manage external notification channels for receiving alerts about workload failures and cluster events in the Shakudo platform. Users can set up integrations with popular communication platforms (Slack, PagerDuty, Mattermost) or email groups to receive real-time notifications when their jobs, microservices, or scheduled jobs encounter errors or when critical cluster events occur (e.g., stack component failures or PVC capacity issues). ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#access--location","content":"Route: ?panel=alert-targetsNavigation: Shakudo Objects → Alert TargetsAccess Requirements: View: All authenticated users can view their own alert targetsCreate: Dashboard Admin or Dashboard Maintainer rolesDelete: Dashboard Admin role (users can delete their own targets)View All: Dashboard Admin or Dashboard Maintainer roles can view all alert targets across users Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Create Alert Targets​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#create-alert-targets","content":"Configure notification channels to receive alerts about workload failures. Supports multiple integration types: Slack: Via webhook URL or bot token with channel specificationPagerDuty: Via integration key for incident managementEmail Groups: Comma-separated list of email addressesMattermost: Via incoming webhook URL ","version":"Next","tagName":"h3"},{"title":"Manage Alert Subscriptions​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#manage-alert-subscriptions","content":"Subscribe alert targets to receive cluster-wide notifications about critical infrastructure events, including stack component failures and persistent volume capacity alerts. This is controlled via the &quot;Receive Cluster Alerts&quot; toggle on each target. ","version":"Next","tagName":"h3"},{"title":"Associate with Workloads​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#associate-with-workloads","content":"Alert targets can be attached to immediate jobs, scheduled jobs, and microservices during workload creation or editing. When a workload fails, all associated alert targets receive notifications. ","version":"Next","tagName":"h3"},{"title":"Monitor Target Usage​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#monitor-target-usage","content":"View which workloads are using each alert target through detailed dialog that shows all associated immediate jobs, scheduled jobs, and microservices with filtering and search capabilities. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#main-view","content":"The main panel displays a data grid showing all alert targets with the following information: Name: Clickable link to view detailed information about the targetType: Visual indicator of the integration type (Slack, PagerDuty, Email Groups, Mattermost)Created On: Timestamp of when the target was createdReceive Cluster Alerts: Toggle switch to enable/disable cluster-wide alert subscriptionsActions: Delete button (visible only for targets owned by the user or for admins) The grid supports: Column visibility managementSorting by various fieldsManual refresh via toolbar buttonPagination (20 items per page) ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#dialogs--modals","content":"Create Alert Target Dialog Purpose: Configure a new notification channelFields: Target Channel: Dropdown to select integration type (Slack, PagerDuty, Email Groups, Mattermost)Alert Target Name: User-friendly name for the targetType-specific fields: Slack: Webhook URL OR (Bot Token + Channel Name)PagerDuty: Integration KeyEmail Groups: Comma-separated email addressesMattermost: Incoming webhook URL Actions: Cancel or CreateValidation: Email format validation for email groups, required field validation Detail Alert Target Dialog Purpose: View comprehensive information about an alert target and its associated workloadsSections: Target Information: Shows integration type with icon and specific details (email addresses for email targets, channel name for Slack)Workload Type Selector: Dropdown to filter by workload type (Immediate Jobs, Scheduled Jobs, Microservices)Associated Workloads Table: Data grid showing all workloads using this target Actions: Close buttonTable Features: Column management, quick search, pagination (10 items per page) Delete Alert Target Dialog Purpose: Confirm deletion of an alert targetPrompt: Confirmation message with target name/IDActions: Cancel or DeleteNote: System-created targets cannot be deleted ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#tables--data-grids","content":"Main Alert Targets Table Columns: Actions, Name, ID (hidden), Type, Created On, Receive Cluster AlertsActions: Click name to view details, toggle cluster alerts, delete targetFiltering: By cluster alert subscription statusFeatures: Column customization, sorting by most fields Associated Workloads Table (in Detail Dialog) Columns: Name, Logs, Status, Env Config, ID (hidden), Pipeline Path (hidden), StartActions: Click name to navigate to workload panel with filter appliedView logs in Grafana (for running workloads)View events and logs (for in-progress workloads) Filtering: By workload type (immediate jobs, scheduled jobs, microservices)Features: Quick search, column customization, manual refresh ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#graphql-operations","content":"Queries: getTargetNotifications - Retrieves all alert targets for the current user (or all targets for admin users). Includes target details like name, type, email/channel info, creation date, and cluster alert subscription status. Mutations: createJobNotificationTarget - Creates a new alert target with specified integration type and configuration. Sensitive data (API keys, tokens, webhook URLs) is base64-encoded before storage.deleteJobNotificationTarget - Permanently deletes an alert target by ID.setNotificationGlobal - Updates the cluster alert subscription status (isGlobalReceiver field) for a specific alert target. Subscriptions: None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#component-structure","content":"Main Component: /components/AlertTargets/AlertTargetsPanel.tsxDialogs: /components/AlertTargets/CreateAlertTargetDialog.tsx/components/AlertTargets/DetailAlertTargetDialog.tsx/components/AlertTargets/DeleteAlertTargetDialog.tsx Supporting Components: /components/AlertTargets/DetailAlertTargetTableButton.tsx - Name button that opens detail dialog/components/AlertTargets/AlertTargetAutocomplete.tsx - Autocomplete component for selecting alert targets/components/AlertTargets/utils.tsx - Helper functions for mapping target types to display names and icons ","version":"Next","tagName":"h3"},{"title":"Integration Types​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#integration-types","content":"The feature supports four notification target types defined in the Prisma schema: SLACK - Slack workspace integrationPAGERDUTY - PagerDuty incident managementEMAIL - Email distribution listsMATTERMOST - Mattermost team chat ","version":"Next","tagName":"h3"},{"title":"Security Considerations​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#security-considerations","content":"Sensitive authentication data (webhook URLs, API tokens, integration keys) is base64-encoded before being sent to the backendUsers can only view and delete their own alert targets by defaultAdmin users (Dashboard Admin, Dashboard Maintainer) can view all alert targets across the organizationOnly Dashboard Admin users can delete any alert target ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Create a Slack Alert Target​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#create-a-slack-alert-target","content":"Click &quot;Create Alert Target&quot; button in the main panelSelect &quot;Slack&quot; from the Target Channel dropdownEnter a descriptive name for the alert targetChoose integration method: Option A: Paste Slack Webhook URLOption B: Enter Slack Bot Token (xoxb-...) and Channel Name Click &quot;Create&quot; to save the configuration ","version":"Next","tagName":"h3"},{"title":"Subscribe Target to Cluster Alerts​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#subscribe-target-to-cluster-alerts","content":"Locate the alert target in the main tableFind the &quot;Receive Cluster Alerts&quot; columnToggle the switch to &quot;on&quot; (blue) to enable cluster alert notificationsThe target will now receive alerts for stack component failures and PVC capacity issues ","version":"Next","tagName":"h3"},{"title":"View Workloads Using an Alert Target​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#view-workloads-using-an-alert-target","content":"Click on an alert target name in the main tableIn the detail dialog, use the &quot;Workload Type&quot; dropdown to filter by: Immediate JobsScheduled JobsMicroservices View the list of workloads with status, environment config, and logsClick on a workload name to navigate to its details panel ","version":"Next","tagName":"h3"},{"title":"Attach Alert Target to a Workload​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#attach-alert-target-to-a-workload","content":"When creating or editing a job/service, find the &quot;Alert Targets&quot; sectionUse the autocomplete field to search and select one or more alert targetsSave the workload configurationThe selected targets will receive notifications when the workload fails ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#related-features","content":"Immediate Jobs - Can be configured to use alert targets for failure notificationsScheduled Jobs - Can be configured to use alert targets for failure notificationsMicroservices - Can be configured to use alert targets for failure notificationsStack Components - Failures trigger notifications to targets with cluster alerts enabled ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Best Practices​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#best-practices","content":"Use descriptive names for alert targets that indicate their purpose (e.g., &quot;Engineering Team Slack&quot;, &quot;Critical Alerts - PagerDuty&quot;)For production workloads, consider using multiple alert targets (e.g., both Slack and email) for redundancyEnable cluster alerts only for targets that should receive infrastructure-related notifications ","version":"Next","tagName":"h3"},{"title":"Email Group Configuration​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#email-group-configuration","content":"Provide comma-separated email addresses without spaces: user1@example.com,user2@example.comEmail validation ensures proper format before savingNo configuration of SMTP server is required - the platform handles email delivery ","version":"Next","tagName":"h3"},{"title":"Slack Integration Options​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#slack-integration-options","content":"Webhook URL: Simpler setup, posts to a single predetermined channelBot Token + Channel: More flexible, allows specifying the channel in the dashboard and can be reused across multiple alert targets ","version":"Next","tagName":"h3"},{"title":"PagerDuty Integration​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#pagerduty-integration","content":"Requires a PagerDuty integration key from your PagerDuty serviceEach alert target can route to a different PagerDuty service/escalation policyAlerts are sent as incidents with appropriate severity levels ","version":"Next","tagName":"h3"},{"title":"Limitations​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#limitations","content":"System-created alert targets (createdBySystem flag) cannot be deleted through the UIAlert targets created by other users are only visible to Dashboard Admin and Dashboard Maintainer rolesCluster alert subscription requires create permissions (Dashboard Admin or Maintainer roles) ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Alert Targets","url":"/developer-docs/features/alert-targets#troubleshooting","content":"If notifications are not being received, verify the integration credentials are correctFor Slack bot tokens, ensure the bot has been added to the target channelFor email targets, check spam/junk folders if notifications are missingSystem-wide alert target issues should be reported to platform administrators ","version":"Next","tagName":"h3"},{"title":"Billing Projects","type":0,"sectionRef":"#","url":"/developer-docs/features/billing-projects","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#overview","content":"The Billing Projects feature allows administrators to create and manage billing project identifiers that can be associated with various resources in the Shakudo platform. Billing projects provide a way to organize and track resources (sessions, jobs, and services) for accounting, cost allocation, and reporting purposes. Each billing project has a unique name and ID that can be referenced when creating platform resources, enabling organizations to attribute resource usage to specific projects, teams, or cost centers. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#access--location","content":"Route: ?panel=billing-projectsNavigation: Admin → Billing ProjectsAccess Requirements: View: All authenticated users can view their own billing projectsView All: dashboard_admin and dashboard_maintainer roles can view all billing projectsCreate: dashboard_admin role requiredDeactivate: dashboard_admin role or project owner Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Create Billing Projects​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#create-billing-projects","content":"Create new billing project identifiers that can be assigned to platform resources. Project names must follow specific naming conventions (alphanumeric characters with dashes, underscores, and dots) and must be unique within the platform. ","version":"Next","tagName":"h3"},{"title":"Deactivate Billing Projects​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#deactivate-billing-projects","content":"Soft-delete billing projects that are no longer needed. Deactivated projects remain in the system for historical tracking but cannot be used for new resource assignments. Only administrators or the project creator can deactivate a billing project. ","version":"Next","tagName":"h3"},{"title":"Filter Resources by Billing Project​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#filter-resources-by-billing-project","content":"Navigate directly from a billing project to view all associated resources (sessions, immediate jobs, scheduled jobs, and services) filtered by that project ID. This provides quick access to understand which resources are attributed to each billing project. ","version":"Next","tagName":"h3"},{"title":"Search and Filter​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#search-and-filter","content":"Search across billing project properties including name, ID, creator email, creation date, and deactivation date. The search functionality supports partial matches and date-based filtering. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#main-view","content":"The main panel displays a data grid table showing all billing projects the user has access to. Active projects are shown by default, ordered with active projects first, then by creation date (newest first). Each row includes action buttons for deactivation and a dropdown menu for filtering related resources. ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#dialogs--modals","content":"Create Billing Project Dialog Purpose: Create a new billing project with a unique nameFields: Name (required): Alphanumeric string with dashes, underscores, and dots allowed Validation Rules: Must begin and end with alphanumeric charactersCannot contain spacesMaximum length of 128 charactersName must be unique across all billing projectsFollows pattern: /^[a-z0-9A-Z\\.\\-\\_\\s]+$/g Actions: Cancel or Create Deactivate Billing Project Dialog Purpose: Confirm deactivation of a billing projectContent: Displays project name and confirmation messageActions: Cancel or Deactivate ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#tables--data-grids","content":"Billing Projects Table Columns: Actions (empty header): Deactivate button and resource filter dropdown menuName: Project name with copy-to-clipboard functionality, displays &quot;deactivated&quot; chip for inactive projectsID: Project ID with copy-to-clipboard functionalityCreated By: Email address of the user who created the projectCreated On: Creation timestamp (format: YYYY-MM-DD HH:mm:ss)Deactivated On (hidden by default): Deactivation timestamp Row Actions: Deactivate: Soft-deletes the project (visible only for active projects)View Menu: Dropdown to filter sessions, immediate jobs, scheduled jobs, or services by this project Filtering: Search bar filters across all columns including ID, name, creator email, and datesVisual Styling: Deactivated projects are visually distinguished with altered row stylingToolbar: Column visibility toggle and refresh button ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#graphql-operations","content":"Queries: getBillingProjects - Retrieves list of billing projects with optional filtering by user ID and active status. Results are ordered by active status (descending) then creation date (descending)getBillingProjectByName - Checks if a billing project name already exists (used for validation during creation) Mutations: createBillingProject - Creates a new billing project with name, creator user ID, and creator email. Returns the created project with all fieldsdeactivateBillingProject - Sets the active field to false for a specified billing project ID. Does not delete the project from the database Subscriptions: None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#component-structure","content":"Main Component: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/BillingProjects/BillingProjectsPanel.tsxDialogs: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/BillingProjects/BillingProjectsCreateDialog.tsx/root/gitrepos/monorepo/apps/hyperplane-dashboard/components/BillingProjects/BillingProjectsDeactivateDialog.tsx Supporting Components: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/BillingProjects/BillingProjectTableMenu.tsx Custom Hook: /root/gitrepos/monorepo/apps/hyperplane-dashboard/hooks/useBillingProjects.tsRole Configuration: /root/gitrepos/monorepo/apps/hyperplane-dashboard/constants/Roles/billing-projects.ts ","version":"Next","tagName":"h3"},{"title":"Role-Based Access Control​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#role-based-access-control","content":"The feature implements granular RBAC: Email Filter: Only dashboard_admin and dashboard_maintainer roles can view all projects; other users see only their own projectsCreate Action: Restricted to dashboard_admin roleDeactivate Action: Available to dashboard_admin role or the user who created the project ","version":"Next","tagName":"h3"},{"title":"Data Model​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#data-model","content":"Billing projects include the following fields: id: Unique identifier (string)name: Project name (string, max 128 characters)active: Boolean flag indicating if project is activehyperplaneUserId: ID of the user who created the projecthyperplaneUserEmail: Email of the user who created the projectcreationDate: Timestamp when project was createddeactivationTime: Timestamp when project was deactivated (null if active) ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Creating a New Billing Project​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#creating-a-new-billing-project","content":"Navigate to Admin → Billing Projects panelClick the &quot;Create Billing Project&quot; button (requires admin role)Enter a unique project name following the validation rulesClick &quot;Create&quot; to save the billing projectThe new project appears in the table and can be immediately used for resource assignment ","version":"Next","tagName":"h3"},{"title":"Associating Resources with Billing Projects​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#associating-resources-with-billing-projects","content":"When creating a session, job, or service, select the billing project from the dropdownThe resource will be tagged with the billing project IDUse the billing project table menu to view all resources associated with a specific project ","version":"Next","tagName":"h3"},{"title":"Viewing Resources by Billing Project​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#viewing-resources-by-billing-project","content":"In the Billing Projects table, click the dropdown menu (chevron icon) on any project rowSelect the resource type to filter: Sessions, Immediate Jobs, Scheduled Jobs, or ServicesThe dashboard navigates to the respective panel with filters applied to show only resources associated with that billing project ","version":"Next","tagName":"h3"},{"title":"Deactivating a Billing Project​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#deactivating-a-billing-project","content":"Locate the project in the Billing Projects tableClick the deactivate icon (HighlightOff) next to the project nameConfirm the deactivation in the dialogThe project is marked as inactive and displays a &quot;deactivated&quot; chipDeactivated projects cannot be used for new resource assignments but remain visible for historical tracking ","version":"Next","tagName":"h3"},{"title":"Searching for Billing Projects​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#searching-for-billing-projects","content":"Use the search bar at the top of the tableEnter search terms matching name, ID, creator email, or dateResults filter automatically with a 300ms debounceClear the search by clicking the X icon in the search fieldClick &quot;Refresh&quot; to reload data with current filters ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#related-features","content":"Sessions - Data science environments can be tagged with billing project IDsImmediate Jobs - Pipeline jobs can be associated with billing projects for cost trackingScheduled Jobs - Recurring pipeline jobs can be assigned billing project identifiersServices (Microservices) - Deployed services can be tagged with billing projects ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Billing Projects","url":"/developer-docs/features/billing-projects#notes--tips","content":"Billing project names cannot contain spaces - use dashes, underscores, or dots as separatorsProject names must start and end with alphanumeric charactersOnce created, billing project names cannot be changed - only deactivation is supportedDeactivated projects remain in the system permanently for audit and historical tracking purposesUse descriptive naming conventions that align with your organization's cost center or project structureThe dropdown menu in each table row provides quick navigation to view all resources associated with that projectSearch functionality supports date formats when filtering by creation or deactivation datesCopy-to-clipboard buttons are provided for both project names and IDs for easy referenceWhen a billing project is deactivated, existing resources retain their association but new resources cannot be created with that project IDRegular users can only see and manage their own billing projects unless they have admin or maintainer rolesThe table displays active projects by default, but the column visibility toggle allows viewing deactivation dates for historical analysis ","version":"Next","tagName":"h2"},{"title":"Cloud SQL Proxy","type":0,"sectionRef":"#","url":"/developer-docs/features/cloud-sql-proxy","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#overview","content":"The Cloud SQL Proxy feature enables users to create and manage Google Cloud SQL proxy connections within the Shakudo platform. This feature allows users to configure secure connections to Google Cloud SQL instances by setting up proxy configurations with optional authentication via service account credentials. The proxy acts as an intermediary that facilitates secure connections to Cloud SQL databases from within the Kubernetes cluster. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#access--location","content":"Route: ?panel=cloud-sql-proxyNavigation: Shakudo Objects → Cloud SQL ProxiesAccess Requirements: View: All authenticated users can view their own Cloud SQL proxiesView All: dashboard-admin or dashboard-maintainer roles required to view all proxies across the organizationCreate: All authenticated users can create Cloud SQL proxiesDeactivate: dashboard-admin role required (users can only deactivate their own proxies) Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Create Cloud SQL Proxy​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#create-cloud-sql-proxy","content":"Configure a new Cloud SQL proxy connection with customizable settings including: Custom proxy name and descriptionOne or more Cloud SQL instance connection stringsOptional custom container image URL (must support wget, Cloud SQL Proxy v2+ only)Optional service account credentials file nameOptional secret reference for authentication (supports both workloads and development namespaces) ","version":"Next","tagName":"h3"},{"title":"List and Search Proxies​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#list-and-search-proxies","content":"View all configured Cloud SQL proxies with searchable and sortable table display showing: Proxy ID, name, and descriptionCustom image URL (if specified)Credentials file name (if specified)Creator emailCopy-to-clipboard functionality for all key fields ","version":"Next","tagName":"h3"},{"title":"Deactivate Proxy​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#deactivate-proxy","content":"Permanently remove a Cloud SQL proxy configuration. This action: Requires admin privileges or ownership of the proxyShows confirmation dialog before deletionCannot be undone once confirmed ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#main-view","content":"The main panel displays a data grid table showing all Cloud SQL proxies the user has permission to view. The table includes: Search/filter bar with real-time filtering across all proxy propertiesColumn visibility controlsRefresh button to reload dataCreate button in the top-right cornerRows are styled based on activation status ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#dialogs--modals","content":"Create Cloud SQL Proxy Dialog Purpose: Full-screen form for creating new Cloud SQL proxy configurationsTabs: General: Basic proxy information and authentication settings Name (required, alphanumeric with dashes/underscores/dots, max 128 chars)Description (optional)Image URL (optional, overrides default Cloud SQL proxy image)File Name (optional, service account key file name for --credentials-file)Secret (optional, select from existing Hyperplane secrets in workloads or development namespaces) Instances: Cloud SQL instance connection strings At least one instance requiredDynamic field array to add multiple instances Live summary panel on the right showing all configured settingsActions: Create Cloud SQL Proxy button (validates form before submission) Deactivate Cloud SQL Proxy Dialog Purpose: Confirmation dialog for deleting a proxyFields: Shows proxy name in confirmation messageActions: Deactivate (confirm) or Cancel ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#tables--data-grids","content":"Cloud SQL Proxies Table Columns: Actions (deactivate icon button)ID (6-character short ID with copy button)Name (truncated to 30 chars with copy button)Image URL (copy button if specified)File Name (copy button if specified)Created By (creator's email)Description (truncated to 64 chars) Actions: Row-level deactivate button (role-based access)Copy to clipboard for ID, name, image URL, and file name Filtering: Real-time search across ID, name, description, and user ID fieldsSorting: Default sort by name (ascending) ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#graphql-operations","content":"Queries: getHyperplaneCloudSqlProxies - Retrieves list of Cloud SQL proxies with optional filtering and pagination Returns: id, name, instances, description, imageUrl, hyperplaneUserEmail, hyperplaneSecretId, fileNameSupports where clause filtering by user, name, description, etc.Orders results by name (ascending) Mutations: createHyperplaneCloudSqlProxy - Creates a new Cloud SQL proxy configuration Required inputs: name, instances (array)Optional inputs: description, imageUrl, fileName, hyperplaneUserEmail, hyperplaneSecretName, hyperplaneSecretNamespaceReturns: id, name, instances, description, imageUrl, hyperplaneUserEmail, fileName deleteHyperplaneCloudSqlProxy - Permanently deletes a Cloud SQL proxy Required input: idReturns: id, name of deleted proxy Subscriptions: None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#component-structure","content":"Main Component: components/CloudSqlProxy/CloudSqlProxyPanel.tsxTable View: components/CloudSqlProxy/CloudSqlProxyTables.tsxDialogs: components/CloudSqlProxy/Dialogs/ CloudSqlProxyCreateDialog.tsx - Full-screen creation formCloudSqlProxyDeactivateDialog.tsx - Deletion confirmation Hooks: hooks/useCloudSqlProxys.tsGraphQL: graphql/cloudsqlproxy/ ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Creating a Cloud SQL Proxy with Default Settings​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#creating-a-cloud-sql-proxy-with-default-settings","content":"Navigate to Shakudo Objects → Cloud SQL ProxiesClick &quot;Create Cloud SQL Proxy&quot; buttonEnter a unique name (alphanumeric with dashes/dots/underscores)Optionally add a descriptionSwitch to &quot;Instances&quot; tabAdd one or more Cloud SQL instance connection stringsReview the summary panel on the rightClick &quot;Create Cloud SQL Proxy&quot;System validates name availability and creates the proxySuccess notification appears and view returns to table ","version":"Next","tagName":"h3"},{"title":"Creating a Cloud SQL Proxy with Custom Authentication​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#creating-a-cloud-sql-proxy-with-custom-authentication","content":"Navigate to Shakudo Objects → Cloud SQL ProxiesClick &quot;Create Cloud SQL Proxy&quot; buttonEnter proxy name and descriptionSpecify a custom Image URL (must support wget, Cloud SQL Proxy v2+)Enter the File Name of your service account key in the secretSelect a Secret from the dropdown (choose namespace: Workloads or Development)Switch to &quot;Instances&quot; tab and add instance connection stringsReview configuration in the summary panelClick &quot;Create Cloud SQL Proxy&quot;System creates proxy with custom authentication settings ","version":"Next","tagName":"h3"},{"title":"Searching for Specific Proxies​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#searching-for-specific-proxies","content":"Navigate to Cloud SQL Proxies panelUse the filter input field at the top of the tableEnter search term (searches across ID, name, description, user email)Table updates in real-time with matching resultsClick &quot;X&quot; to clear filter and show all proxies ","version":"Next","tagName":"h3"},{"title":"Deactivating a Cloud SQL Proxy​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#deactivating-a-cloud-sql-proxy","content":"Locate the proxy in the tableClick the deactivate icon (circle with X) in the leftmost columnConfirmation dialog appears with proxy nameClick &quot;Deactivate&quot; to confirm deletionSuccess notification appears and proxy is removed from tableNote: Only admins or the proxy owner can perform this action ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#related-features","content":"Secrets - Manage service account credentials referenced by Cloud SQL proxiesService Accounts - Configure service accounts for authenticationPipeline Jobs - Can utilize Cloud SQL proxies for database connectionsMicroservices - Can access databases through configured Cloud SQL proxies ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Cloud SQL Proxy","url":"/developer-docs/features/cloud-sql-proxy#notes--tips","content":"Proxy Names: Must start and end with alphanumeric characters, can contain dashes, underscores, and dots in between, maximum 128 charactersDefault Image: If no custom image URL is provided, the system uses the default Google Cloud SQL Proxy imageCustom Images: When using a custom image URL, ensure the image includes wget and supports Cloud SQL Proxy v2 or laterService Account Files: The fileName field corresponds to the --credentials-file flag in Cloud SQL Proxy and should match a file within the selected secretPublic Instances: If connecting to a public Cloud SQL instance or using workload identity, the secret selection is optionalNamespace Selection: Secrets can be selected from either the &quot;Workloads&quot; (hyperplane-pipelines) or &quot;Development&quot; (hyperplane-jhub) namespaceInstance Format: Instance connection strings should follow the Google Cloud SQL connection format (project:region:instance)Role-Based Access: Standard users can only see and delete their own proxies; admins and maintainers can view all proxies across the organizationPermanent Deletion: Deactivating a Cloud SQL proxy is permanent and cannot be undoneName Validation: The system checks for name uniqueness before creating a new proxyReal-time Filtering: The search filter applies OR logic across multiple fields for flexible searching ","version":"Next","tagName":"h2"},{"title":"Container Images","type":0,"sectionRef":"#","url":"/developer-docs/features/container-images","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#overview","content":"The Container Images panel (also known as the Image Builder) enables users to build custom Docker container images from Dockerfiles stored in Git repositories. This feature integrates with Harbor registry to store built images and provides a complete workflow for creating, managing, and tracking container image builds. Users can build images from any branch or commit, tag them for organization, view build logs, and convert successful builds into Environment Configs for use in Shakudo workloads. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#access--location","content":"Route: ?panel=container-imagesNavigation: Shakudo Objects → Container ImagesAccess Requirements: None (available to all authenticated users)Feature Flags: harborImageBuilderEnabled - Harbor registry must be installed and active for this feature to be available ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Build Docker Images from Git​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#build-docker-images-from-git","content":"Create custom container images by specifying a Dockerfile path in a Git repository. The system builds the image using Harbor's in-cluster image builder and stores the resulting artifact in the Harbor registry. ","version":"Next","tagName":"h3"},{"title":"Tag and Version Management​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#tag-and-version-management","content":"Apply multiple tags to images for easy identification and versioning. Tags are displayed as chips throughout the interface and can be used to organize and track different versions of the same image. ","version":"Next","tagName":"h3"},{"title":"Clone Existing Builds​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#clone-existing-builds","content":"Duplicate existing image build configurations to quickly create new builds with minor modifications. All settings (image name, tags, Git repository, branch, commit, and Dockerfile path) are pre-populated from the source build. ","version":"Next","tagName":"h3"},{"title":"Convert to Environment Config​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#convert-to-environment-config","content":"Transform successful image builds directly into Environment Configs (EC), enabling the custom images to be used as base containers for Sessions, Jobs, and Microservices throughout the platform. ","version":"Next","tagName":"h3"},{"title":"Track Build Status​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#track-build-status","content":"Monitor the progress of image builds in real-time with status indicators (Building, Built, Failed, Local). View live logs during active builds and access historical logs through integrated Grafana links. ","version":"Next","tagName":"h3"},{"title":"Filter and Search​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#filter-and-search","content":"Apply filters by image name, digest, creation date, status, branch name, Git commit, and owner email. Quick search functionality allows searching across image names and digests simultaneously. ","version":"Next","tagName":"h3"},{"title":"Pin Important Images​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#pin-important-images","content":"Pin frequently used or important images to keep them at the top of the table for easy access. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#main-view","content":"The main panel displays a hierarchical data grid showing all container images grouped by image name. Each image name acts as a parent row that can be expanded to reveal historical builds of that image, sorted by creation date (newest first). The table supports pagination with 20 items per page and automatically refreshes every 30 seconds to show updated build statuses. Table Columns: Image Name (expandable group) - Shows the base image name with pin button and detail linkTag - Displays image tags as chips with a menu for viewing all tags when multiple existStatus - Visual status indicator (Building, Built, Failed, Local)Logs - Live log viewer for active builds and Grafana link for historical logsBuild Date - Timestamp of when the build was initiated (YYYY-MM-DD HH:mm:ss format)Git Commit ID - Copyable commit hash used for the buildSHA256 Hash - Copyable image digest identifierActions - Dropdown menu with available operations Toolbar Features: Filter button with comprehensive filtering dialogColumn visibility toggleRefresh buttonQuick search fieldReset Filters buttonActive filter chips with individual remove buttons ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#dialogs--modals","content":"Build Docker Image Dialog Purpose: Create a new container image build jobFields: Image Name (required) - Name for the container imageTags (required, multiple) - Press Enter to add each tagGit Repository (required, autocomplete) - Select from configured Git repositoriesBranch (required) - Git branch nameCommit (optional) - Specific commit hash, uses branch HEAD if omittedDockerfile Path (required, autocomplete) - Relative path from repository root to Dockerfile Actions: Cancel, Create (with loading indicator)Features: Includes tutorial component link for guidance Filter Dialog Purpose: Apply advanced filters to the image listFields: Image Name (text search)Image Digest (text search)Job ID (text search)Creation Date Range (start and end dates)Status (dropdown: Building, Built, Failed, Local)Branch Name (text)Git Commit (text)Owner Email (text) Actions: Apply Filters, Clear All Delete/Cancel Confirmation Dialog Purpose: Confirm deletion of built images or cancellation of active buildsContent: Displays image name and confirmation messageActions: Close, Delete/Cancel (with loading indicator) Image Build Job Details Panel Purpose: View comprehensive details about a specific buildContent: Image name, ID, and owner at the topStatus indicatorCreation timestampImage digest (copyable)Image tags (as chips)Full image URL for pulling (copyable)Dockerfile path (copyable)Git repository URL (copyable)Git branch name (copyable)Git commit ID (copyable) Actions: Open in Harbor (for completed builds)Historical Logs (Grafana link)Back button to return to table ","version":"Next","tagName":"h3"},{"title":"Actions Menu​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#actions-menu","content":"Each image build has an Actions dropdown menu with the following options: Open in Harbor (only for Built/Local images) Opens the Harbor registry interface showing the specific image artifactProvides access to security scanning, tags, and pull commands Clone Creates a duplicate build configurationOpens the Build Docker Image dialog pre-filled with all settingsAllows quick creation of similar images with modifications Convert To EC Transforms the image into an Environment ConfigNavigates to Environment Configs panel with pre-populated fieldsIncludes image URL, description, and readme (if available) Delete/Cancel For active builds (Building status): Cancels the build jobFor completed builds: Deletes both the build record and Harbor image artifactRequires confirmation before execution ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#graphql-operations","content":"Queries: getImageBuilderJobs - Retrieves paginated list of image builder jobs with filtering, ordered by pinned status then creation datecountImageBuilderJobs - Gets total count of jobs matching filter criteria for paginationsearchImageBuilderJobs - Full-text search across image builder jobs by search termGetImageBuildJobVcServerDocument - Fetches Git repository details for a specific build jobGetImageBuildJobDescriptionDocument - Retrieves description and readme metadata for EC conversiongetImageBuilderLogs - Fetches build logs for live log viewing Mutations: createImageBuilderJob - Initiates a new container image build with specified Git source and DockerfiledeleteImageandImageBuilderJob - Removes build job record and deletes corresponding Harbor image artifactupdateOneImageBuilderJob (via pinImageBuildJob) - Toggles pin status for a build job Subscriptions: None (uses polling with 30-second interval for status updates) ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#component-structure","content":"Main Component: /components/ImageBuilder/ImageBuilderPanel.tsxTable Component: /components/ImageBuilder/ImageBuilderTable.tsxCreate Dialog: /components/ImageBuilder/ImageBuilderCreate.tsxDetails Panel: /components/ImageBuilder/ImageBuildJobDetailDialog.tsxAction Menu: /components/ImageBuilder/ImageBuilderActionMenu.tsxFilter Dialog: /components/ImageBuilder/ImageBuildTableFilterDialog.tsxLive Log Viewer: /components/ImageBuilder/ImageBuilderLiveLog.tsxSearch Results: /components/Search/ContainerImagesSearchResults.tsx ","version":"Next","tagName":"h3"},{"title":"State Management (Jotai Atoms)​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#state-management-jotai-atoms","content":"ImageBuildJobsPanelSectionAtom - Controls view toggle between table and details ('table' | 'details')ImageBuilderJobsTablePageAtom - Manages pagination state (current page number)ImageBuilderJobFiltersAtom - Stores active filter criteriaImageBuilderJobsDataLoadingAtom - Tracks loading state for UI feedbackJobsDetailsValueAtom - Holds currently selected job for details viewImageBuilderOpenCreateDialogAtom - Controls visibility of create dialogImageBuilderCloneDefaultValuesAtom - Stores values for cloning operations ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Building a New Container Image​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#building-a-new-container-image","content":"Click &quot;Create a Container Image&quot; button (disabled if Harbor is not installed)Enter an image name (e.g., &quot;custom-python-env&quot;)Add one or more tags by typing and pressing Enter (e.g., &quot;v1.0&quot;, &quot;latest&quot;)Select a Git repository from the dropdownSpecify the branch name (defaults to branch from repository config)Optionally specify a commit hash (uses branch HEAD if blank)Enter the relative path to your Dockerfile (e.g., &quot;./docker/Dockerfile&quot; or &quot;./Dockerfile&quot;)Click &quot;Create&quot; to start the build jobMonitor build progress in the table with live status updatesView live logs by clicking the log icon for active buildsOnce completed (status: Built), the image is available in Harbor ","version":"Next","tagName":"h3"},{"title":"Cloning and Modifying an Existing Image​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#cloning-and-modifying-an-existing-image","content":"Locate the image you want to clone in the tableClick the &quot;Actions&quot; dropdown button for that imageSelect &quot;Clone&quot; from the menuThe Build Docker Image dialog opens with all fields pre-filledModify any fields as needed (e.g., change tags, update commit, or adjust image name)Click &quot;Create&quot; to build the modified imageThe new build appears in the table as a separate entry ","version":"Next","tagName":"h3"},{"title":"Converting an Image to an Environment Config​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#converting-an-image-to-an-environment-config","content":"Locate a successfully built image (status: Built or Local)Click the &quot;Actions&quot; dropdown for that imageSelect &quot;Convert To EC&quot; from the menuYou are automatically redirected to the Environment Configs panelThe image URL is pre-populated in the EC creation formComplete any additional EC configuration (name, description, resource limits, etc.)Save the EC to make it available for Sessions, Jobs, and Microservices ","version":"Next","tagName":"h3"},{"title":"Filtering and Finding Images​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#filtering-and-finding-images","content":"Click the filter button in the table toolbarSet one or more filter criteria: Search by image name or digestFilter by build status (Building, Built, Failed)Specify creation date rangeFilter by Git branch or commitFilter by owner email Click &quot;Apply Filters&quot;Active filters appear as chips below the toolbarClick the X on any chip to remove that specific filterClick &quot;Reset Filters&quot; to clear all filters at onceUse the quick search field for real-time filtering across image names and digests ","version":"Next","tagName":"h3"},{"title":"Viewing Detailed Build Information​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#viewing-detailed-build-information","content":"Click on any image name in the table (appears as a blue link)The details panel slides in showing comprehensive build informationReview all metadata, Git source details, and image identifiersCopy any field value using the copy buttonClick &quot;Open in Harbor&quot; to view the image in Harbor's web interfaceClick &quot;Historical Logs&quot; to view complete build logs in GrafanaClick &quot;Back&quot; to return to the table view ","version":"Next","tagName":"h3"},{"title":"Deleting or Canceling Builds​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#deleting-or-canceling-builds","content":"For active builds, click Actions → Cancel to stop the build in progressFor completed builds, click Actions → Delete to remove the build record and Harbor artifactConfirm the action in the dialog that appearsThe image is removed from the table and deleted from Harbor registry ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#related-features","content":"Git Repositories - Configure Git repositories used as source for image buildsEnvironment Configs - Use built images as base containers for workloadsStack Components - Harbor registry component must be installed and activeSessions - Use custom Environment Configs with built images for development sessionsImmediate Jobs - Run workloads using custom container imagesMicroservices - Deploy microservices with custom-built images ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Container Images","url":"/developer-docs/features/container-images#notes--tips","content":"Harbor Dependency: The Container Images feature requires Harbor registry to be installed and in ACTIVE status. If Harbor is not available, the &quot;Create a Container Image&quot; button will be disabled with a tooltip explaining the requirement. Image Hierarchy: The table groups builds by image name, allowing you to see the history of builds for each image. Expand an image name to see all historical builds sorted by date (newest first). Tag Strategy: Use meaningful tags like version numbers (v1.0, v2.0) or environment indicators (dev, staging, prod) to organize your images. Multiple tags can be applied to the same build. Commit Specificity: Specifying a commit hash ensures reproducible builds. If omitted, the build uses the HEAD of the specified branch, which may change over time. Dockerfile Paths: The Dockerfile path is relative to the Git repository root. Common paths include ./Dockerfile, ./docker/Dockerfile, or ./path/to/custom.Dockerfile. Build Monitoring: Builds are automatically polled every 30 seconds for status updates. For real-time monitoring, use the live log viewer available for active builds. Image URLs: Built images are stored in Harbor at harbor.{domain}/in-cluster-image-builder/{imageName}@{digest}. Use this full URL when referencing the image in Environment Configs or direct Docker pulls. Security Scanning: After builds complete, Harbor automatically performs security vulnerability scanning on images. Access these reports through the &quot;Open in Harbor&quot; action. Pin for Quick Access: Pin frequently used or important images to keep them at the top of the table, making them easier to find when working with many images. EC Conversion Benefits: Converting images to Environment Configs allows them to be used throughout Shakudo with proper resource management, access control, and metadata tracking. Search Integration: Container images are indexed in the global platform search (Cmd/Ctrl+K). Search results show image name, tags, Dockerfile source, owner, and creation date. Filtering Persistence: Filters can be saved in the URL query parameters, allowing you to bookmark specific filtered views of your images. Role-Based Filtering: Users without elevated permissions automatically see only their own images. Administrators and users with appropriate roles can see all images across the platform. Tutorial Available: Click the video icon next to the panel title to watch a tutorial on using the Container Images feature. ","version":"Next","tagName":"h2"},{"title":"Data Stack Graph","type":0,"sectionRef":"#","url":"/developer-docs/features/data-stack-graph","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#overview","content":"The Data Stack Graph is a real-time network visualization feature that displays the data flow and connections between stack components (databases, message queues, etc.), Shakudo processes (jobs, sessions, microservices), and Kubernetes namespaces. It provides a visual representation of active network traffic and communication patterns within the platform, helping users understand how different components interact with each other. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#access--location","content":"Route: ?panel=data-stack-graphNavigation: Monitoring → Data Stack GraphAccess Requirements: NoneFeature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Real-time Network Monitoring​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#real-time-network-monitoring","content":"Visualizes active connections between stack components and Shakudo workloads with automatic updates every 5 seconds. The graph shows data flow direction with animated particles moving along connection lines to represent active traffic. ","version":"Next","tagName":"h3"},{"title":"Connection Discovery​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#connection-discovery","content":"Automatically detects and displays network connections between: Stack Components (Platform Apps like databases, message brokers, monitoring tools)Microservices (Deployed Shakudo services)Jobs (Pipeline workloads)Sessions (Development environments)Kubernetes Namespaces (Container orchestration layers) ","version":"Next","tagName":"h3"},{"title":"Standalone Component Detection​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#standalone-component-detection","content":"Identifies and lists components that have no active network connections, helping users understand which resources are isolated or potentially underutilized. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#main-view","content":"The panel consists of a two-column layout: Left Panel (Standalone Components) Lists components with no active connections detectedDisplays component icons and namesGroups multiple jobs/sessions with count indicators (e.g., &quot;Jobs (5)&quot;, &quot;Sessions (3)&quot;)Shows &quot;All components have active connections&quot; when no isolated components exist Right Panel (Interactive Network Graph) Force-directed graph visualization of connected componentsNodes represent different types of resources with distinctive icons: Platform Apps: Component-specific logosMicroservices: Shakudo microservice iconJobs: Pipeline workload iconSessions: Development environment iconNamespaces: Generic component icon Edges show directional data flow between nodesAnimated particles flow along edges representing active trafficNode highlights: Components with recent traffic show an orange glow effectAuto-zoom functionality to fit all nodes within view ","version":"Next","tagName":"h3"},{"title":"Visual Elements​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#visual-elements","content":"Node Types: Platform Apps &amp; Microservices: White background with component logoJobs: Pipeline icon on gray backgroundSessions: Development environment icon on gray backgroundNamespaces: Generic icon on gray background Edge Styling: Active connections: Gray lines (#808080) with animated particlesInactive connections: Light gray lines (#D9D9D9) with no animationTraffic indicators: Orange animated particles (#CF602980) flowing from source to target Status Indicators: Recent Activity: Orange glow/shadow around nodes that have had recent network trafficLast Update: Timestamp displayed at bottom right showing when data was last refreshed ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#dialogs--modals","content":"This feature does not include any dialogs or modals. All interactions occur within the main visualization. ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#tables--data-grids","content":"No tables are present. The feature uses a list view for standalone components and a force-graph visualization for connected components. ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#graphql-operations","content":"Queries: getNodesAndLinks - Retrieves visual node and edge data for the network graph Returns getVisualNodesAndEdges: Array of stack components and their connection informationReturns getVisualPodInfo: Array of pod/workload information including jobs, sessions, and microservices getActiveTrafficStatus - Checks for active network traffic status (defined but not actively used in current implementation) Mutations:None Subscriptions:None Polling: The getNodesAndLinks query polls every 5 seconds (5000ms) to maintain real-time updatesPolling starts when component mounts and stops on unmount ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#component-structure","content":"Main Component: /components/DataStackGraph/DataStackGraphPanel.tsxVisualization: /components/DataStackGraph/DataStackForceGraph.tsxState Management: /atoms/DataStackGraphAtoms.tsxGraphQL Queries: /graphql/data-stack-graph/ ","version":"Next","tagName":"h3"},{"title":"Technical Implementation Details​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#technical-implementation-details","content":"Force-Directed Graph Configuration: Link Distance: 200 pixels between nodesCharge Strength: -1000 (repulsion force between nodes)Distance Max: 500 pixels maximum interaction distanceCenter Strength: 0 (no centering force)Warmup Ticks: 250 iterations before renderingCooldown Ticks: 1000 iterations for stabilizationCooldown Time: 1000ms Node Rendering: Node size: 64 pixels (IMAGE_SIZE constant)Platform Apps and Microservices render on white backgroundOther node types render on gray (#D9D9D9) backgroundRecent traffic indicated by orange shadow/glow (#CF6029)Custom canvas rendering for each node type Link Rendering: Directional particles represent request flowParticle speed: 0.005 (slow animation for visibility)Particle width: 7 pixelsLink width: 2 pixelsNumber of particles is randomized (3-7 particles per active link) State Management: Uses Jotai atom (DataStackGraphLastUpdateDateAtom) to track last update timestampUpdate timestamp refreshed every 5 seconds via intervalGraph engine re-renders only when node list changes to optimize performance ","version":"Next","tagName":"h3"},{"title":"Data Processing​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#data-processing","content":"Node Filtering: Only nodes with active connections (present in link IDs) are displayed in the graphStandalone nodes are moved to the left panel listSessions and Jobs are grouped with counters when isolatedNamespace nodes are excluded from standalone list Node Mapping: Kubernetes namespaces renamed for clarity: hyperplane-pipelines → &quot;Workloads&quot;hyperplane-jhub → &quot;Development&quot; ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Understanding Component Connectivity​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#understanding-component-connectivity","content":"Open the Data Stack Graph panel from Monitoring navigationReview the network graph to see how components communicateObserve animated particles to understand data flow directionCheck the &quot;Standalone&quot; list to identify isolated components ","version":"Next","tagName":"h3"},{"title":"Monitoring Active Traffic​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#monitoring-active-traffic","content":"Look for nodes with orange glow indicating recent network activityObserve particle animation speed and density for traffic volume indicatorsCheck the last update timestamp to confirm data freshnessAllow graph to auto-refresh every 5 seconds for real-time monitoring ","version":"Next","tagName":"h3"},{"title":"Identifying Component Relationships​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#identifying-component-relationships","content":"Locate a specific component by its icon or hover over nodes to see namesFollow edge lines to trace data flow pathsIdentify which components serve as data sources vs. consumersUnderstand dependencies between microservices and backing services ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#related-features","content":"Distributed Workloads: Monitor job execution and resource usageStack Components: Manage deployment and configuration of platform applicationsMicroservices: Deploy and manage custom services that appear in the graphSessions: Development environments that may connect to stack components ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Data Stack Graph","url":"/developer-docs/features/data-stack-graph#notes--tips","content":"Performance Considerations: The graph automatically zooms to fit all nodes when first rendered or when node topology changesGraph stabilization takes approximately 1 second after data updatesPolling occurs every 5 seconds but component only re-renders when node list changes Visual Interpretation: Thicker particle streams indicate more active connections (randomized 3-7 particles per link)Gray edges without particles indicate configured but inactive connectionsNodes are automatically positioned by force-directed algorithm for optimal spacingGraph layout may shift slightly as new connections are discovered or removed Limitations: Particle count is randomized and does not represent actual request volumeThe feature displays network connectivity but not performance metricsHistorical data is not stored; only current connections are shownNo drill-down capability to see detailed request logs or metrics Best Practices: Use this feature to validate microservice deployment connectivityCheck for unexpected connections that might indicate misconfigurationMonitor the standalone list for resources that should be connected but aren'tCombine with other monitoring panels for comprehensive system health assessment Browser Requirements: Requires HTML5 Canvas support for graph renderingBest viewed on desktop browsers with adequate screen spaceResponsive layout adjusts for mobile devices but desktop recommended for clarity ","version":"Next","tagName":"h2"},{"title":"Datalake (Shakudo Data Lakehouse)","type":0,"sectionRef":"#","url":"/developer-docs/features/datalake","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#overview","content":"The Datalake panel provides a comprehensive interface for managing and exploring the Shakudo Data Lakehouse - an Apache Iceberg-based data lakehouse built on object storage. This feature enables users to browse S3 buckets, explore Iceberg catalogs and tables, query sample data, and interact with data through an integrated PySpark terminal. The lakehouse combines the flexibility of data lakes with the structure and ACID guarantees of data warehouses, using Apache Iceberg as the table format and supporting both Nessie and Postgres as catalog backends. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#access--location","content":"Route: ?panel=datalakeNavigation: Admin → DatalakeAccess Requirements: None (feature-flag gated)Feature Flags: datalakeEnabled ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Browse Object Storage​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#browse-object-storage","content":"Navigate through the S3-compatible object storage bucket that backs the data lakehouse. View folders and files, see metadata like size, object count, and last modified timestamps. The file browser provides a hierarchical view of the storage structure. ","version":"Next","tagName":"h3"},{"title":"Explore Iceberg Catalogs​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#explore-iceberg-catalogs","content":"View and interact with Apache Iceberg catalogs that organize tables into logical namespaces. Each catalog contains multiple tables with metadata about schema, partitions, and data files. Catalogs are grouped by name and display statistics about total tables and storage size. ","version":"Next","tagName":"h3"},{"title":"Query Sample Data​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#query-sample-data","content":"Preview data from Parquet files and Iceberg tables directly in the browser. Sample data can be viewed in both table format and JSON format, with configurable sample sizes (10, 20, 50, 100, 250, or 500 rows). This allows quick data validation without running full queries. ","version":"Next","tagName":"h3"},{"title":"Integrated PySpark Terminal​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#integrated-pyspark-terminal","content":"Access an interactive PySpark shell pre-configured with the lakehouse connection. Execute Spark SQL queries, perform data transformations, and interact with Iceberg tables programmatically. The terminal can be restarted to clear the session state. ","version":"Next","tagName":"h3"},{"title":"Configuration Management​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#configuration-management","content":"Access setup guides for configuring PySpark sessions and Nessie/Dremio integrations. These guides provide copy-paste ready code snippets with bucket names and connection strings pre-filled. ","version":"Next","tagName":"h3"},{"title":"Bytebase SQL Sandbox​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#bytebase-sql-sandbox","content":"For systems with Bytebase integration, users can open a SQL sandbox environment to run queries against the lakehouse using a web-based SQL editor. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#main-view","content":"The main panel displays: Header: Panel title &quot;Shakudo Data Lakehouse&quot; with an &quot;Advanced&quot; menu for accessing configuration dialogsBucket Details Card: Shows bucket name, last modified date, total objects, total size, and Postgres catalog connection statusNavigation: Breadcrumb-style path navigation showing current location in the folder hierarchyContent Area: Either displays the catalog list view or the file/folder browser depending on navigation state ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#dialogs--modals","content":"Interactive Shell Dialog Purpose: Provides a fullscreen PySpark terminal for interactive data queriesFeatures: Embedded iframe terminal, session restart button, clipboard access for copy/pasteActions: Execute Spark commands, restart session, close dialog Spark Session Setup Dialog Purpose: Displays markdown documentation for configuring PySpark sessionsFields: Pre-filled code examples with bucket name and Postgres URLActions: Copy code snippets, close dialog Nessie Dremio Setup Dialog Purpose: Displays markdown documentation for Nessie catalog configurationFields: Setup instructions and configuration examplesActions: Copy code snippets, close dialog Sample Data Viewer Dialog Purpose: Display sample rows from Parquet files or Iceberg tablesFields: Data table view, JSON view toggle, sample size selectorActions: Switch between table/JSON views, copy JSON to clipboard, close dialog ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#tables--data-grids","content":"Iceberg Catalogs Grid Display: Card-based grid layout grouped by catalog nameColumns: Table name, catalog name, namespace, object count, sizeActions: Click to explore catalog contents, view sample data, copy path, open in BytebaseFeatures: Expandable accordions for each catalog group Files and Folders Table Display: Striped table with alternating row colorsColumns: Name (with folder/file icons), Last Modified, Objects (for folders), Size, ActionsActions: Click folders to navigate, view sample data, copy path, copy code examples, download filesFeatures: Pagination (5/10/25/50 rows per page), hover highlighting for foldersFiltering: None (shows all contents of current path) ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#graphql-operations","content":"Queries: isDatalakeAvailable - Checks if the datalake feature is configured and availablegetDatalakeBucketDetails - Retrieves bucket metadata including name, size, file count, and last modifiedgetPostgresFullUrl - Gets the Postgres connection URL for the Iceberg cataloggetIcebergCatalogs - Lists all Iceberg catalogs with their tables and metadata locationsgetDatalakeBucketFilesAndFolders - Lists files and folders at a given S3 prefixgetDatalakeObjectDetails - Gets detailed metadata for a specific S3 object or folderisIcebergTable - Checks if a given path is an Iceberg tablegetSampleData - Retrieves sample rows from a Parquet file or Iceberg table Mutations:None - this is a read-only interface Subscriptions:None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#component-structure","content":"Main Component: components/Datalake/Panel.tsxCore Component: components/Datalake/Datalake.tsxBucket Details: components/Datalake/DatalakeBucketDetails.tsxCatalogs View: components/Datalake/Catalogs.tsxFile Browser: components/Datalake/DatalakePathFolderAndFiles.tsxCatalog Cards: components/Datalake/CatalogCard2.tsxDialogs: components/Datalake/DatalakeIntegratedTerminalDialog.tsxcomponents/Datalake/PySparkConfigDialog.tsxcomponents/Datalake/NessieConfigDialog.tsxcomponents/Datalake/IcebergTableSampleData.tsx Actions: components/Datalake/DataLakeTableObjectActions.tsx ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#state-management","content":"The component uses Jotai atoms for state management: BucketNameAtom - Stores the current bucket nameSubpathAtom - Tracks the current navigation path in the bucketDataLakePostgresFullUrlAtom - Stores the Postgres catalog connection URLLoadingFilesAndFolderAtom - Tracks loading state for file browserDataSampleSizeAtom - Controls the number of rows to fetch when sampling dataDataLakeCatalogSelected - Stores the currently selected catalog for exploration ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Browse Data Lakehouse Contents​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#browse-data-lakehouse-contents","content":"Navigate to Admin → Datalake panelView the main bucket details card showing overall statisticsClick on an Iceberg catalog in the catalogs list to explore its contentsNavigate through folders by clicking on folder rowsUse the breadcrumb navigation to return to parent folders or catalogs listClick &quot;Refresh&quot; button to update bucket statistics ","version":"Next","tagName":"h3"},{"title":"Query Sample Data from a Table​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#query-sample-data-from-a-table","content":"Browse to a catalog or folder containing dataLocate the table or Parquet file you want to previewClick the eye icon in the Actions columnWait for data to load (respects the sample size setting)Toggle between &quot;Table&quot; and &quot;JSON&quot; views using the toggle buttonsOptionally copy the JSON data to clipboardClose the dialog when finished ","version":"Next","tagName":"h3"},{"title":"Run Interactive Spark Queries​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#run-interactive-spark-queries","content":"Click the &quot;Advanced&quot; button in the panel headerSelect &quot;Interactive Shell&quot; from the dropdown menuWait for the PySpark terminal to load (shows iframe)Execute Spark SQL or PySpark commands in the terminalUse the restart button if you need to reset the sessionClose the dialog when finished ","version":"Next","tagName":"h3"},{"title":"Set Up PySpark Connection​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#set-up-pyspark-connection","content":"Click the &quot;Advanced&quot; button in the panel headerSelect &quot;Spark Session Setup&quot; from the dropdownReview the configuration documentation with pre-filled valuesCopy the code snippets to use in your own notebooks or scriptsClose the dialog when finished ","version":"Next","tagName":"h3"},{"title":"Access Bytebase SQL Sandbox​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#access-bytebase-sql-sandbox","content":"Ensure Bytebase is configured (visible if datalakeBytebase platform parameter is set)Click the &quot;Try it in Sandbox&quot; button in the catalogs viewOpens Bytebase in a new browser tabRun SQL queries against the lakehouse catalog ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#related-features","content":"Sessions - For running Jupyter notebooks that connect to the datalakeJobs - For scheduling data pipeline jobs that read/write to the lakehouseStack Components - For deploying complementary tools like Dremio or Trino ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Performance Considerations​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#performance-considerations","content":"Large folders may take time to load - the interface fetches all items at once before displayingSample data queries are limited to configured sample sizes to prevent slow queriesThe integrated terminal maintains session state, so restart if you encounter memory issues ","version":"Next","tagName":"h3"},{"title":"Data Organization Best Practices​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#data-organization-best-practices","content":"Iceberg tables are organized in a hierarchy: catalog → namespace → tableEach table's data files are stored in a data/ subdirectoryMetadata files are stored in a metadata/ subdirectoryUse consistent naming conventions for catalogs and namespaces ","version":"Next","tagName":"h3"},{"title":"Path Formats​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#path-formats","content":"S3 paths are displayed in the format: s3://bucket-name/path/to/objectInternal paths used for navigation exclude the s3:// prefix and bucket nameCopy path buttons provide the full S3 URL for use in external tools ","version":"Next","tagName":"h3"},{"title":"Iceberg Table Detection​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#iceberg-table-detection","content":"The system automatically detects whether a path contains an Iceberg tableIceberg tables have special handling for sample data (reads from data/ subdirectory)Non-Iceberg Parquet files are read directly ","version":"Next","tagName":"h3"},{"title":"Connection Information​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#connection-information","content":"The Postgres connection URL shown in the bucket details card is masked for securityHover over the connection chip to reveal the masked URLClick the chip to copy the full unmasked URL to clipboard ","version":"Next","tagName":"h3"},{"title":"Availability​","type":1,"pageTitle":"Datalake (Shakudo Data Lakehouse)","url":"/developer-docs/features/datalake#availability","content":"If the datalake is not configured, an informational message appearsThe feature requires backend services to be running and properly configuredCheck with your platform administrator if the datalake appears unavailable ","version":"Next","tagName":"h3"},{"title":"Distributed Workloads Dashboard","type":0,"sectionRef":"#","url":"/developer-docs/features/distributed-workloads-dashboard","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#overview","content":"The Distributed Workloads Dashboard provides a centralized view for monitoring and managing distributed compute clusters created within the Shakudo platform. This feature displays active Dask and Ray clusters that have been deployed as part of user sessions or pipeline jobs, allowing administrators and users to view cluster details, access cluster dashboards, and manage cluster lifecycle. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#access--location","content":"Route: ?panel=distributed-workloads-dashboardNavigation: Monitoring → Distributed WorkloadsAccess Requirements: None specified (accessible to authenticated users)Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"View Active Dask Clusters​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#view-active-dask-clusters","content":"Monitor all active Dask clusters across the platform, including those created from JupyterHub sessions and pipeline jobs. View cluster specifications such as worker cores, RAM allocation, and number of worker nodes. ","version":"Next","tagName":"h3"},{"title":"View Active Ray Clusters​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#view-active-ray-clusters","content":"Monitor all active Ray clusters across the platform. Similar to Dask clusters, Ray clusters can originate from sessions or pipeline jobs and display their resource configuration. ","version":"Next","tagName":"h3"},{"title":"Access Cluster Dashboards​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#access-cluster-dashboards","content":"Quickly navigate to the native monitoring dashboards for both Dask and Ray clusters through convenient links in the interface. Dask clusters link to their status page, while Ray clusters link to the Ray dashboard. ","version":"Next","tagName":"h3"},{"title":"Stop Ray Clusters​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#stop-ray-clusters","content":"Directly terminate Ray clusters from the dashboard interface. This action removes the cluster's pods, services, and associated Kubernetes resources. ","version":"Next","tagName":"h3"},{"title":"Refresh Cluster Data​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#refresh-cluster-data","content":"Manually refresh the list of active clusters to get the most up-to-date information about cluster status and resource allocation. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#main-view","content":"The panel displays two separate tables showing active compute clusters: Compute Nodes Header: Displays &quot;Compute Nodes&quot; as the main title with a Refresh buttonDask Clusters Section: Shows all active Dask clusters with a subtitle explaining these are &quot;Dask clusters created as part of sessions or pipeline jobs&quot;Ray Clusters Section: Shows all active Ray clusters with a subtitle explaining these are &quot;Ray clusters created as part of sessions or pipeline jobs&quot; Both tables use server-side pagination with a default page size of 8 items per table. ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#dialogs--modals","content":"Stop Ray Cluster Dialog Purpose: Confirm before terminating a Ray clusterTrigger: Click the cancel icon button in the Ray Clusters tableFields: Cluster type identifierNamespace information Actions: Cancel: Close the dialog without taking actionConfirm: Stop the Ray cluster and remove all associated resources Behavior: Shows a loading indicator during the deletion process and refreshes the table after 2 seconds on success ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#tables--data-grids","content":"Dask Clusters Table Columns: Dashboard Link (icon): Opens the Dask dashboard in a new tabSource: Origin of the cluster (session or pipeline job)Owner: Username of the cluster ownerCluster Name: Full cluster identifier with copy-to-clipboard functionalityCores per Worker: CPU cores allocated to each worker nodeRAM per Worker: Memory allocated to each worker node# of Worker Nodes: Total number of worker nodes in the cluster Actions: Click dashboard icon to open Dask status pageClick cluster name to copy to clipboard Filtering: NonePagination: Server-side, 8 items per page Ray Clusters Table Columns: Actions (icons): Stop cluster button and dashboard linkSource: Origin of the cluster (session or pipeline job)Owner: Username of the cluster ownerName: Ray cluster type identifierCluster Name: Full cluster identifier with copy-to-clipboard functionalityCores per Worker: CPU cores allocated to each worker nodeRAM per Worker: Memory allocated to each worker node# of Worker Nodes: Total number of worker nodes in the cluster Actions: Stop cluster (cancel icon) - Opens confirmation dialogOpen Ray dashboard (external link icon) - Opens dashboard in new tabClick cluster name to copy to clipboard Filtering: NonePagination: Server-side, 8 items per page ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"API Endpoints​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#api-endpoints","content":"GET /api/distributed-workloads-dashboard/get-nodes Retrieves all active Dask and Ray clusters from Kubernetes namespacesQueries pods in both JupyterHub and Pipelines namespacesFilters pods based on Kubernetes labels: Dask: app=dask and dask.org/component=schedulerRay: Labels with type starting with ray-worker Returns cluster details including resource allocations and dashboard URLs POST /api/distributed-workloads-dashboard/ray/stop-cluster Terminates a Ray cluster by deleting associated Kubernetes resourcesParameters: type (cluster type), namespace (Kubernetes namespace)Deletes: Ray worker podsAssociated services (-svc suffix)Istio virtual services (-vs suffix) Returns success/error status with a descriptive message ","version":"Next","tagName":"h3"},{"title":"Kubernetes Integration​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#kubernetes-integration","content":"The dashboard interacts directly with the Kubernetes API to: Discover Clusters: Lists namespaced pods using the Kubernetes Core V1 APIExtract Metadata: Reads pod labels to identify cluster ownership, source, and configurationResource Information: Parses pod specs to determine CPU and memory limitsCleanup: Uses both Core V1 API and Custom Objects API to delete Ray cluster resources Key Kubernetes labels used: dask.org/cluster-name: Identifies Dask cluster membershipdask.org/component: Distinguishes scheduler from worker podstype: Identifies Ray worker pods (starts with ray-worker)hyperplane.dev/user: Cluster owner usernamehyperplane.dev/source: Origin (session or pipeline)hyperplane.dev/component-id: Component identifier for URL construction ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#component-structure","content":"Main Component: /components/Panels/DistributedWorkloads.tsxTable Component: /shakudo-apps/distributed-workloads-dashboard/components/Tables/ClusterNodes.tsxDialog: /shakudo-apps/distributed-workloads-dashboard/components/Dialogs/Ray/StopCluster.tsxAPI Routes: /pages/api/distributed-workloads-dashboard/get-nodes.ts/pages/api/distributed-workloads-dashboard/ray/stop-cluster.ts ","version":"Next","tagName":"h3"},{"title":"Data Structures​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#data-structures","content":"DaskCluster Interface: { id: string | undefined; user: string | undefined; url: string | undefined; nodeCores: string | undefined; nodeGbRam: string | undefined; numNodes: number | undefined; clusterName: string | undefined; source: string | undefined; }  RayCluster Interface: { id: string | undefined; user: string | undefined; url: string | undefined; type: string | undefined; nodeCores: string | undefined; nodeGbRam: string | undefined; numNodes: number | undefined; clusterName: string | undefined; source: string | undefined; namespace: string | undefined; }  ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Viewing Active Clusters​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#viewing-active-clusters","content":"Navigate to Monitoring → Distributed WorkloadsView the list of active Dask clusters in the first tableScroll down to view the list of active Ray clusters in the second tableClick the Refresh button to update the cluster information ","version":"Next","tagName":"h3"},{"title":"Accessing a Cluster Dashboard​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#accessing-a-cluster-dashboard","content":"Locate the cluster in either the Dask or Ray tableClick the external link icon (OpenInNew) in the first columnThe cluster's native dashboard opens in a new browser tab ","version":"Next","tagName":"h3"},{"title":"Stopping a Ray Cluster​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#stopping-a-ray-cluster","content":"Locate the Ray cluster you want to stop in the Ray Clusters tableClick the cancel icon (X) in the Actions columnReview the confirmation dialog showing the cluster type and namespaceClick &quot;Confirm&quot; to stop the cluster or &quot;Cancel&quot; to abortWait for the deletion to complete (loading indicator appears)The table automatically refreshes after 2 seconds to reflect the change ","version":"Next","tagName":"h3"},{"title":"Copying Cluster Names​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#copying-cluster-names","content":"Locate the cluster in either tableClick on the cluster name in the &quot;Cluster Name&quot; columnThe full cluster name is copied to your clipboardA success notification appears confirming the copy action ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#related-features","content":"Sessions Management - Sessions can create Dask and Ray clustersJobs - Pipeline jobs can deploy distributed compute clustersStack Components - Pre-configured data tools that may use distributed computing ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Distributed Workloads Dashboard","url":"/developer-docs/features/distributed-workloads-dashboard#notes--tips","content":"Namespace Awareness: The dashboard monitors both JupyterHub and Pipelines namespaces, ensuring all clusters are visible regardless of where they were createdResource Monitoring: Use this dashboard to monitor resource allocation across all distributed compute workloads to identify optimization opportunitiesDask vs Ray: Dask clusters cannot be stopped from this interface (no stop button provided), while Ray clusters can be terminated directlyAutomatic URL Construction: Dashboard URLs are automatically constructed based on cluster metadata and environment configurationLabel-Based Discovery: The system relies on Kubernetes labels to identify and classify clusters, so proper labeling is criticalDual Namespace Support: If the JupyterHub and Pipelines namespaces are the same, the system intelligently avoids duplicate queriesError Handling: If cluster retrieval fails, an error notification is displayed and the tables show empty statesReal-time Updates: The Refresh button queries the Kubernetes API directly, ensuring you always have current informationCopy-to-Clipboard: Long cluster names are truncated with ellipsis in the UI, but clicking copies the full nameSource Tracking: The &quot;Source&quot; column helps identify whether a cluster originated from a session or pipeline job ","version":"Next","tagName":"h2"},{"title":"Environment Configs","type":0,"sectionRef":"#","url":"/developer-docs/features/environment-configs","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#overview","content":"The Environment Configs panel is the central management hub for creating and managing environment configurations (also called &quot;Pod Specs&quot;) in the Shakudo Dashboard. Environment Configs define the computational resources, container images, and runtime settings that can be used when launching sessions, running jobs, or deploying microservices. This feature allows platform administrators and users to create reusable, standardized environment templates with specific CPU, memory, GPU allocations, Docker images, and advanced Kubernetes configurations. Environment Configs serve as templates that ensure consistency across workloads while providing flexibility for different computational requirements. They can be configured for both the main Shakudo cluster and satellite clusters. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#access--location","content":"Route: ?panel=environment-configsNavigation: Platform Management → Environment ConfigsAccess Requirements: View: Available to all authenticated usersCreate/Edit/Delete: Requires specific roles (configured via EnvironmentConfigRoles) Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Create Environment Configurations​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#create-environment-configurations","content":"Define new environment templates with customized resource allocations, Docker images, and Kubernetes settings. Users can specify CPU/memory limits and requests, GPU resources, node selectors, environment variables, volumes, tolerations, and custom Kubernetes spec fields. Configurations can include a README for documentation purposes. ","version":"Next","tagName":"h3"},{"title":"Edit Existing Configurations​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#edit-existing-configurations","content":"Modify environment configurations to update resource limits, change Docker images, or adjust Kubernetes settings. When an environment config is updated, new jobs and sessions will use the updated version, while in-progress jobs and sessions continue using the old version. Services using the updated config are automatically restarted. ","version":"Next","tagName":"h3"},{"title":"Clone Configurations​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#clone-configurations","content":"Duplicate existing environment configs to quickly create variations with similar settings. This accelerates the creation of new configs by starting with a proven template. ","version":"Next","tagName":"h3"},{"title":"View Configuration Details​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#view-configuration-details","content":"Inspect complete environment config specifications including all resource settings, environment variables, custom fields, volumes, tolerations, and node selectors. The details view provides links to view all sessions, jobs, and services using the configuration. ","version":"Next","tagName":"h3"},{"title":"Delete Configurations​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#delete-configurations","content":"Remove environment configs that are no longer needed. This action is permanent and cannot be reversed. Shakudo default environment configs cannot be deleted. ","version":"Next","tagName":"h3"},{"title":"Satellite Cluster Management​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#satellite-cluster-management","content":"Create and manage environment configs specifically for satellite clusters, enabling distributed workload execution across multiple Kubernetes clusters. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#main-view","content":"The Environment Configs panel features a tabbed interface with two primary views: Table View: Displays all environment configs in a searchable data grid with the following columns: Actions column (clone, edit, delete)Name (clickable to view details)Owner (shows creator email or Shakudo logo for defaults)Display NameImage (Docker image URL with copy functionality)CPU Limit and RequestMemory Limit and RequestGPU (type and count)Node Selector (key-value pairs)ID (hidden by default) Satellite Cluster View: Similar table specifically for managing environment configs on satellite clusters. Requires selecting a satellite cluster from a dropdown before viewing or creating configs. The table supports: Full-text search across all propertiesColumn visibility customizationRefresh functionalityPagination (20 items per page)Visual indicators for configs with custom fields ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#dialogs--modals","content":"Create Environment Config Dialog Purpose: Create new environment configurations from scratchTabs: General, Resources, Advanced, ReadmeFields: General Tab: Name, Internal Name (auto-generated, Kubernetes-compliant), Description, Image URLResources Tab: CPU Request/Limit, Memory Request/Limit, GPU Type, GPU CountAdvanced Tab: Volumes (YAML), Volume Mounts (YAML), Tolerations (YAML), Node Selector Key/Value, Environment Variables (key-value pairs), Custom Fields (spec path overrides)Readme Tab: Markdown editor for documentation Actions: Create Environment Config, Cancel, View Equivalent GraphQL MutationFeatures: Real-time validation, auto-generation of internal names, live summary preview Edit Environment Config Dialog Purpose: Modify existing environment configurationsFields: Same as Create dialog, but Internal Name is disabled (immutable)Actions: Save Changes, Cancel, View Equivalent GraphQL MutationFeatures: Confirmation dialog explaining impact of changes (can be disabled), dirty state detectionNote: Displays warning when changes will restart services using this config Environment Config Details Dialog Purpose: View comprehensive information about a specific environment configSections: Summary panel with all resource specificationsEnvironment Variables tableCustom Fields listVolumes, Volume Mounts, and Tolerations viewersREADME.md displayQuick navigation buttons to view associated Sessions, Jobs, and Services Actions: Back, View Equivalent GraphQL Mutation Delete Confirmation Dialog Purpose: Confirm permanent deletion of an environment configFields: Warning message with config nameActions: Delete, CancelWarning: Action is irreversible Custom Field Add/Edit Dialog Purpose: Add or edit custom Kubernetes spec field overridesFields: Path (autocomplete with examples like spec.nodeSelector, spec.tolerations)Value (YAML editor with syntax highlighting) Actions: Save, CancelFeatures: Path validation (must start with &quot;spec.&quot;), backend validation of path and value, example field suggestionsNote: Can be opened in view-only mode from details view Custom Field Container Purpose: Display individual custom field entriesShows: Field path with edit and delete actionsFeatures: Inline editing and removal ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#tables--data-grids","content":"Environment Configs Table Columns: Actions, Name, Owner, Display Name, Image, CPU Limit, CPU Request, Memory Limit, Memory Request, GPU, Node Selector, IDActions: Clone (opens Create dialog with pre-filled values), Edit, DeleteFiltering: Real-time search across all columnsSpecial Features: Visual indicator (info icon) for configs with custom fieldsShakudo logo icon for default system configsCopy-to-clipboard for Image URL, Node Selector, and IDClickable names open details dialog Pagination: 20 items per page Satellite Environment Configs Table Columns: Same as main tableContext: Requires satellite cluster selectionActions: Same as main table but operations apply to selected satellite clusterNote: Independent from main cluster configs ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#graphql-operations","content":"Queries: getEnvironmentConfigs - Retrieves all environment configs with optional search filtering across multiple fields (name, description, owner, resources, image, etc.)GetPodSpecNameDocument - Validates that a proposed environment config name is availablevalidatePathAndValue - Validates custom field path and YAML value before savingGetSatelliteEnvironmentConfigsDocument - Retrieves environment configs for a specific satellite cluster Mutations: createEnvironmentConfig - Creates a new environment configuration with all specified parameters including custom fieldsupdateEnvironmentConfig - Updates an existing environment config, triggering service restarts if applicabledeleteEnvironmentConfig - Permanently deletes an environment configuration by IDcreateSatelliteEnvironmentConfig - Creates environment config on a satellite clusterupdateSatelliteEnvironmentConfig - Updates satellite cluster environment configdeleteSatelliteEnvironmentConfig - Deletes satellite cluster environment config Subscriptions: None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#component-structure","content":"Main Component: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/EnvironmentConfigs/EnvironmentConfigPanel.tsxTable Component: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/EnvironmentConfigs/EnvironmentConfigTables.tsxDialogs: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/EnvironmentConfigs/Dialogs/EnvironmentConfigCreateDialog.tsx/root/gitrepos/monorepo/apps/hyperplane-dashboard/components/EnvironmentConfigs/Dialogs/EnvironmentConfigEditDialog.tsx/root/gitrepos/monorepo/apps/hyperplane-dashboard/components/EnvironmentConfigs/Dialogs/EnvironmentConfigDetailsDialog.tsx/root/gitrepos/monorepo/apps/hyperplane-dashboard/components/EnvironmentConfigs/Dialogs/EnvironmentConfigDeleteDialog.tsx/root/gitrepos/monorepo/apps/hyperplane-dashboard/components/EnvironmentConfigs/Dialogs/EnvironmentConfigCustomFieldAddOrEditDialog.tsx/root/gitrepos/monorepo/apps/hyperplane-dashboard/components/EnvironmentConfigs/Dialogs/EnvironmentConfigCustomFieldContainer.tsx State Management: Jotai atoms for panel navigation and data sharing ","version":"Next","tagName":"h3"},{"title":"Resource Validation​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#resource-validation","content":"The feature includes comprehensive validation for Kubernetes resource specifications: CPU Resources: Accepts numeric values or millicores (e.g., &quot;2&quot;, &quot;2000m&quot;)Valid units: 'm' (millicores)Validates that values are positive and within safe integer bounds Memory Resources: Accepts binary units: Ki, Mi, Gi, Ti, Pi, Ei (power of 1024)Accepts decimal units: k, M, G, T, P, E (power of 1000)Validates that values are positive and within safe integer boundsExample: &quot;2Gi&quot;, &quot;2048Mi&quot;, &quot;2000000k&quot; GPU Resources: Numeric values only (count of GPUs)Supported types: nvidia.com/gpu, nvidia.com/mig-* (MIG types currently disabled) Internal Name Validation: Must be 1-63 charactersLowercase alphanumeric with dashes and dotsMust start and end with alphanumeric characterNo spaces allowedMust be unique across all environment configsReal-time availability checking ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Creating a Basic Environment Config​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#creating-a-basic-environment-config","content":"Navigate to Environment Configs panelClick &quot;Create Environment Config&quot; buttonFill in General tab: Enter Name (e.g., &quot;Python ML Environment&quot;)Verify auto-generated Internal Name or customizeAdd DescriptionSpecify Image URL (Docker container) Configure Resources tab: Set CPU Request and Limit (e.g., &quot;2&quot;, &quot;4&quot;)Set Memory Request and Limit (e.g., &quot;4Gi&quot;, &quot;8Gi&quot;)Optionally configure GPU if needed (Optional) Configure Advanced settings: Add environment variablesConfigure volumes, mounts, tolerationsSet node selectors for specific hardware (Optional) Add README documentation in MarkdownReview summary panel on the rightClick &quot;Create Environment Config&quot; ","version":"Next","tagName":"h3"},{"title":"Cloning and Customizing an Existing Config​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#cloning-and-customizing-an-existing-config","content":"Find the environment config you want to clone in the tableClick the Clone icon in the Actions columnCreate dialog opens with all fields pre-populatedModify the Name and Internal Name (required - must be unique)Adjust any other settings as needed (resources, image, etc.)Click &quot;Create Environment Config&quot; ","version":"Next","tagName":"h3"},{"title":"Editing Resource Limits for Existing Config​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#editing-resource-limits-for-existing-config","content":"Locate the environment config in the tableClick the Edit icon in the Actions columnNavigate to the Resources tabUpdate CPU and/or Memory limits and requestsReview changes in the summary panelClick &quot;Save Changes&quot;Confirm in the dialog (explains that services will restart)Changes are applied to new sessions/jobs immediately; services restart automatically ","version":"Next","tagName":"h3"},{"title":"Adding Custom Kubernetes Spec Fields​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#adding-custom-kubernetes-spec-fields","content":"Open Environment Config in Edit or Create modeNavigate to Advanced tabScroll to &quot;Custom Fields&quot; sectionClick the Add buttonIn the Custom Field dialog: Enter or select a spec path (e.g., &quot;spec.nodeSelector&quot;)Enter the YAML value in the editorUse provided examples for common patterns Click Save to validate and add the fieldRepeat for additional custom fieldsSave the environment config ","version":"Next","tagName":"h3"},{"title":"Viewing Environment Config Usage​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#viewing-environment-config-usage","content":"Click on any environment config name in the tableDetails dialog opens showing full configurationScroll to the bottom of the right panelClick &quot;View Sessions&quot;, &quot;View Jobs&quot;, or &quot;View Services&quot;Navigates to respective panel with pre-filtered results showing only items using this config ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#related-features","content":"Sessions Panel: Uses environment configs to determine session resources and container imagesJobs Panel: Immediate and scheduled jobs select environment configs for execution environmentServices Panel: Microservices use environment configs for deployment specificationsSatellite Clusters: Manages distributed environment configs across multiple Kubernetes clusters ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Best Practices​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#best-practices","content":"Naming Convention: Use descriptive names that indicate the purpose and resource level (e.g., &quot;python-ml-gpu-large&quot;, &quot;spark-processing-xlarge&quot;)Documentation: Always fill in the README tab to explain the intended use case, pre-installed packages, and any special configurationsResource Requests vs Limits: Set requests to guaranteed minimum resources and limits to prevent resource overconsumptionGPU Configurations: Ensure node selectors and tolerations are properly configured to schedule on GPU-enabled nodesTesting: Clone production configs for testing changes before modifying the originalVersion Control: Use descriptive names with version indicators when creating multiple variations (e.g., &quot;python-ml-v2&quot;) ","version":"Next","tagName":"h3"},{"title":"Important Considerations​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#important-considerations","content":"Immutability: The Internal Name (Kubernetes object name) cannot be changed after creationShakudo Defaults: System-provided environment configs (marked with Shakudo logo) cannot be edited or deletedService Restarts: Editing an environment config automatically restarts all services using it, causing brief downtimeCustom Fields: Custom fields override existing spec values and can create or update nested Kubernetes fields - use with cautionResource Availability: Ensure cluster has available resources matching your config specificationsImage Accessibility: Verify Docker images are accessible from the cluster (check registry credentials if using private images)Validation: Backend validates custom field paths and values - ensure YAML syntax is correctSatellite Clusters: Satellite environment configs are completely separate from main cluster configs ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#troubleshooting","content":"&quot;Environment Config name is already used&quot;: Choose a different Internal Name - this field must be globally unique&quot;Please enter a valid CPU unit&quot;: Use numeric values or millicores (e.g., &quot;2&quot; or &quot;2000m&quot;)&quot;Please enter a valid memory unit&quot;: Use Kubernetes memory units (e.g., &quot;4Gi&quot;, &quot;4096Mi&quot;)Custom field validation errors: Ensure path starts with &quot;spec.&quot; and YAML value is valid&quot;Insufficient Permissions&quot;: Contact administrator for Environment Config creation/edit rolesServices not using updated config: Services automatically restart, but check service panel for any errors during restartGPU scheduling failures: Verify node selectors, tolerations, and GPU resource types match cluster configuration ","version":"Next","tagName":"h3"},{"title":"Advanced Features​","type":1,"pageTitle":"Environment Configs","url":"/developer-docs/features/environment-configs#advanced-features","content":"Equivalent GraphQL Mutation: Every create/edit dialog provides a &quot;View Equivalent GraphQL Mutation&quot; button to see the exact API call being made - useful for automation and scriptingEnvironment Variables: Support key-value pairs that are injected as environment variables into podsCustom Fields: Advanced feature allowing direct Kubernetes spec field overrides using path notation (e.g., spec.containers[0].securityContext.runAsUser)Tutorial Video: Click the tutorial icon next to &quot;Environment Configs&quot; title to watch usage guideMarkdown README: Supports full Markdown syntax including code blocks, links, and formatting for comprehensive documentation ","version":"Next","tagName":"h3"},{"title":"Git Repositories","type":0,"sectionRef":"#","url":"/developer-docs/features/git-repositories","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#overview","content":"The Git Repositories panel enables teams to link external Git repositories to the Shakudo platform, allowing workloads (jobs, microservices, sessions) to access and sync code from version control systems. The panel supports both Primary Cluster and Satellite Cluster deployments, with automatic synchronization monitoring and SSH key management for secure repository access. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#access--location","content":"Route: ?panel=git-repositoriesNavigation: Shakudo Objects → Git RepositoriesAccess Requirements: Create/Link Repository: Dashboard Admin or Dashboard Maintainer rolesUnlink Repository: Dashboard Admin role onlyView SSH Keys: Dashboard Admin or Dashboard Maintainer roles Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Link Git Repository to Primary Cluster​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#link-git-repository-to-primary-cluster","content":"Connect external Git repositories to the Shakudo platform's primary cluster, enabling workloads to access code from your version control system. Each repository is assigned a unique SSH key pair for secure access. When to use: Link repositories that will be used by jobs, microservices, or sessions running on the primary cluster. ","version":"Next","tagName":"h3"},{"title":"Link Git Repository to Satellite Cluster​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#link-git-repository-to-satellite-cluster","content":"Connect external Git repositories to specific Satellite Clusters, allowing distributed deployments to access code independently. Each satellite cluster maintains its own set of repository connections. When to use: Deploy code to remote Satellite Clusters that need independent Git repository access. ","version":"Next","tagName":"h3"},{"title":"Monitor Repository Sync Status​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#monitor-repository-sync-status","content":"Track real-time synchronization status of linked repositories, including commit information, sync health, and any errors. The system polls every 5 seconds to provide up-to-date status information. When to use: Verify that repositories are in sync before running workloads, or troubleshoot synchronization issues. ","version":"Next","tagName":"h3"},{"title":"Manage SSH Keys​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#manage-ssh-keys","content":"View and copy SSH public keys for each repository to add as deploy keys in your Git provider (GitHub, GitLab, Bitbucket, etc.). When to use: Configure repository access after linking, or retrieve keys when rotating access credentials. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#main-view","content":"The panel features a tabbed interface with two main sections: Primary Cluster Tab: Displays all Git repositories linked to the main Shakudo cluster Search and filter capabilities across all repository propertiesReal-time sync status updates every 5 secondsQuick access to tutorial videos for guidance Satellite Clusters Tab: Shows Git repositories linked to specific Satellite Clusters Cluster selection dropdown to view repositories for a specific satelliteIndependent repository management per satellite clusterSame monitoring capabilities as primary cluster ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#dialogs--modals","content":"Link Git Repository Dialog (Primary Cluster) Purpose: Connect a new Git repository to the primary clusterFields: Name: Unique identifier for the repository (alphanumeric, spaces, and special characters: / @ : . _ -)Git Repository SSH URL: SSH URL in Git format (e.g., git@github.com:org/repo.git)Default Branch: Branch name to sync (e.g., main, master, develop)Use Default SSH Key: Optional legacy mode to use a shared SSH key across multiple repositories (not recommended) Actions: Link or CancelValidation: Name uniqueness check, URL format validation Unlink Git Repository Dialog Purpose: Remove a Git repository connection from the platformWarning: Permanently disconnects the repository; SSH keys should be manually removed from the Git providerActions: Unlink or CancelAccess: Dashboard Admin only SSH Key Dialog Purpose: Display and copy SSH public key for repository configurationContent: Shows either custom SSH key (recommended) or default SSH key (legacy)Information tooltips explain the difference between custom and default SSH keysActions: Copy to clipboard, Close Link Git Repository to Satellite Cluster Dialog Purpose: Connect a Git repository to a specific Satellite ClusterFields: Same as primary cluster link dialogActions: Link or Cancel Satellite Cluster SSH Key Dialog Purpose: Display SSH public key for satellite cluster repositoryContent: SSH public key specific to the satellite clusterActions: Copy to clipboard, Close Tutorial Video Dialog Purpose: Display instructional video for linking Git repositoriesContent: Video with title and descriptionActions: Close ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#tables--data-grids","content":"Primary Cluster Git Repositories Table Columns: Actions: Unlink button, SSH Key buttonName: Repository identifier, with &quot;default&quot; badge if marked as default serverStatus: Visual indicator (IN_SYNC, BEHIND, BRANCH_NOT_FOUND, REMOTE_NOT_FOUND) with tooltip showing commit detailsGit Repository URL: Copyable SSH URLBranch: Copyable branch nameLast Commit ID: Copyable commit hashLast Commit Date: Formatted timestamp (YYYY-MM-DD HH:mm:ss)ID: Hidden by default, copyable repository ID Actions: Unlink repository (admin only)View/copy SSH key (admin/maintainer) Filtering: Text-based search across all fields (name, URL, branch, commit info)Refresh: Manual refresh button and automatic polling every 5 secondsPagination: 20 items per page Satellite Cluster Git Repositories Table Columns: Same as Primary Cluster table, plus: Satellite Cluster Name: Displays the selected satellite cluster name Actions: Same as Primary Cluster tableFiltering: No search (filtered by selected satellite cluster only)Note: Table only displays when a satellite cluster is selected ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#graphql-operations","content":"Queries: GetVcServersDocument - Retrieves all Git repositories for the primary cluster with optional search filtering. Excludes DELETED and DELETING statuses. Polls every 5 seconds.GetVcServersByNameDocument - Validates repository name uniqueness during creationGetSatelliteGitServersDocument - Retrieves Git repositories for a specific Satellite Cluster. Polls every 5 seconds.GetGitStatusDocument - Retrieves Git sync status for default repository monitoring Mutations: CreateVcServerDocument - Creates a new Git repository connection for the primary cluster Parameters: name, url, defaultBranch, useDefaultSSHKeyReturns: repository ID, name, URL, branch, SSH key setting, status DeleteVcServerDocument - Marks a Git repository for deletion by setting status to DELETING Parameters: repository IDReturns: repository ID CreateSatelliteGitServerDocument - Creates a Git repository connection for a Satellite Cluster Parameters: satelliteClusterName, name, url, defaultBranch, useDefaultSSHKeyReturns: satellite repository details Subscriptions: None (uses polling for real-time updates) ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#component-structure","content":"Main Component: components/GitRepositories/GitRepositoriesPanel.tsxDialogs: components/GitRepositories/Dialogs/GitRepositoriesLinkDialog.tsxcomponents/GitRepositories/Dialogs/GitRepositoriesUnlinkDialog.tsxcomponents/GitRepositories/Dialogs/GitRepositorySSHKeyDialog.tsxcomponents/GitRepositories/Dialogs/SatelliteCluster/SatelliteGitRepositoriesLinkDialog.tsxcomponents/GitRepositories/Dialogs/SatelliteCluster/SatelliteGitRepositoriesUnlinkDialog.tsxcomponents/GitRepositories/Dialogs/SatelliteCluster/SatelliteGitRepositorySSHKeyDialog.tsx Supporting Components: components/GitRepositories/GitRepositoriesStatus.tsx - Git status monitoringcomponents/GitRepositories/GitRepositoriesStatusIcon.tsx - Status visualizationcomponents/GitRepositories/GitRepositoriesAutocomplete.tsx - Repository selection ","version":"Next","tagName":"h3"},{"title":"Repository Status States​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#repository-status-states","content":"IN_SYNC: Repository is synchronized with the remote branch (green check icon)BEHIND: Local repository is behind the remote branch (orange warning icon)BRANCH_NOT_FOUND: Specified branch doesn't exist in the repository (red error icon)REMOTE_NOT_FOUND: Cannot connect to remote repository; SSH key may be missing or invalid (red error icon)FAILING/FAILED: Kubernetes resources failed to create (gray disabled icon) ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Link a New Git Repository to Primary Cluster​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#link-a-new-git-repository-to-primary-cluster","content":"Click &quot;Link Git Repository&quot; button in the top-right cornerEnter a unique name for the repositoryProvide the Git repository SSH URL (format: git@provider.com:org/repo.git)Specify the default branch to sync (e.g., main)Optionally enable &quot;Use default SSH key&quot; (not recommended for security)Click &quot;Link&quot; to create the connectionClick the SSH Key icon in the Actions columnCopy the SSH public keyAdd the key as a deploy key in your Git provider's repository settingsWait for the status to change to &quot;IN_SYNC&quot; ","version":"Next","tagName":"h3"},{"title":"Link a Repository to a Satellite Cluster​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#link-a-repository-to-a-satellite-cluster","content":"Switch to the &quot;Satellite Clusters&quot; tabSelect a Satellite Cluster from the dropdownClick &quot;Link Git Repository To Satellite Cluster&quot; buttonFollow the same steps as linking to primary cluster (steps 2-10 above) ","version":"Next","tagName":"h3"},{"title":"Troubleshoot Repository Sync Issues​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#troubleshoot-repository-sync-issues","content":"Check the Status column for error indicatorsHover over the status icon to view detailed error informationFor &quot;REMOTE_NOT_FOUND&quot; errors: Click the SSH Key icon to view the public keyVerify the key is added to your Git provider's repository settingsEnsure the repository URL is correct For &quot;BRANCH_NOT_FOUND&quot; errors: Verify the branch name exists in the repositoryCheck for typos in the branch name Use the Refresh button to force an immediate sync checkMonitor the Last Commit Date to verify ongoing synchronization ","version":"Next","tagName":"h3"},{"title":"Remove a Git Repository​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#remove-a-git-repository","content":"Locate the repository in the tableClick the unlink icon (✖) in the Actions columnReview the warning message about SSH key cleanupClick &quot;Unlink&quot; to confirmManually remove the SSH key from your Git provider's repository settings ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#related-features","content":"Immediate Jobs - Jobs can reference Git repositories for code executionScheduled Jobs - Scheduled workloads can sync code from linked repositoriesMicroservices - Services can deploy code from linked Git repositoriesSessions - Development sessions can clone and work with linked repositoriesSatellite Clusters - Remote cluster deployments can link their own repositoriesEnvironment Configs - Environments specify which repository to use for workload execution ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"SSH Key Management​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#ssh-key-management","content":"Custom SSH Keys (Recommended): Each repository gets a unique SSH key pair, following security best practices. Most Git providers support multiple deploy keys per repository.Default SSH Key (Legacy): A single shared SSH key across multiple repositories. Not recommended as GitHub and GitLab restrict this for security reasons. Only use if adding the SSH key to a bot user account with repository read access. ","version":"Next","tagName":"h3"},{"title":"Repository Naming​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#repository-naming","content":"Names must be unique across all repositories in the clusterValid characters: letters, numbers, spaces, and special characters (/ @ : . _ -)Choose descriptive names that indicate the repository's purpose ","version":"Next","tagName":"h3"},{"title":"Sync Monitoring​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#sync-monitoring","content":"The panel automatically polls for updates every 5 secondsManual refresh is available if immediate status check is neededStatus tooltips provide detailed commit information and error messages ","version":"Next","tagName":"h3"},{"title":"Performance Considerations​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#performance-considerations","content":"Large repositories may take longer to perform initial syncNetwork issues between Shakudo and the Git provider can cause REMOTE_NOT_FOUND errorsSatellite clusters sync independently and may have different sync states than primary cluster ","version":"Next","tagName":"h3"},{"title":"Security Best Practices​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#security-best-practices","content":"Always use custom SSH keys for each repositoryRemove SSH keys from Git providers when unlinking repositoriesRestrict repository access to read-only (deploy keys) when possibleUse dedicated service/bot accounts for repository access rather than personal accounts ","version":"Next","tagName":"h3"},{"title":"Default Repository​","type":1,"pageTitle":"Git Repositories","url":"/developer-docs/features/git-repositories#default-repository","content":"A repository marked with the &quot;default&quot; badge is used as the primary code source for the platformThe default repository's sync status is monitored separately and shown in the platform status interfaceOnly one repository can be marked as default at a time ","version":"Next","tagName":"h3"},{"title":"Home Panel","type":0,"sectionRef":"#","url":"/developer-docs/features/home","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#overview","content":"The Home panel serves as the central dashboard landing page in Shakudo, providing users with quick access to common actions and an overview of their active stack components. It features a personalized greeting, quick-start buttons for primary workflows, and displays pinned or active stack components along with helpful tutorial resources. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#access--location","content":"Route: ?panel=homeNavigation: Shakudo → Home (Shakudo logo icon in the navigation menu)Access Requirements: None - available to all authenticated usersFeature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Quick Action Buttons​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#quick-action-buttons","content":"The Home panel provides four primary action buttons that navigate users to key platform features: Start a Session - Opens the Sessions panel to create a new development environmentStart a Job - Opens the Jobs panel to create an immediate jobStart a Microservice - Opens the Services panel to create a new microserviceMonitor Nodes - Opens Grafana in a new tab to monitor Kubernetes compute resources ","version":"Next","tagName":"h3"},{"title":"Stack Components Display​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#stack-components-display","content":"Shows a curated list of stack components (platform apps) to provide quick access to installed tools: Displays pinned apps for the current user when availableFalls back to showing all active apps if the user has no pinned appsEach app card displays the app's icon, name, and indicates pinned statusClicking an app card opens either a dedicated management page or the app's UI in a dialog ","version":"Next","tagName":"h3"},{"title":"Tutorial Resources​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#tutorial-resources","content":"Displays a filtered list of tutorials with usage documentation to help users learn about the platform and its features. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#main-view","content":"The Home panel consists of three main sections: Welcome Header Personalized greeting using the user's first name or email prefixCentered at the top of the page Quick Action Grid Four action buttons arranged horizontally (responsive grid layout)Each button displays an icon and descriptive textIcons: Code (Sessions), Square Play (Jobs), Cloud Cog (Microservices), Activity (Monitor) Stack Components Section Section header with &quot;Stack Components&quot; titleLoading indicator shown during data fetchGrid of app cards showing either: User's pinned apps (if any exist)Active platform apps (fallback when no pinned apps) Each card shows app icon, name, and pin indicator (if pinned) Get Started Section Section header with &quot;Get Started&quot; titleTutorial panel displaying available tutorials with usage documentationFixed height of 160px for home panel displayNo search bar enabled for home panel tutorials Footer Standard home page footer at the bottom ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#dialogs--modals","content":"SessionStatusDialog Purpose: Display status information for a newly created or selected sessionTriggered when a session ID is set (currently not actively used in the displayed code flow)Actions: Close dialog PlatformAppIframeDialog (via AppCardSmall) Purpose: Display stack component UI in an embedded iframeTriggered when clicking on an app card that has a web UIShows the app's interface without leaving the dashboard ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#graphql-operations","content":"Queries: GetPinnedApps - Retrieves pinned platform apps for the current user Variables: user ID, optional limit and offsetFilters: show=true, installStatus=ACTIVEReturns: app details including name, URLs, images, install status, namespace, usage docsAlso returns: count of pinned apps GetApps - Retrieves all platform apps with optional filtering Variables: optional where clause for filteringReturns: complete app details including license informationOrdered by name (ascending) Mutations:None - Home panel is read-only Subscriptions:None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#component-structure","content":"Main Component: components/Panels/Home.tsxSupporting Components: components/HomePage/StarterButton.tsx - Quick action buttonscomponents/PlatformApps/PlatformAppsCardSmall.tsx - Stack component cardscomponents/Tutorials/TutorialPanel.tsx - Tutorial resource displaycomponents/HomePage/Footer.tsx - Page footercomponents/Sessions/Dialog/SessionStatusDialog.tsx - Session status modal ","version":"Next","tagName":"h3"},{"title":"Data Flow​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#data-flow","content":"Fetches user's pinned apps using GetPinnedApps query with user IDFetches all platform apps using GetApps queryFilters active apps (ACTIVE, SCALING, or PAUSING status) excluding pinned appsDisplays pinned apps if available, otherwise shows active appsTutorials are filtered to only show those with usage_docs_url ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#state-management","content":"Uses React context for user data (HyperplaneUserContext) and platform parameters (PlatformParametersContext)Local state for session dialog visibilityApollo Client cache-first fetch policy for optimal performance ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Starting a New Session​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#starting-a-new-session","content":"User clicks &quot;Start a session&quot; buttonRedirects to Sessions panel with create section openUser can configure and launch a new development environment ","version":"Next","tagName":"h3"},{"title":"Starting a New Job​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#starting-a-new-job","content":"User clicks &quot;Start a Job&quot; buttonRedirects to Jobs panel with create section openUser can configure and launch an immediate job ","version":"Next","tagName":"h3"},{"title":"Starting a Microservice​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#starting-a-microservice","content":"User clicks &quot;Start a Microservice&quot; buttonRedirects to Services panel with create section openUser can configure and deploy a new microservice ","version":"Next","tagName":"h3"},{"title":"Accessing Stack Components​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#accessing-stack-components","content":"User views pinned or active stack components in the gridUser clicks on an app cardSystem checks if app has a dedicated management page or web UIEither navigates to management page or opens iframe dialog with app UI ","version":"Next","tagName":"h3"},{"title":"Monitoring Infrastructure​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#monitoring-infrastructure","content":"User clicks &quot;Monitor Nodes&quot; buttonOpens Grafana dashboard in new browser tabShows Kubernetes compute resources for user's namespaceDashboard auto-refreshes every 10 seconds ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#related-features","content":"Sessions Panel - Development environment managementJobs Panel - Immediate job executionScheduled Jobs Panel - Scheduled job managementServices Panel - Microservice deployment and managementStack Components Panel - Comprehensive stack component managementTutorials Panel - Full tutorial library ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"User Identity Display​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#user-identity-display","content":"The greeting uses the user's first name if availableFalls back to the email username (part before @) if first name is not setDisplay is capitalized for proper presentation ","version":"Next","tagName":"h3"},{"title":"Stack Component Behavior​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#stack-component-behavior","content":"Pinned apps are prioritized and always shown when availableActive apps serve as a fallback to ensure something is always displayedPin indicator appears in the top-right corner of pinned app cardsApps can be clicked only if they have ACTIVE install status ","version":"Next","tagName":"h3"},{"title":"App Card Navigation Logic​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#app-card-navigation-logic","content":"Apps with dedicated management pages (found in appPageDirectory) navigate to the apps panel with specific app viewApps with web UIs open in an iframe dialog within the dashboardApps without UIs show informational notifications ","version":"Next","tagName":"h3"},{"title":"Grafana Integration​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#grafana-integration","content":"Grafana URL is dynamically constructed using the platform domain and user's default namespacePre-configured to show Kubernetes compute resources dashboardUses organization ID 1 and Prometheus datasource ","version":"Next","tagName":"h3"},{"title":"Performance Optimization​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#performance-optimization","content":"Uses cache-first fetch policy to minimize API callsMemoizes active apps list to prevent unnecessary recalculationsEfficient filtering using Set data structure for pinned app IDs ","version":"Next","tagName":"h3"},{"title":"Tutorial Display​","type":1,"pageTitle":"Home Panel","url":"/developer-docs/features/home#tutorial-display","content":"Home panel shows a condensed version (160px height vs 220px for dedicated tutorials panel)Only tutorials with usage documentation URLs are displayedSearch bar is disabled on the home panel version ","version":"Next","tagName":"h3"},{"title":"Immediate Jobs","type":0,"sectionRef":"#","url":"/developer-docs/features/jobs","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#overview","content":"The Immediate Jobs panel provides comprehensive management of one-time pipeline jobs in the Shakudo Dashboard. It enables users to create, monitor, and control data pipeline jobs that execute immediately upon creation, without a recurring schedule. Users can manage jobs across both the primary cluster and satellite clusters, with full filtering, monitoring, and action capabilities. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#access--location","content":"Route: ?panel=jobsNavigation: Workloads → JobsAccess Requirements: None (public access, role-based filtering applies)Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Create Immediate Jobs​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#create-immediate-jobs","content":"Create and execute one-time pipeline jobs with comprehensive configuration options including environment settings, git integration, resource allocation, notification setup, and custom parameters. Jobs can be created for the primary cluster or satellite clusters. ","version":"Next","tagName":"h3"},{"title":"Monitor Job Execution​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#monitor-job-execution","content":"View real-time status of running jobs and historical records of completed jobs. Track job progress, duration, resource utilization, and execution logs through integrated Grafana dashboards and event monitoring. ","version":"Next","tagName":"h3"},{"title":"Manage Job Lifecycle​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#manage-job-lifecycle","content":"Control job execution with actions including: Clone existing jobs with configuration preservationConvert immediate jobs to scheduled jobsTerminate running or pending jobsPin important jobs for quick accessPublish jobs for team visibilityClone with customized Pod YAML specifications ","version":"Next","tagName":"h3"},{"title":"Filter and Search​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#filter-and-search","content":"Apply advanced filtering across multiple dimensions including status, job name, environment configuration, parameters, timeframes, duration, owner, billing project, and custom search across names, paths, IDs, and authors. ","version":"Next","tagName":"h3"},{"title":"Multi-Cluster Support​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#multi-cluster-support","content":"Manage jobs across the primary Kubernetes cluster and satellite clusters with dedicated views and cluster-specific configurations. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#main-view","content":"The panel presents a dual-tab interface for cluster selection (Primary Cluster / Satellite Clusters) and job type (Running / History). Each view includes: Create Immediate Job button (top-right)Filter chips for active filters with reset optionParameter filtering dialog for complex parameter queriesPagination controls (20 jobs per page)Real-time data refresh capabilities ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#dialogs--modals","content":"Create Immediate Job Dialog Purpose: Configure and launch a new immediate pipeline jobFields: Job name, environment config, pipeline path, working directory, git settings (branch/commit), timeout settings, retry configuration, parameters, notifications, secrets, billing project, service account, cloud SQL proxy settingsActions: Create job, clone from existing, cancel Job Details Dialog Purpose: View comprehensive information about a specific jobFields: All job metadata, execution logs, events, parameters, resource allocations, custom configurationsActions: Access via clicking job name in table Events and Logs Dialog Purpose: Real-time monitoring of job execution logs and Kubernetes eventsFields: Pod events, container logs, timestampsActions: Available for running jobs in the &quot;Logs&quot; column Cancel Job Dialog Purpose: Terminate a running or pending jobActions: Confirm termination (available via Actions menu) Parameters Filter Dialog Purpose: Filter jobs by specific parameter key-value pairsFields: Multiple parameter pairs (key/value combinations)Actions: Apply filters, accessible via info chip in filter bar ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#tables--data-grids","content":"Running Jobs Table (Current Immediate Jobs) Columns (visible): Actions, Name (with pin), Status, Logs (events/Grafana), Env Config, ID, Pipeline Path, Created, DurationColumns (hidden): Custom Image URL, Output Notebooks Path, Billing Project ID, Service Account ID, Status Reason, Schedule, Owner, Branch, Commit ID, Active Timeout, Max TriesActions: View details, pin/unpin, actions menu (clone, convert to scheduled, terminate)Filtering: Quick search, advanced filters via toolbar, status auto-filtered to 'in progress' and 'pending'Features: Real-time updates, copy-to-clipboard for IDs and paths, published job indicators History Jobs Table (Past Immediate Jobs) Columns (visible): Actions, Name (with pin), Status (with failure reason tooltips), Logs (Grafana), Env Config, ID, Pipeline Path, Created, End, DurationColumns (hidden): Same as Running Jobs tableActions: View details, pin/unpin, clone, convert to scheduled, publish/unpublishFiltering: Quick search, advanced filters via toolbar, status auto-filtered to exclude 'in progress' and 'pending'Features: Status reason tooltips for failed jobs, historical job analysis, duration tracking Both tables support: Column visibility management (show/hide columns)Pinned columns (Actions pinned to right)Row pinning (pinned jobs appear at top)Pagination with page count displayLoading states with circular progress indicatorsError handling with user-friendly messages ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#graphql-operations","content":"Queries: getImmediateJobs - Retrieves paginated list of immediate jobs with comprehensive filtering, ordered by pinned status and start timecountJobs - Returns total count of jobs matching filter criteria for paginationgetImmediateSatelliteJobs - Retrieves immediate jobs from a specific satellite cluster (polls every 5 seconds)countJobsOnSatelliteCluster - Returns count of satellite cluster jobs for paginationgetSatelliteClusters - Retrieves satellite cluster information for job operations Mutations: createPipelineJobWithAlerting - Creates a new immediate job with full configuration including alerting setupcancelPipelineJob - Terminates a running or pending immediate jobupdateOnePipelineJob (pinJob) - Toggles pin status for a jobcreateSatellitePipelineJobWithAlerting - Creates immediate job on satellite cluster Subscriptions: None (uses polling for satellite cluster jobs at 5-second intervals) ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#component-structure","content":"Main Component: /components/Jobs/Immediate/ImmediateJobsPanel.tsxTables: /components/Jobs/Immediate/Tables/CurrentImmediateJobsTable.tsx/components/Jobs/Immediate/Tables/PastImmediateJobsTable.tsx Main Table Component: /components/Jobs/Immediate/ImmediateJobsTables.tsxDialogs: /components/Jobs/Immediate/ImmediateJobDialog.tsx (create/clone)/components/Jobs/Immediate/satelliteCluster/ImmediateSatelliteJobDialog.tsx (satellite jobs)/components/Jobs/shared/Details/satelliteCluster/SatelliteJobsDetailsDialog.tsx/components/Jobs/shared/EventsAndLogsDialog.tsx/components/Jobs/shared/SatelliteEventsAndLogsDialog.tsx Actions: /components/Jobs/Immediate/ImmediateJobActionsMenu.tsxLoader: /components/Jobs/shared/JobsLoader.tsx ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#state-management","content":"Uses Jotai atoms for global state: JobsPanelSectionAtom - Controls current view (table/create/details/clone)CurrentImmediateJobsAtom / PastImmediateJobsAtom - Job data storageCurrentJobFiltersAtom / PastJobFiltersAtom - Active filter statesCurrentImmediateJobsTablePageAtom / PastImmediateJobsTablePageAtom - Pagination stateCurrentJobsDataLoadingAtom / PastJobsDataLoadingAtom - Loading statesimmediateJobsClusterTypeTabAtom - Cluster tab selection (0=primary, 1=satellite)immediateJobsTableTypeTabAtom - Job type tab selection (0=running, 1=history)selectedSatelliteClusterAtom / selectedSatelliteClusterIdAtom - Satellite cluster selection ","version":"Next","tagName":"h3"},{"title":"Filter System​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#filter-system","content":"Advanced filtering supports: Quick Search: Searches across job name, pipeline path, ID, owner, and user emailStatus: Filters by job status (automatically applied per table)Timeframe: Filter by recent time ranges (last hour, day, week, etc.)Timeline: Filter by specific start/completion date rangesDuration: Filter by execution duration (more than / less than)Parameters: Filter by custom parameter key-value pairsJob Type: Filter by environment configurationOwner/Group: Filter by job ownershipBilling Project: Filter by billing project IDUser: Role-based filtering (non-admins see only their own jobs unless published) Filters persist in URL query parameters for shareable links and browser navigation. ","version":"Next","tagName":"h3"},{"title":"Role-Based Access​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#role-based-access","content":"Email Filter: Users without WorkloadRoles.filters.email permission automatically see only their own jobs and published jobsPublic Jobs: Jobs marked as &quot;published&quot; are visible to all usersFilter enforcement happens at GraphQL query level with user filter parameter ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Create and Run a New Job​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#create-and-run-a-new-job","content":"Click &quot;Create Immediate Job&quot; button in top-rightConfigure job settings (name, environment, git, parameters)Set optional notifications and resource allocationsClick &quot;Create&quot; to launch the jobJob appears in &quot;Running&quot; tab with &quot;in progress&quot; or &quot;pending&quot; statusMonitor logs via &quot;Logs&quot; column (events dialog or Grafana link)Upon completion, job moves to &quot;History&quot; tab ","version":"Next","tagName":"h3"},{"title":"Clone an Existing Job​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#clone-an-existing-job","content":"Navigate to desired job in Running or History tableClick &quot;Actions&quot; menu on the job rowSelect &quot;Clone&quot; from dropdownModify configuration in pre-populated dialogCreate the cloned jobNew job starts with same configuration as original ","version":"Next","tagName":"h3"},{"title":"Monitor Job Progress​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#monitor-job-progress","content":"Find job in &quot;Running&quot; tabClick job name to view detailed informationUse &quot;Logs&quot; column to access real-time eventsClick Grafana icon to view comprehensive logs in Grafana dashboardMonitor status, duration, and resource utilizationTerminate job if needed via Actions menu ","version":"Next","tagName":"h3"},{"title":"Filter Jobs by Parameters​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#filter-jobs-by-parameters","content":"Apply filters via toolbar filter optionsClick on parameter chip (if parameters already filtered)Add/edit parameter key-value pairs in dialogApply filters to see matching jobsClear filters using &quot;Reset Filters&quot; button ","version":"Next","tagName":"h3"},{"title":"Convert to Scheduled Job​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#convert-to-scheduled-job","content":"Locate immediate job in tableClick &quot;Actions&quot; menuSelect &quot;Convert to Scheduled&quot;Configure schedule settings (cron expression, timezone)Save to create scheduled version of job ","version":"Next","tagName":"h3"},{"title":"Publish a Job for Team Access​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#publish-a-job-for-team-access","content":"Find your job in the History or Running tableClick &quot;Actions&quot; menuSelect &quot;Publish&quot; to make job visible to all team membersPublished jobs show &quot;published&quot; chip indicatorOther users can now view and clone your job ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#related-features","content":"Scheduled Jobs - Recurring jobs with cron schedulesServices - Long-running microservices with exposed portsSessions - Interactive development environmentsEnvironment Configs - Job runtime configurationsSatellite Clusters - Remote cluster management ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Performance Optimization​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#performance-optimization","content":"Tables load 20 jobs per page to balance performance and usabilitySatellite cluster jobs poll every 5 seconds for status updatesPrimary cluster jobs use standard GraphQL queries without pollingFilter operations include email-based scoping to reduce query load ","version":"Next","tagName":"h3"},{"title":"Job Status Lifecycle​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#job-status-lifecycle","content":"pending: Job created, waiting for resourcesin progress: Job actively runningdone: Job completed successfullyfailed: Job encountered errors (check Status Reason tooltip)cancelled: Job manually terminated ","version":"Next","tagName":"h3"},{"title":"Status Reason Interpretation​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#status-reason-interpretation","content":"&quot;Timed out&quot;: BackoffLimitExceeded - Job exceeded retry limit&quot;All retries failed&quot;: DeadlineExceeded - Job exceeded active timeoutCustom reasons displayed as-is from Kubernetes ","version":"Next","tagName":"h3"},{"title":"Best Practices​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#best-practices","content":"Pin Important Jobs: Use pin feature to keep critical jobs at top of listUse Descriptive Names: Job names appear in all views and should be meaningfulSet Appropriate Timeouts: Configure both timeout (total) and activeTimeout (per retry)Leverage Parameters: Use parameters for flexible, reusable job configurationsMonitor Billing: Use Billing Project ID to track costs per projectEnable Notifications: Configure alerts for long-running or critical jobsPublish for Collaboration: Share successful job configurations with team via publish feature ","version":"Next","tagName":"h3"},{"title":"Limitations​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#limitations","content":"Jobs cannot be edited after creation (clone and modify instead)Maximum 20 jobs displayed per pageStatus updates for primary cluster jobs not real-time (refresh manually)Satellite cluster jobs limited to available satellite clustersFilter persistence tied to URL (clearing browser history loses filters) ","version":"Next","tagName":"h3"},{"title":"Grafana Integration​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#grafana-integration","content":"Grafana links automatically generated for jobs with logsLinks include time range scoping based on job start/completion timesSatellite cluster Grafana uses cluster-specific domainJobs must have completed or be running to generate valid links ","version":"Next","tagName":"h3"},{"title":"Satellite Cluster Considerations​","type":1,"pageTitle":"Immediate Jobs","url":"/developer-docs/features/jobs#satellite-cluster-considerations","content":"Must select a satellite cluster before creating jobsSatellite jobs have separate configuration dialogPod specs and resources validated against satellite cluster capabilitiesClone operations preserve satellite cluster contextSatellite jobs cannot be converted to primary cluster jobs ","version":"Next","tagName":"h3"},{"title":"Outbound Traffic Access","type":0,"sectionRef":"#","url":"/developer-docs/features/outbound-traffic-access","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#overview","content":"The Outbound Traffic Access panel provides granular control over network egress (outbound traffic) from Kubernetes namespaces in the Shakudo cluster. This feature enables administrators to implement security policies by restricting which external hosts and services workloads can access, including a cluster-wide Air Gap Mode for complete network isolation. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#access--location","content":"Route: ?panel=outbound-traffic-accessNavigation: Network → Outbound Traffic AccessAccess Requirements: dashboard-adminFeature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Cluster-Wide Air Gap Mode​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#cluster-wide-air-gap-mode","content":"Enable or disable Air Gap Mode across the entire cluster. When enabled, all namespaces are completely isolated from external networks, blocking all outbound traffic. This provides maximum security for sensitive environments but prevents access to external APIs, package repositories, and other internet resources. ","version":"Next","tagName":"h3"},{"title":"Per-Namespace Traffic Control​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#per-namespace-traffic-control","content":"Configure outbound traffic policies for individual namespaces with three granular access levels: Full Access: Allow all outbound traffic from the namespaceNo Access: Block all outbound traffic (complete isolation)Partial Access: Allow traffic only to specific whitelisted hosts ","version":"Next","tagName":"h3"},{"title":"Host Whitelisting​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#host-whitelisting","content":"Define specific external hosts that workloads in a namespace can access. Supports domain patterns including wildcards (e.g., *.example.com, api.service.com). ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#main-view","content":"The panel displays a grid of namespace cards, with each card showing: Namespace nameCurrent access status (indicated by shield icon): Green shield (solid): Namespace has no outbound access and is protectedGreen shield (outlined): Namespace has partial access (specific hosts whitelisted)No icon: Namespace has full outbound access Settings button to configure namespace-specific policies The view includes: Air Gap Mode toggle (top right): Global switch to enable/disable cluster-wide traffic blockingSearch field: Filter namespaces by nameBackdrop overlay: When Air Gap Mode is enabled or updating, an informational overlay appears over all namespace cards ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#dialogs--modals","content":"Air Gap Mode Confirmation Dialog Purpose: Confirm enabling or disabling cluster-wide Air Gap ModeActions: Confirm or CancelWarning: Explains that enabling blocks all outbound traffic, disabling restores access Namespace Settings Dialog Purpose: Configure outbound traffic policy for a specific namespaceFields: Three access level options (radio buttons): Full Access: Allow all outbound trafficNo Access: Allow no outbound trafficPartial Access: Allow only some hosts Host(s) field: Comma-separated list of allowed hosts (visible only when Partial Access is selected) Actions: Save or CloseValidation: Hosts must match the pattern *.domain.com or subdomain.domain.com ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#tables--data-grids","content":"The panel uses a responsive grid layout (not a table) to display namespace cards: Grid adapts to screen size: 6 columns (xs), 4 columns (sm), 2 columns (md)Each card is clickable to open settingsCards are disabled when Air Gap Mode is active ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#graphql-operations","content":"Queries: getNamespaces - Retrieves all user-accessible namespaces (excludes system namespaces like istio-system, kube-system, hyperplane-core, etc.)isAirGapModeOn - Checks if cluster-wide Air Gap Mode is currently enablednamespace($namespace) - Fetches outbound egress control details for a specific namespace, including: allowGlobalOutboundAccess: Boolean indicating if namespace has full accessallowedHosts: Array of whitelisted host patternsoutboundTrafficPolicyActive: Boolean indicating if traffic policy is active Mutations: toggleAirGapMode($enable) - Enables or disables cluster-wide Air Gap ModeenableNamespaceBlanketAccess($namespace) - Grants full outbound access to a namespacedisableNamespaceBlanketAccess($namespace) - Removes all outbound access from a namespacesetNamespaceAllowedHosts($namespace, $hosts) - Sets specific whitelisted hosts for a namespace Subscriptions:None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#component-structure","content":"Main Component: components/TrafficControl/Panel.tsxNamespace Grid: components/TrafficControl/Namespaces.tsxNamespace Card: components/TrafficControl/Card.tsxAir Gap Mode Toggle: components/TrafficControl/AirGapModeSwitch.tsxSettings Dialog: components/TrafficControl/NamespaceSettings.tsx ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#state-management","content":"Uses Jotai atoms for global state: AirGapModeOnAtom: Tracks whether Air Gap Mode is enabledAirGapModeUpdatingAtom: Tracks whether Air Gap Mode is currently updating Apollo Client for GraphQL data fetching and cachingLocal component state for dialog open/close and form inputs ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Enable Air Gap Mode for Maximum Security​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#enable-air-gap-mode-for-maximum-security","content":"Navigate to Network → Outbound Traffic AccessClick the &quot;Air Gap Mode&quot; toggle switch in the top rightReview the confirmation dialog warning about blocking all outbound trafficClick &quot;Confirm&quot; to enableWait for the update to complete (backdrop overlay will show progress)All namespace cards will be disabled and overlaid with Air Gap Mode message ","version":"Next","tagName":"h3"},{"title":"Configure Namespace for Partial Access​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#configure-namespace-for-partial-access","content":"Locate the target namespace card in the grid (use search if needed)Click the &quot;Settings&quot; button on the namespace cardIn the dialog, select &quot;Partial Access&quot;Enter comma-separated host patterns in the hosts field (e.g., api.github.com, *.npmjs.org)Click &quot;Save&quot;The namespace card will display a green outlined shield icon indicating partial access ","version":"Next","tagName":"h3"},{"title":"Block All Traffic from a Namespace​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#block-all-traffic-from-a-namespace","content":"Find the namespace card and click &quot;Settings&quot;Select &quot;No Access&quot; optionClick &quot;Save&quot;The namespace card will display a solid green shield icon indicating it's protected ","version":"Next","tagName":"h3"},{"title":"Restore Full Access to a Namespace​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#restore-full-access-to-a-namespace","content":"Open the namespace's Settings dialogSelect &quot;Full Access&quot; optionClick &quot;Save&quot;The shield icon will disappear, indicating unrestricted access ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#related-features","content":"Stack Components Panel - Components deployed in namespaces are affected by traffic policiesSessions Panel - Development environments run in namespaces and may need external accessJobs Panel - Pipeline jobs run in namespaces and may require access to external data sources ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Important Considerations​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#important-considerations","content":"Air Gap Mode overrides all namespace settings: When enabled, even namespaces configured for full access will be blockedSystem namespaces are excluded: Core infrastructure namespaces (istio-system, kube-system, hyperplane-core, etc.) are not shown or affected by these controlsChanges take effect immediately: There is no grace period when enabling restrictionsValidation is strict: Host patterns must be valid domains with optional wildcard prefixes (e.g., *.example.com is valid, example.* is not) ","version":"Next","tagName":"h3"},{"title":"Best Practices​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#best-practices","content":"Start with least privilege: Configure namespaces with No Access by default, then add specific hosts as neededUse wildcards carefully: *.domain.com allows access to all subdomains, which may be broader than neededTest before production: Enable Air Gap Mode in a test environment first to identify external dependenciesDocument exceptions: Keep a record of which namespaces need external access and whyRegular audits: Periodically review namespace access settings to remove unnecessary permissions ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Outbound Traffic Access","url":"/developer-docs/features/outbound-traffic-access#troubleshooting","content":"Workloads failing after enabling restrictions: Check logs for connection errors, then add required hosts to the allowed listUnable to save settings: Ensure host patterns are valid domain names (no IP addresses, must include TLD)Changes not taking effect: Air Gap Mode may be enabled at the cluster level, blocking all traffic regardless of namespace settingsSettings button disabled: Air Gap Mode is currently enabled or updating; wait for it to complete or disable Air Gap Mode first ","version":"Next","tagName":"h3"},{"title":"Persistent Volumes","type":0,"sectionRef":"#","url":"/developer-docs/features/persistent-volumes","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#overview","content":"The Persistent Volumes panel provides comprehensive management of Kubernetes Persistent Volume Claims (PVCs) across all namespaces in the cluster. This feature enables administrators to view, monitor, and resize storage volumes used by applications and services running on the platform. It displays real-time storage usage metrics, identifies which pods are using each volume, and allows for dynamic volume expansion without service disruption. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#access--location","content":"Route: ?panel=persistent-volumesNavigation: Kubernetes Resources → Persistent VolumesAccess Requirements: dashboard-admin roleFeature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"View Persistent Volume Claims​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#view-persistent-volume-claims","content":"Browse all PVCs across all namespaces with detailed information including name, namespace, size, storage class, status, and associated pods. The main view displays PVCs in a card-based layout with color-coded status indicators and key metadata at a glance. ","version":"Next","tagName":"h3"},{"title":"Monitor Storage Usage​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#monitor-storage-usage","content":"View real-time storage usage statistics for each PVC, including total capacity, used space, available space, and usage percentage. Usage metrics are retrieved from Prometheus and displayed with visual indicators (progress bars) that change color based on utilization thresholds (green: &lt;70%, yellow: 70-90%, red: &gt;90%). ","version":"Next","tagName":"h3"},{"title":"Resize Persistent Volumes​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#resize-persistent-volumes","content":"Dynamically expand PVC storage capacity without downtime. The feature validates resize requests to ensure only size increases are allowed (volume shrinking is not supported by Kubernetes) and provides real-time feedback during the resize operation. ","version":"Next","tagName":"h3"},{"title":"Filter and Search​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#filter-and-search","content":"Filter PVCs by namespace or status, and search across multiple fields including PVC name, namespace, status, and storage class. Advanced sorting capabilities allow ordering by name, namespace, status, size, or creation date in ascending or descending order. ","version":"Next","tagName":"h3"},{"title":"Track Pod Usage​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#track-pod-usage","content":"Identify which pods are currently using each PVC, helping with dependency analysis, cleanup decisions, and capacity planning. The interface shows a count of pods using each volume and provides detailed pod names in the details view. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#main-view","content":"The main panel displays PVCs as interactive cards in a paginated list. Each card shows: Avatar: Color-coded avatar with the first letter of the PVC nameName: Full PVC nameNamespace: Namespace chip (outlined style)Status: Color-coded status chip (green for Bound, yellow for Pending, red for Lost)Storage Size: Allocated capacity (e.g., &quot;10Gi&quot;)Storage Class: Type of storage (e.g., &quot;local-path&quot;, &quot;standard&quot;)Pod Count: Number of pods currently using the volumeCreation Date: When the PVC was created The interface includes a comprehensive control panel with: Filter Section: Dropdowns to filter by namespace and statusSort Section: Dropdown to select sort field (Name, Namespace, Status, Size, Created) with toggle button for ascending/descending orderSearch Bar: Text input with live search across name, namespace, status, and storage classAction Buttons: Reset filters button (appears when filters are active) and refresh button to reload dataResult Counter: Shows current page results, filtered total, and grand total ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#dialogs--modals","content":"PVC Details Dialog Purpose: Display comprehensive information about a selected PVC and enable resize operationsTriggered by: Clicking on any PVC card in the main viewSections: Basic Information: Name, namespace, status, size, storage class, creation timestampPod Usage: List of all pods currently mounting this PVCStorage Usage: Real-time usage statistics from Prometheus (if available) Total capacity vs. used spaceVisual progress bar with color codingAvailable space calculationSource pod from which metrics were collectedSpecial note for local-path storage explaining that metrics reflect node filesystem usage Resize PVC: Interface to expand volume capacity Fields: New Size: Text input accepting Kubernetes size formats (e.g., &quot;20Gi&quot;, &quot;500Mi&quot;, &quot;2Ti&quot;)Real-time validation to ensure new size is larger than current sizeHelper text with format examples and validation errors Actions: Resize PVC: Submit button (disabled until valid size is entered)Close: Dismiss dialog without changes ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#tables--data-grids","content":"The main view uses a card-based layout rather than a traditional table, providing: Columns (displayed as card metadata): Name (with avatar)Namespace (chip)Status (colored chip)Storage Size (icon + text)Storage Class (icon + text)Pod Count (icon + text)Created Date (icon + formatted date) Actions: Click any card to open the details dialogFiltering: Namespace filter (dropdown with all unique namespaces)Status filter (dropdown with all unique statuses)Global search (searches across name, namespace, status, storage class) Sorting: Sort by Name, Namespace, Status, Size, or Created dateToggle between ascending and descending orderProper numeric sorting for sizes (converts to bytes for accurate comparison) Pagination: 20 items per page with first/last/prev/next navigation ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"API Operations​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#api-operations","content":"Endpoints: GET /api/pvcs/list - Retrieve all PVCs across all namespaces with pod usage informationPOST /api/pvcs/usage - Get real-time storage usage metrics for a specific PVC from PrometheusPOST /api/pvcs/resize - Resize a PVC by updating its storage request Authentication:All API endpoints require: Bearer token authentication (Keycloak JWT)dashboard-admin role authorization List PVCs Operation: Queries Kubernetes API for all PVCs across namespacesQueries all pods to determine PVC usage relationshipsReturns array of PVC objects with metadata and associated pod namesResponse includes: name, namespace, size, storageClass, status, volumeName, created, usingPods, podCount Usage Metrics Operation: Accepts PVC name and namespace as parametersFinds pods currently using the PVCQueries Prometheus for volume statistics: Primary metrics: kubelet_volume_stats_used_bytes and kubelet_volume_stats_capacity_bytesFallback metrics: container_fs_usage_bytes and container_fs_limit_bytes Calculates available space and usage percentageFormats bytes to human-readable units (B, Ki, Mi, Gi, Ti)Returns null usage with informative message if metrics unavailable Resize Operation: Accepts PVC name, namespace, and new sizeReads current PVC specification from KubernetesUpdates only the storage request fieldApplies the change using Kubernetes API replace operationFrontend validates size increase before allowing submission ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#component-structure","content":"Main Component: components/PVCs/PVCPanel.tsxTable Component: components/PVCs/PVCTable.tsxDialog Component: components/PVCs/PVCDetailsDialog.tsxHook: hooks/usePVCs.tsAPI Routes: pages/api/pvcs/list.tspages/api/pvcs/usage.tspages/api/pvcs/resize.ts Utilities: utils/sizeUtils.ts (size parsing and validation) ","version":"Next","tagName":"h3"},{"title":"Data Flow​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#data-flow","content":"Initial Load: PVCPanel mounts → usePVCs hook triggers → API call to /api/pvcs/list → Kubernetes API queries PVCs and Pods → Data processed and returnedUsage Metrics: User clicks PVC card → PVCDetailsDialog opens → API call to /api/pvcs/usage → Prometheus queries → Usage data displayedResize: User enters new size → Validation occurs → Resize button clicked → API call to /api/pvcs/resize → Kubernetes API updates PVC → Success notification → Dialog closes ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#state-management","content":"Local State: React hooks manage component-level state (search, filters, sorting, pagination, dialog open/close)Data Fetching: Custom usePVCs hook handles API calls with loading, error, and refetch capabilitiesDialog State: Separate state for resize operation status, usage loading, and error messagesAbort Controllers: Used to prevent memory leaks by canceling in-flight requests when components unmount ","version":"Next","tagName":"h3"},{"title":"Special Considerations​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#special-considerations","content":"Local-Path Storage: Special handling and user notification that usage metrics for local-path storage reflect entire node filesystem, not just the PVCSize Validation: Frontend validates that new size is greater than current size using byte conversion for accurate comparisonPrometheus Metrics: Graceful degradation if Prometheus is unavailable or metrics not collectedMemory Leak Prevention: Abort controllers and mounted refs prevent state updates after component unmountsPod Status: Usage metrics only available from running pods; appropriate messaging for stopped/pending pods ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"View Storage Capacity Across Cluster​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#view-storage-capacity-across-cluster","content":"Navigate to Persistent Volumes panelReview the list of all PVCs with their sizes and statusesUse namespace filter to focus on specific applicationsSort by size to identify largest volumesReview pod count to understand utilization ","version":"Next","tagName":"h3"},{"title":"Check Storage Usage for a Specific Volume​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#check-storage-usage-for-a-specific-volume","content":"Locate the PVC in the main list (use search if needed)Click on the PVC card to open details dialogView the Storage Usage sectionCheck the usage percentage and visual progress barNote available space and consider resize if needed ","version":"Next","tagName":"h3"},{"title":"Expand a Volume That's Running Out of Space​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#expand-a-volume-thats-running-out-of-space","content":"Open the PVC details dialog for the target volumeReview current usage in the Storage Usage sectionScroll to the Resize PVC sectionEnter new size larger than current (e.g., if current is &quot;10Gi&quot;, enter &quot;20Gi&quot;)Verify that validation passes (no error shown)Click &quot;Resize PVC&quot; buttonWait for confirmation notificationClose dialog and monitor the PVC status ","version":"Next","tagName":"h3"},{"title":"Identify Unused Volumes for Cleanup​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#identify-unused-volumes-for-cleanup","content":"Review the PVC list in the main panelLook for PVCs with pod count of 0Click on suspected unused PVCs to verify no pods listedNote: This panel provides viewing only; deletion requires other tools or kubectl ","version":"Next","tagName":"h3"},{"title":"Troubleshoot Storage Issues​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#troubleshoot-storage-issues","content":"Search for the problematic PVC using the search barOpen details dialog to check statusReview which pods are using the volumeCheck storage usage metrics if pod is runningVerify storage class for provisioner issuesCheck creation timestamp for age-related issues ","version":"Next","tagName":"h3"},{"title":"Filter PVCs by Namespace for Team-Specific Review​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#filter-pvcs-by-namespace-for-team-specific-review","content":"Use the namespace dropdown filter in the control panelSelect the target namespace (e.g., &quot;hyperplane-pipelines&quot;, &quot;hyperplane-jhub&quot;)Review all PVCs in that namespaceSort by size or creation date as neededUse &quot;Reset all filters&quot; to return to full view ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#related-features","content":"Job Management - Many jobs create temporary PVCs for data persistenceSession Management - JupyterHub sessions use PVCs for user workspacesMicroservices - Some microservices may mount PVCs for data storageStack Components - Database and storage components create PVCs ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"General Usage​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#general-usage","content":"Bound Status: Indicates the PVC is successfully attached to a Persistent Volume and ready for usePending Status: Usually means the provisioner is creating the volume or waiting for resourcesLost Status: Indicates the underlying storage has been lost or is unavailable ","version":"Next","tagName":"h3"},{"title":"Resize Considerations​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#resize-considerations","content":"One-Way Operation: PVCs can only be expanded, never shrunk. Plan capacity increases carefully.Storage Class Support: Not all storage classes support volume expansion. Check the allowVolumeExpansion field in the StorageClass.File System Expansion: After PVC resize, the pod may need to be restarted for the file system to recognize the new capacity.Validation: The UI validates size increases before submission, converting sizes to bytes for accurate comparison (e.g., recognizes that &quot;10Gi&quot; &lt; &quot;10000Mi&quot;). ","version":"Next","tagName":"h3"},{"title":"Storage Usage Metrics​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#storage-usage-metrics","content":"Prometheus Required: Usage statistics require Prometheus to be deployed and collecting kubelet metrics.Running Pods Only: Metrics are only available when at least one pod mounting the PVC is in Running state.Local-Path Note: For local-path storage, usage shows the entire node's filesystem usage, not just the PVC, since local-path uses a directory on the node rather than a dedicated volume.Metric Collection Delay: Newly created PVCs may not show usage immediately; allow a few minutes for metrics to populate. ","version":"Next","tagName":"h3"},{"title":"Search and Filter Tips​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#search-and-filter-tips","content":"Global Search: Searches across multiple fields simultaneously (name, namespace, status, storage class)Combined Filters: Use multiple filters together (e.g., namespace + status + search) for precise resultsReset Filters: Click the reset button to quickly return to unfiltered viewPagination: Results are paginated at 20 per page; filters reset to page 1 when changed ","version":"Next","tagName":"h3"},{"title":"Performance​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#performance","content":"Large Clusters: For clusters with hundreds of PVCs, initial load may take several secondsRefresh Button: Use the refresh button to reload data after making changes outside the UIResponsive Design: The interface adapts to mobile and tablet screens with adjusted layouts ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Persistent Volumes","url":"/developer-docs/features/persistent-volumes#troubleshooting","content":"Usage Metrics Not Available: Check that Prometheus is running and that kubelet metrics are being scrapedResize Fails: Verify the storage class supports volume expansion and check Kubernetes events for detailed error messagesEmpty List: Ensure you have dashboard-admin role and that PVCs exist in the cluster ","version":"Next","tagName":"h3"},{"title":"Plugins Panel","type":0,"sectionRef":"#","url":"/developer-docs/features/plugins","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#overview","content":"The Plugins panel provides access to n8n, a powerful workflow automation platform integrated into Shakudo. n8n enables users to create automated workflows that connect APIs, services, databases, and other Shakudo stack components through a visual interface. This panel loads the n8n web interface in an embedded view, allowing users to design, execute, and monitor automation workflows without leaving the Shakudo dashboard. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#access--location","content":"Route: ?panel=pluginsNavigation: Header (Quick Access) → PluginsAccess Requirements: Must have pluginsPanelEnabled feature flag enabled at the platform levelNo specific RBAC requirements - access follows n8n's own authentication Feature Flags: pluginsPanelEnabled (environment variable: HYPERPLANE__DASHBOARD_PLUGINS_PANEL_ENABLED) ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Workflow Automation Hub​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#workflow-automation-hub","content":"Access n8n as a central automation hub for orchestrating processes across the Shakudo platform. Design visual workflows that connect multiple services, trigger actions based on events, and automate data pipelines without writing code. ","version":"Next","tagName":"h3"},{"title":"Visual Workflow Designer​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#visual-workflow-designer","content":"Use n8n's drag-and-drop interface to build automation workflows with nodes representing different services, actions, and logic. Connect nodes to create complex automation scenarios that span multiple Shakudo components. ","version":"Next","tagName":"h3"},{"title":"Stack Component Integration​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#stack-component-integration","content":"Connect n8n workflows to other Shakudo stack components using internal Kubernetes service URLs. Integrate with databases (Supabase, ClickHouse), AI services (Ollama, LiteLLM), messaging systems (Kafka), observability tools (Langfuse), and more. ","version":"Next","tagName":"h3"},{"title":"Event-Driven Automation​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#event-driven-automation","content":"Trigger workflows based on webhooks, scheduled events, Kafka messages, or data changes in connected services. Create responsive automation that reacts to events across your data infrastructure. ","version":"Next","tagName":"h3"},{"title":"Shakudo Pipeline Orchestration​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#shakudo-pipeline-orchestration","content":"Trigger and monitor Shakudo immediate and scheduled jobs from n8n workflows. Build complex data processing pipelines that combine Shakudo's compute capabilities with n8n's automation logic. ","version":"Next","tagName":"h3"},{"title":"AI Workflow Automation​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#ai-workflow-automation","content":"Query LLM models hosted on Ollama or routed through LiteLLM directly from n8n workflows. Build AI-powered automation for content generation, text summarization, data analysis, and intelligent decision-making. ","version":"Next","tagName":"h3"},{"title":"Data Processing and ETL​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#data-processing-and-etl","content":"Fetch, transform, and load data between different systems. Create ETL workflows that move data between Supabase, ClickHouse, MinIO, and external APIs, with built-in error handling and retry logic. ","version":"Next","tagName":"h3"},{"title":"Monitoring and Observability​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#monitoring-and-observability","content":"Integrate with Langfuse to log and trace workflow executions, API calls, and LLM responses. Track workflow performance and debug issues with detailed execution history. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#main-view","content":"The Plugins panel displays a full-screen iframe that loads the n8n web interface from the plugins subdomain (e.g., https://plugins.{domain}). The iframe adapts to the dashboard's navigation state: Desktop View: Left padding adjusts based on whether the main navigation drawer is locked (expanded) or collapsedMobile View: No left padding, full-width display for optimal mobile experienceResponsive Layout: Automatically adjusts to viewport size changes The n8n interface within the iframe provides: Canvas: Visual workflow editor with drag-and-drop nodesNode Panel: Library of available integrations and actionsExecution History: View past workflow runs and debug failuresCredentials Manager: Securely store API keys and connection detailsSettings: Configure workflow behavior, error handling, and scheduling ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#dialogs--modals","content":"The Plugins panel itself does not implement any dialogs. All interactions occur within the embedded n8n interface, which has its own modal dialogs for: Node configurationCredential managementWorkflow settingsExecution detailsError messages ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#tables--data-grids","content":"No tables are implemented in the panel component. All data visualization occurs within the n8n interface. ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#graphql-operations","content":"Queries:None - The panel uses iframe embedding and does not make direct GraphQL calls Mutations:None - The panel uses iframe embedding and does not make direct GraphQL calls Subscriptions:None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#component-structure","content":"Main Component: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/Plugins/PluginPanel.tsxExport Name: N8nPanel (exported as default) ","version":"Next","tagName":"h3"},{"title":"Implementation Details​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#implementation-details","content":"The panel is implemented as a simple iframe wrapper that: Constructs the n8n URL from platform parameters: ${protocol}://plugins.${domain}Applies responsive padding based on navigation drawer state (via DrawerLockedAtom)Detects mobile viewport using Material-UI's responsive breakpointsRenders a borderless iframe with 100% height and width ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#state-management","content":"Jotai Atom: DrawerLockedAtom - Tracks whether the main navigation drawer is locked/expandedContext: PlatformParametersContext - Provides domain and protocol for URL constructionResponsive State: Material-UI's useMediaQuery for mobile detection ","version":"Next","tagName":"h3"},{"title":"URL Pattern​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#url-pattern","content":"The n8n instance is hosted on a dedicated subdomain following the pattern: External URL: https://plugins.{domain} (e.g., https://plugins.dev.hyperplane.dev)Protocol: Inherits from platform configuration (typically HTTPS) ","version":"Next","tagName":"h3"},{"title":"Feature Flag Configuration​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#feature-flag-configuration","content":"The panel is only accessible when the platform administrator enables it via environment variable: HYPERPLANE__DASHBOARD_PLUGINS_PANEL_ENABLED=true  ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Automating AI-Powered Reporting​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#automating-ai-powered-reporting","content":"Create a new workflow in n8nAdd a webhook trigger or schedule trigger nodeAdd an HTTP Request node to query Ollama for AI-generated summaries: URL: http://ollama.hyperplane-ollama.svc.cluster.local:11434Method: POSTInclude your prompt and model parameters Add a Supabase node to store the AI-generated resultsAdd a notification node (Slack, email) to send the reportActivate the workflow and test execution ","version":"Next","tagName":"h3"},{"title":"Event-Driven Data Pipeline​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#event-driven-data-pipeline","content":"Create a workflow with a Kafka trigger nodeConfigure the Kafka node with your topic and connection detailsAdd processing nodes to transform the incoming event dataAdd an HTTP Request node to trigger a Shakudo immediate job: Use the Shakudo API to create and monitor the jobPass event data as job parameters Add a Supabase node to log the pipeline executionAdd a Langfuse node to track observability metricsActivate the workflow to process events in real-time ","version":"Next","tagName":"h3"},{"title":"Scheduled Data Ingestion from External API​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#scheduled-data-ingestion-from-external-api","content":"Create a workflow with a Schedule trigger (e.g., daily at midnight)Add an HTTP Request node to fetch data from an external APIAdd data transformation nodes (Set, Function) to clean and format the dataAdd a database node (Supabase, ClickHouse) to insert the processed dataAdd error handling with conditional logic to retry failuresAdd a notification node to alert on success or failureActivate the workflow and monitor executions ","version":"Next","tagName":"h3"},{"title":"Connecting Multiple Stack Components​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#connecting-multiple-stack-components","content":"Map out your workflow across Supabase, Ollama, LiteLLM, and AppsmithCreate a workflow that: Fetches data from Supabase (PostgreSQL node)Processes data with AI using Ollama or LiteLLM (HTTP Request nodes)Stores enriched data back to SupabaseUpdates an Appsmith dashboard via API call Configure internal Kubernetes service URLs for all components: Supabase: postgresql://&lt;user&gt;:&lt;pass&gt;@supabase.hyperplane-supabase:5432/&lt;db&gt;Ollama: http://ollama.hyperplane-ollama.svc.cluster.local:11434LiteLLM: http://litellm.hyperplane-litellm.svc.cluster.local:4000Langfuse: http://langfuse.hyperplane-langfuse.svc.cluster.local:3000 Test each connection individually before running the full workflowMonitor execution logs in n8n and observability in Langfuse ","version":"Next","tagName":"h3"},{"title":"Triggering and Monitoring Shakudo Jobs​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#triggering-and-monitoring-shakudo-jobs","content":"Create a workflow with an appropriate trigger (webhook, schedule, event)Add an HTTP Request node to create a Shakudo immediate job: Use the Shakudo GraphQL API or REST endpointInclude job parameters (script, environment, resources) Add a Wait node or polling logic to monitor job completionAdd conditional logic to handle job success or failureAdd follow-up actions based on job results (notifications, data processing)Store job metadata in Supabase for audit trailActivate and test the workflow with a test job ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#related-features","content":"Stack Components - Install and manage n8n and other automation toolsJobs - Shakudo pipeline jobs that can be triggered from n8n workflowsScheduled Jobs - Recurring jobs that can complement n8n automationServices - Long-running services that n8n can interact withSecrets - Store API credentials used by n8n workflows securely ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Best Practices​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#best-practices","content":"Use Internal URLs: Always connect to stack components using Kubernetes internal service URLs (e.g., http://service.namespace.svc.cluster.local:port) for better performance and securityStore Credentials Securely: Use n8n's credential manager to store API keys and database passwords instead of hardcoding them in workflowsError Handling: Add error handling nodes to workflows to gracefully handle failures and retry transient errorsTest Incrementally: Build workflows step-by-step, testing each node individually before connecting them togetherMonitor Execution History: Regularly review workflow execution logs to identify bottlenecks and failuresUse Langfuse: Integrate Langfuse for observability when building AI-powered workflows to track LLM usage and performance ","version":"Next","tagName":"h3"},{"title":"Integration Examples​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#integration-examples","content":"Connecting to Supabase: Host: supabase.hyperplane-supabase Port: 5432 Database: postgres (or your database name) User: postgres (or your username) Password: (use n8n credentials manager)  Querying Ollama: URL: http://ollama.hyperplane-ollama.svc.cluster.local:11434/api/generate Method: POST Body: { &quot;model&quot;: &quot;llama3.2&quot;, &quot;prompt&quot;: &quot;Your prompt here&quot;, &quot;stream&quot;: false }  Using LiteLLM Gateway: URL: http://litellm.hyperplane-litellm.svc.cluster.local:4000/chat/completions Method: POST Headers: Authorization: Bearer &lt;your-api-key&gt; Body: OpenAI-compatible format  ","version":"Next","tagName":"h3"},{"title":"Common Use Cases​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#common-use-cases","content":"Automated AI Workflows Process user inputs and query Ollama for content generationUse LiteLLM to route requests across multiple LLM providersStore AI-generated results in Supabase for application use Data Ingestion and ETL Fetch data from external APIs on a scheduleTransform and clean data using n8n's built-in functionsLoad data into Supabase, ClickHouse, or MinIO Pipeline Orchestration Trigger Shakudo immediate jobs when new data arrivesMonitor job completion and process resultsChain multiple jobs together with conditional logic Event-Driven Processing Subscribe to Kafka topics and process messagesReact to database changes via webhooksTrigger workflows based on external system events Observability and Monitoring Log all workflow executions to LangfuseTrack LLM usage and costsSend alerts to Slack or email on failures ","version":"Next","tagName":"h3"},{"title":"Performance Considerations​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#performance-considerations","content":"Workflow Complexity: Complex workflows with many nodes may have longer execution times - consider breaking them into smaller, chained workflowsExternal API Calls: Network latency to external APIs can slow down workflows - use appropriate timeout settingsPolling vs Webhooks: Prefer webhook triggers over polling when possible to reduce resource usageConcurrent Executions: n8n supports concurrent workflow executions, but be mindful of rate limits on connected servicesData Volume: For large data processing tasks, consider triggering Shakudo jobs instead of processing directly in n8n ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Common Issues​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#troubleshooting-common-issues","content":"Connection Timeouts: Verify internal service URLs are correct and services are running in their namespacesAuthentication Failures: Check that credentials are properly configured in n8n's credential managerWorkflow Stuck: Check the execution log for the specific node that's blocking - may need to adjust timeout settingsMissing Data: Verify that the previous node's output format matches the expected input format of the next nodeService Not Accessible: Ensure the stack component is installed, active, and has the correct service name in its namespace ","version":"Next","tagName":"h3"},{"title":"Important Limitations​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#important-limitations","content":"Iframe Limitations: The n8n interface runs in an iframe, which may have some browser restrictions on cookies, storage, or popups depending on browser security settingsAuthentication: n8n authentication is separate from Shakudo dashboard authentication - users need n8n credentials to access workflowsFeature Flag Required: The Plugins panel must be explicitly enabled by platform administrators via environment variableNo Direct GraphQL: The panel does not interact with Shakudo's GraphQL API directly - all n8n operations happen within the iframeSubdomain Dependency: Requires a properly configured plugins subdomain pointing to the n8n instance ","version":"Next","tagName":"h3"},{"title":"Installation and Setup​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#installation-and-setup","content":"n8n must be installed as a Shakudo stack component before the Plugins panel becomes useful. The installation process: Install n8n via Stack Components panel or Helm chartConfigure the n8n namespace (typically hyperplane-n8n)Set up Keycloak redirect URLs for SSO integration (optional)Create GraphQL mutation to register n8n as a platform appEnable the pluginsPanelEnabled feature flagAccess n8n through the Plugins panel For detailed installation instructions, refer to the n8n stack component documentation in /stack-components/n8n/. ","version":"Next","tagName":"h3"},{"title":"Workflow Development Tips​","type":1,"pageTitle":"Plugins Panel","url":"/developer-docs/features/plugins#workflow-development-tips","content":"Start Simple: Begin with basic workflows and gradually add complexityUse the Manual Trigger: Test workflows manually before activating production triggersVersion Control: Export workflows as JSON files and store them in git for versioningDocumentation: Add notes to workflow nodes to document logic and integration detailsNaming Conventions: Use clear, descriptive names for workflows and nodesResource Monitoring: Monitor workflow execution times and resource usage to optimize performance ","version":"Next","tagName":"h3"},{"title":"Satellite Clusters","type":0,"sectionRef":"#","url":"/developer-docs/features/satellite-clusters","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#overview","content":"The Satellite Clusters panel enables administrators to configure and manage connections to external Kubernetes clusters that operate as satellite deployments of the main Shakudo platform. These satellite clusters extend the platform's capabilities by allowing workloads (jobs, services, sessions) to run on geographically distributed or resource-specific infrastructure while maintaining centralized management through the main dashboard. Satellite clusters are typically used for: Running workloads in different geographic regionsDeploying to customer-owned infrastructureIsolating workloads with specific compliance or security requirementsScaling beyond the primary cluster's capacity ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#access--location","content":"Route: ?panel=satellite-clustersNavigation: Admin → Satellite ClustersAccess Requirements: Dashboard Admin role (DASHBOARD_ADMIN_ROLE)Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Link Satellite Clusters​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#link-satellite-clusters","content":"Connect external Kubernetes clusters to the main Shakudo platform by providing cluster configuration details. Linked satellite clusters appear as deployment targets for jobs, services, and other workloads. ","version":"Next","tagName":"h3"},{"title":"View Cluster Inventory​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#view-cluster-inventory","content":"Browse all configured satellite clusters with key metadata including cluster name, domain, description, creation date, and the user who created the link. ","version":"Next","tagName":"h3"},{"title":"Search and Filter​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#search-and-filter","content":"Search across all cluster properties (ID, name, domain, description, user ID) to quickly locate specific satellite clusters in large deployments. ","version":"Next","tagName":"h3"},{"title":"Unlink Satellite Clusters​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#unlink-satellite-clusters","content":"Remove satellite cluster connections from the platform. The system automatically identifies and warns about dependent resources (like git servers) that will be affected by the removal. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#main-view","content":"The panel displays a data grid table with the following features: Title: &quot;Satellite Clusters&quot; with subtitle &quot;Configure a connection to an existing satellite cluster&quot;Create Button: &quot;Link a Satellite Cluster&quot; button (top-right, admin-only)Search Bar: Filter by any property valueColumn Controls: Toggle column visibilityRefresh Button: Manually refresh the cluster listPagination: 20 clusters per page ","version":"Next","tagName":"h3"},{"title":"Table Columns​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#table-columns","content":"The satellite clusters table displays: Action Column Delete/Unlink button (trash icon) for each clusterOnly visible to users with admin permissions ID Unique cluster identifier (first 6 characters shown)Click to copy full ID to clipboardWidth: Medium (MEDIUM_TABLE_COLUMN) Name Human-readable cluster nameTruncated with ellipsis after 30 charactersClick to copy full name to clipboardWidth: Extra Large (X_LARGE_TABLE_COLUMN) Domain Cluster domain/endpoint URLTruncated with ellipsis after 30 charactersClick to copy full domain to clipboardWidth: Large (LARGE_TABLE_COLUMN) Created By Email address of the user who created the cluster linkWidth: Medium (MEDIUM_TABLE_COLUMN) Created On Timestamp in format: YYYY-MM-DD HH:mm:ssWidth: Date column (DATE_TABLE_COLUMN) Description Optional cluster descriptionTruncated with ellipsis after 64 charactersWidth: Flexible (flex: 1) ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#dialogs--modals","content":"Link a Satellite Cluster Dialog Purpose: Create a new satellite cluster connectionTrigger: Click &quot;Link a Satellite Cluster&quot; buttonFields: Name (required): Alphanumeric cluster identifier Must begin and end with alphanumeric charactersCan contain dashes, underscores, dots between charactersMaximum 128 charactersNo spaces allowed Description (optional): Free-text cluster descriptionDomain (required): Cluster domain name Must be a valid domain format (e.g., cluster.example.io)Validated against regex pattern for proper domain structure Actions: Cancel: Close dialog without savingCreate: Submit and create the cluster link Validation: Real-time form validation with error messagesSuccess notification: &quot;Created new satellite cluster '[name]'&quot; Delete Satellite Cluster Confirmation Purpose: Confirm cluster unlink operationTrigger: Click delete icon (trash/X) in table rowContent: Warning message: &quot;Are you sure you want to delete satellite cluster '[name]'?&quot;List of dependent git servers that will be deleted (if any) Actions: Cancel: Close dialog without deletingDelete: Permanently unlink the cluster Pre-deletion check: Queries for satellite git servers associated with the clusterSuccess notification: &quot;Deactivated satellite cluster '[name]'&quot; ","version":"Next","tagName":"h3"},{"title":"Interactive Features​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#interactive-features","content":"Copy to Clipboard: Click any ID, name, or domain value to copy to clipboard with success notificationSearch/Filter: Type in search bar to filter clusters by any field (ID, name, domain, description, user)Column Visibility: Use column selector to show/hide specific columnsRefresh: Manual refresh button to reload cluster list from serverLoading States: Progress indicators during data fetching and operations ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#graphql-operations","content":"Queries: getSatelliteClusters - Retrieves paginated list of satellite clusters with optional filtering Variables: $offset, $limit, $where (HyperplaneSatelliteClusterWhereInput)Returns: id, name, domain, description, hyperplaneUserEmail, creationDateOrder: Alphabetically by name (ascending) getSatelliteGitServers - Retrieves git servers associated with a satellite cluster Variables: $satelliteClusterNameUsed in delete confirmation to show dependent resourcesFetch policy: network-only (always fresh data) Mutations: createSatelliteCluster - Creates a new satellite cluster connection Variables: $name (required), $domain (required), $description, $hyperplaneUserEmailReturns: id, name, domain, description, hyperplaneUserEmail, creationDate deleteSatelliteCluster - Removes a satellite cluster connection Variables: $name (required)Returns: id, nameNote: Also deletes associated satellite git servers Subscriptions: None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#component-structure","content":"Main Component: /components/SatelliteClusters/SatelliteClusterPanel.tsxDialogs: /components/SatelliteClusters/Dialogs/SatelliteClusterCreateDialog.tsx/components/SatelliteClusters/Dialogs/SatelliteClusterDeleteDialog.tsx Hook: /hooks/useSatelliteClusters.tsGraphQL Schema: /graphql/hyperplaneSatelliteCluster/Roles: /constants/Roles/satellite-clusters.ts ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#state-management","content":"Search State: Local component state for search input filteringDialog State: Local state for dialog open/close controlUser Roles: Global Jotai atom HyperplaneUserRolesAtom for permission checksForm State: React Hook Form for create dialog validation ","version":"Next","tagName":"h3"},{"title":"Search Implementation​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#search-implementation","content":"The search feature filters clusters using OR logic across multiple fields: id (contains)name (contains)domain (contains)description (contains)hyperplaneUserId (contains) Search triggers with 300ms debounce to optimize server requests. ","version":"Next","tagName":"h3"},{"title":"Permission System​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#permission-system","content":"All satellite cluster operations require the DASHBOARD_ADMIN_ROLE: Create: Admin role required (button disabled otherwise with tooltip: &quot;Insufficient Permissions&quot;)Unlink/Delete: Admin role required (delete buttons hidden for non-admins)View: Available to all users, but only admins can navigate to the panel ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Linking a New Satellite Cluster​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#linking-a-new-satellite-cluster","content":"Navigate to Admin → Satellite ClustersClick &quot;Link a Satellite Cluster&quot; button (admin only)Enter required information: Name: Unique identifier (e.g., &quot;west-coast-cluster&quot;)Domain: Cluster endpoint (e.g., &quot;west.satellite.example.com&quot;)Description: Optional notes about the cluster Click &quot;Create&quot; to submitVerify cluster appears in the tableUse the cluster as a deployment target in Jobs, Services, or Sessions panels ","version":"Next","tagName":"h3"},{"title":"Searching for a Cluster​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#searching-for-a-cluster","content":"Click the search input field at the top of the tableType any part of the cluster's name, domain, or descriptionTable automatically filters to matching clustersClick the X icon to clear the search filter ","version":"Next","tagName":"h3"},{"title":"Removing a Satellite Cluster Link​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#removing-a-satellite-cluster-link","content":"Locate the cluster in the tableClick the delete icon (trash/X) in the leftmost columnReview the confirmation dialog for warnings about dependent resourcesClick &quot;Delete&quot; to confirmVerify the cluster is removed from the tableNote: Associated git servers will also be deleted ","version":"Next","tagName":"h3"},{"title":"Copying Cluster Information​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#copying-cluster-information","content":"Locate the cluster in the tableClick on any copyable field (ID, Name, Domain)Receive clipboard confirmation messagePaste the value where needed (e.g., in configuration files or other panels) ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#related-features","content":"Immediate Jobs - Create jobs that run on satellite clustersScheduled Jobs - Schedule recurring jobs on satellite clustersServices - Deploy microservices to satellite clustersSessions - Launch development environments on satellite clustersGit Repositories - Satellite git servers depend on cluster linksEnvironment Configs - Satellite-specific environment configurationsService Accounts - Satellite service accounts for cluster authenticationSecrets - Satellite secrets for secure credential management ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Cluster Naming Conventions​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#cluster-naming-conventions","content":"Use descriptive, location-based names (e.g., &quot;us-west-2&quot;, &quot;eu-central-production&quot;)Avoid spaces and special characters (use dashes or underscores)Keep names concise but meaningful for easy identificationConsider including environment indicators (dev, staging, prod) ","version":"Next","tagName":"h3"},{"title":"Domain Requirements​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#domain-requirements","content":"Must be a fully qualified domain name (FQDN)Should point to the satellite cluster's API endpointEnsure DNS resolution is configured before linkingUse HTTPS/TLS-secured endpoints for production clusters ","version":"Next","tagName":"h3"},{"title":"Deletion Impact​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#deletion-impact","content":"Deleting a satellite cluster removes its connection from the platformAll associated satellite git servers will be automatically deletedExisting jobs/services on the cluster may become unmanageable from the dashboardConsider migrating workloads before unlinking clusters ","version":"Next","tagName":"h3"},{"title":"Resource Dependencies​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#resource-dependencies","content":"Satellite clusters serve as the foundation for several dependent resources: Git Servers: Satellite-specific git repositoriesService Accounts: Authentication credentials for cluster accessSecrets: Encrypted configuration values scoped to the clusterEnvironment Configs: Pod specifications and resource templatesJobs: Workloads running on the satellite infrastructureServices: Long-running applications deployed to the cluster ","version":"Next","tagName":"h3"},{"title":"Multi-Cluster Strategy​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#multi-cluster-strategy","content":"Use satellite clusters to distribute workloads geographicallyAssign billing projects to specific clusters for cost trackingImplement cluster-level resource quotas for capacity managementMonitor cluster health and performance independentlyPlan for cluster-level disaster recovery and failover ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#troubleshooting","content":"If cluster linking fails, verify domain accessibility from the main platformCheck network connectivity between main and satellite clustersEnsure satellite cluster has proper authentication configuredReview error messages for domain validation failuresContact platform administrators for cluster configuration assistance ","version":"Next","tagName":"h3"},{"title":"Security Considerations​","type":1,"pageTitle":"Satellite Clusters","url":"/developer-docs/features/satellite-clusters#security-considerations","content":"Only platform administrators can manage satellite cluster connectionsSatellite clusters inherit platform-wide security policiesUse dedicated service accounts for cluster-to-cluster communicationRegularly audit cluster access and usage patternsImplement network policies to restrict cross-cluster traffic as needed ","version":"Next","tagName":"h3"},{"title":"Scheduled Jobs","type":0,"sectionRef":"#","url":"/developer-docs/features/scheduled-jobs","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#overview","content":"The Scheduled Jobs panel provides comprehensive management capabilities for recurring pipeline jobs that execute on a defined schedule using cron expressions. This feature allows users to create, monitor, and manage automated data pipelines, batch processing tasks, and other recurring workloads that run at specified intervals across both the primary Kubernetes cluster and satellite clusters. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#access--location","content":"Route: ?panel=scheduled-jobsNavigation: Workloads → Scheduled JobsAccess Requirements: None (standard user access)Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Create Scheduled Jobs​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#create-scheduled-jobs","content":"Users can create new scheduled jobs with full control over execution timing, resource allocation, and pipeline configuration. Jobs can be configured to run shell scripts or multi-step YAML pipelines on a recurring schedule with support for timezone-specific execution, parallel runs, and automatic retry mechanisms. ","version":"Next","tagName":"h3"},{"title":"Monitor Active and Past Executions​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#monitor-active-and-past-executions","content":"The panel provides separate views for currently scheduled jobs (pending, in progress, or scheduled status) and historical job executions (completed, failed, or cancelled). Users can track job status, execution duration, and view detailed logs and metrics for troubleshooting. ","version":"Next","tagName":"h3"},{"title":"Clone and Modify Jobs​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#clone-and-modify-jobs","content":"Existing scheduled jobs can be cloned to create new jobs with similar configurations, streamlining the process of creating variations or testing different parameters. Jobs can also be cloned from their customized pod YAML specifications for advanced users. ","version":"Next","tagName":"h3"},{"title":"Suspend and Resume Jobs​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#suspend-and-resume-jobs","content":"Scheduled jobs can be suspended to temporarily stop them from running on their schedule without deleting the configuration. Suspended jobs can be resumed at any time to continue their scheduled execution. ","version":"Next","tagName":"h3"},{"title":"Trigger Manual Executions​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#trigger-manual-executions","content":"Users can manually trigger immediate execution of a scheduled job outside of its normal schedule, useful for testing or running jobs on-demand while maintaining the scheduled configuration. ","version":"Next","tagName":"h3"},{"title":"Satellite Cluster Support​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#satellite-cluster-support","content":"The feature supports managing scheduled jobs across both the primary cluster and remote satellite clusters, providing a unified interface for distributed workload management. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#main-view","content":"The panel uses a tabbed interface with two primary sections: Primary Cluster Tab: Manages scheduled jobs running on the main Kubernetes clusterSatellite Clusters Tab: Manages scheduled jobs on connected satellite clusters (requires satellite cluster selection) Each cluster view contains two sub-tabs: Scheduled: Shows active scheduled jobs (status: pending, in progress, or scheduled)History: Shows past job executions (status: done, failed, cancelled) ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#dialogs--modals","content":"Create/Clone Scheduled Job Dialog Purpose: Full-screen dialog for creating new scheduled jobs or cloning existing onesFields: General Tab: Name (alphanumeric with dashes, dots, underscores; max 63 chars)Environment Config (pod specification)Pipeline Type (Shell or Multi-step YAML)Pipeline Path (relative path to shell script or YAML file)Schedule (cron expression)TimezoneRun Parallel (allow concurrent executions) Advanced Tab: Use Default CommandActive Timeout (seconds, -1 to 86400)Max Retries (0-10)Working DirectoryGit Repository (server, branch, commit)Failure Alerts (enable/disable with alert targets)Cloud SQL Proxies (sidecar for GCP database connections)External Dependencies (secrets, service accounts, billing projects) Parameters Tab: Key-value pairs for job parameters Actions: Create Scheduled Job, View GraphQL Mutation, Customize Pod YAML Job Details Dialog Purpose: Displays comprehensive details about a specific scheduled jobIncludes: Job configuration, execution history, logs, and metricsActions: Clone, Trigger Now, Suspend/Resume, Delete Create from Pod YAML Dialog Purpose: Advanced creation dialog for customizing the Kubernetes pod specificationAllows direct editing of pod YAML before job creation ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#tables--data-grids","content":"Current Scheduled Jobs Table Columns: Actions (dropdown menu)Name (clickable link to details, with pin button)Status (visual indicator)Schedule (human-readable cron description)Environment ConfigCustom Image URL (hidden by default)ID (copy-to-clipboard)Pipeline Path (copy-to-clipboard)Created (timestamp)Output Notebooks Path (hidden by default)Billing Project ID (hidden by default)Service Account ID (hidden by default)Status Reason (hidden by default)Owner (hidden by default)Branch (hidden by default)Commit ID (hidden by default)Active Timeout (hidden by default)Max Tries (hidden by default) Actions: Clone, Clone from YAML, Toggle Publish, Trigger Now, Suspend/Resume, DeleteFiltering: Quick search (name, pipeline path, ID, author), advanced filters (status, type, group, billing project, parameters)Features: Pin jobs to top, pagination (20 per page), real-time status updates Past Scheduled Jobs Table Columns: Same as Current Scheduled Jobs TableActions: Clone, Clone from YAML (Delete not available for completed jobs)Filtering: Same as Current Scheduled Jobs Table with history-specific statuses ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#graphql-operations","content":"Queries: getScheduledJobs - Retrieves scheduled jobs from the primary cluster with filtering and pagination Parameters: offset, limit, whereClause (supports complex filtering)Returns: Job details including configuration, status, timing, and metadataIncludes: countJobs for pagination getScheduledSatelliteJobs - Retrieves scheduled jobs from satellite clusters Parameters: satelliteClusterName, offset, limit, whereClauseReturns: Same job details as primary cluster query GetPipelineJobPodSpecJson - Generates Kubernetes pod specification from job parameters Used during job creation to validate and preview the pod configuration Mutations: cancelScheduledJob - Deletes a scheduled job and cancels any running executions Parameters: id (job ID)Returns: Confirmation with deleted job ID suspendOrResumeScheduledJob - Toggles the suspended state of a scheduled job Parameters: id (job ID), suspend (boolean)Returns: Updated job with new suspended status createPipelineJobWithAlerting - Creates a new scheduled job with all configuration options Parameters: jobName, jobType, schedule, timezone, timeout, activeTimeout, maxRetries, pipelineYamlPath, workingDir, gitServerName, branchName, commitId, parameters, notificationsEnabled, notificationTargetIds, and many moreReturns: Created job details PinJob - Pins or unpins a job to keep it at the top of the table Parameters: id (job ID), pin (boolean)Returns: Updated pin state Subscriptions: None (uses polling with 5-second interval for satellite clusters) ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#component-structure","content":"Main Component: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/Jobs/Scheduled/ScheduledJobsPanel.tsxTables: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/Jobs/Scheduled/Tables/CurrentScheduledJobsTable.tsx/root/gitrepos/monorepo/apps/hyperplane-dashboard/components/Jobs/Scheduled/Tables/PastScheduledJobsTable.tsx Dialogs: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/Jobs/Scheduled/ScheduledJobDialog.tsx/root/gitrepos/monorepo/apps/hyperplane-dashboard/components/Jobs/Scheduled/satelliteCluster/ScheduledSatelliteJobDialog.tsx Actions Menu: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/Jobs/Scheduled/ScheduledJobsActionsMenu.tsx ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Creating a Basic Scheduled Job​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#creating-a-basic-scheduled-job","content":"Click &quot;Create Scheduled Job&quot; button in the top-rightEnter a unique name (alphanumeric with dashes, dots, underscores)Select an Environment Config (pod specification)Choose Pipeline Type (Shell or Multi-step YAML)Enter the relative path to your script or YAML fileConfigure the schedule using a cron expressionSelect the timezone for execution(Optional) Configure advanced settings: timeouts, retries, git repository(Optional) Add parameters as key-value pairsReview the Job Summary panel on the rightClick &quot;Create Scheduled Job&quot; ","version":"Next","tagName":"h3"},{"title":"Cloning an Existing Job​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#cloning-an-existing-job","content":"Locate the job you want to clone in either the Scheduled or History tableClick the &quot;Actions&quot; dropdown menu for that jobSelect &quot;Clone&quot; from the menuThe create dialog opens pre-populated with the job's configurationModify any settings (name, schedule, parameters, etc.)Click &quot;Create Scheduled Job&quot; to create the clone ","version":"Next","tagName":"h3"},{"title":"Suspending and Resuming a Job​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#suspending-and-resuming-a-job","content":"Find the scheduled job in the Scheduled tabClick the &quot;Actions&quot; dropdown menuSelect &quot;Suspend&quot; to pause scheduled executionsTo resume, click &quot;Actions&quot; → &quot;Resume&quot; on the suspended jobSuspended jobs remain in the Scheduled tab with a &quot;suspended&quot; status indicator ","version":"Next","tagName":"h3"},{"title":"Triggering a Manual Execution​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#triggering-a-manual-execution","content":"Navigate to the Scheduled tabFind the job you want to run immediatelyClick &quot;Actions&quot; → &quot;Trigger Now&quot;The job creates a new immediate execution while maintaining its scheduleView the execution in the Immediate Jobs panel ","version":"Next","tagName":"h3"},{"title":"Managing Jobs on Satellite Clusters​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#managing-jobs-on-satellite-clusters","content":"Click the Satellite Clusters tab (satellite icon)Select a satellite cluster from the dropdownView, create, clone, or manage jobs specific to that clusterJobs on satellite clusters have the same functionality as primary cluster jobsNote: Satellite cluster jobs cannot be suspended (only available on primary cluster) ","version":"Next","tagName":"h3"},{"title":"Filtering and Searching Jobs​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#filtering-and-searching-jobs","content":"Use the quick search bar to find jobs by name, pipeline path, ID, or authorApply advanced filters using the table toolbar: Status (pending, in progress, scheduled, done, failed, cancelled)Environment Config typeGroupBilling Project IDUser (filtered by email for non-admin users)Parameters (key-value pairs) View active filters as chips above the tablesClear individual filters by clicking the X on a chipClick &quot;Reset Filters&quot; to clear all filters ","version":"Next","tagName":"h3"},{"title":"Publishing Jobs for Team Access​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#publishing-jobs-for-team-access","content":"Locate the job in the Scheduled or History tabClick &quot;Actions&quot; → &quot;Publish&quot; on a private jobPublished jobs display a &quot;published&quot; chip next to the namePublished jobs are visible to all users in the workspaceTo unpublish, click &quot;Actions&quot; → &quot;Unpublish&quot; ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#related-features","content":"Jobs - One-time job executionsServices - Long-running microservices with exposed portsEnvironment Configs - Pod specifications and resource allocationsGit Repositories - Source control integration for pipeline codeBilling Projects - Cost tracking and allocation for job executionsAlert Targets - Notification channels for job failures ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Scheduling Best Practices​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#scheduling-best-practices","content":"Use crontab.guru to validate and understand cron expressionsConsider timezone settings carefully, especially for jobs that need to run at specific business hoursThe &quot;Run Parallel&quot; option should be disabled for jobs that cannot run concurrently (e.g., jobs that write to the same database table)Use the human-readable schedule description in the table to verify your cron expression ","version":"Next","tagName":"h3"},{"title":"Resource Management​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#resource-management","content":"Active Timeout (-1 for unlimited, or 1-86400 seconds) determines how long a job can run before being terminatedMax Retries (0-10) controls automatic retry attempts for failed job startsDefault values: 86400s timeout, 86400s active timeout, 2 max retries ","version":"Next","tagName":"h3"},{"title":"Git Integration​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#git-integration","content":"Default Environment Configs require git repository integration (cannot be disabled)Custom Environment Configs can optionally use git repositories or rely on files baked into the imageSpecifying a commit ID requires also specifying the branch nameWorking directory defaults to /tmp/git/monorepo/ after git sync ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#parameters","content":"Parameters are passed to the pipeline as environment variablesUse parameters for dynamic configuration without modifying codeParameter keys and values must both be non-empty stringsFilter jobs by parameter values in the advanced filters ","version":"Next","tagName":"h3"},{"title":"Pinning Jobs​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#pinning-jobs","content":"Pin frequently accessed jobs to keep them at the top of the tablePinned jobs appear first, followed by other jobs sorted by start time (descending)Pin status is preserved across page refreshes ","version":"Next","tagName":"h3"},{"title":"Performance Considerations​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#performance-considerations","content":"Tables are paginated at 20 jobs per page for optimal performanceSatellite cluster jobs poll every 5 seconds for status updatesUse filters to narrow down large job listsHidden columns can be shown via the column menu (three-dot icon in column headers) ","version":"Next","tagName":"h3"},{"title":"URL State Management​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#url-state-management","content":"Filters are persisted in URL parameters (currentFilters, pastFilters)Job details can be accessed directly via URL with ?panel=scheduled-jobs&amp;jobId={id}Satellite cluster job details require both jobId and satelliteClusterName parameters ","version":"Next","tagName":"h3"},{"title":"Limitations​","type":1,"pageTitle":"Scheduled Jobs","url":"/developer-docs/features/scheduled-jobs#limitations","content":"Maximum job name length: 63 charactersJob names must start and end with alphanumeric charactersJob names cannot contain spacesMaximum active timeout: 86400 seconds (24 hours)Maximum retries: 10Only jobs on the primary cluster can be suspended/resumedPublishing/unpublishing is only available for primary cluster jobs ","version":"Next","tagName":"h3"},{"title":"Access Logs (SecOps)","type":0,"sectionRef":"#","url":"/developer-docs/features/secops","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#overview","content":"The Access Logs panel provides security operations (SecOps) capabilities for tracking and analyzing user access patterns across the Shakudo platform. This feature integrates with OAuth2 Proxy logs to monitor which users have accessed specific domains, offering visibility into authentication and access activities for security auditing and compliance purposes. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#access--location","content":"Route: ?panel=secopsNavigation: Admin → Access LogsAccess Requirements: Dashboard admin role (rbacInfo[0] must be true)User must be part of the dashboard-admin role Feature Flags: secOpsEnabled - Must be set via environment variable HYPERPLANE_DASHBOARD_SECOPS_ENABLED=true ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"User Access Tracking​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#user-access-tracking","content":"Monitor OAuth2 Proxy logs to track user authentication and access patterns across domains. This provides a centralized view of who is accessing which services and when. ","version":"Next","tagName":"h3"},{"title":"Grafana Dashboard Integration​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#grafana-dashboard-integration","content":"Direct integration with a dedicated Grafana dashboard (secops-oauth2-proxy) that provides advanced filtering, visualization, and analysis capabilities for access logs. ","version":"Next","tagName":"h3"},{"title":"Security Auditing​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#security-auditing","content":"Enable security teams to audit user access patterns, identify unauthorized access attempts, and maintain compliance with security policies. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#main-view","content":"The panel displays an embedded Grafana dashboard in kiosk mode (fullscreen, no Grafana navigation chrome) that shows OAuth2 Proxy logs. The view includes: Title Section: &quot;User Access Logs&quot; with descriptive text explaining the panel's purposeEmbedded Dashboard: Full-height iframe displaying the Grafana SecOps dashboardExternal Link Button: Icon button (open in new tab) to access the full Grafana dashboard with filtering capabilities The interface is designed to provide immediate visibility into access logs while offering a path to more advanced analysis in Grafana. ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#dialogs--modals","content":"No dialogs or modals are implemented in this panel. All interactions occur through the embedded Grafana dashboard or by opening the full Grafana interface in a new tab. ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#tables--data-grids","content":"This panel does not implement its own tables. All data visualization and tabular display is handled by the embedded Grafana dashboard, which provides: OAuth2 Proxy log entriesUser access eventsDomain access patternsTimestamp and authentication details For filtering and detailed analysis, users must click the &quot;Open in new tab&quot; button to access the full Grafana dashboard. ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#graphql-operations","content":"This panel does not use any GraphQL operations. It relies entirely on Grafana for data retrieval and visualization. Queries: None Mutations: None Subscriptions: None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#component-structure","content":"Main Component: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/SecOps/SecOpsPanel.tsxDialogs: NoneTables: None (data visualization handled by Grafana) ","version":"Next","tagName":"h3"},{"title":"External Dependencies​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#external-dependencies","content":"Grafana: Dashboard hosted at https://grafana.{domain}/d/ddv992wudcvlsd/secops-oauth2-proxyOAuth2 Proxy: Source of authentication and access logsDomain Context: Uses platform domain from PlatformParametersContext to construct Grafana URLs ","version":"Next","tagName":"h3"},{"title":"Implementation Details​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#implementation-details","content":"The component uses: dayjs for time handling (with relativeTime, timezone, and UTC plugins loaded but not actively used in current implementation)Material-UI components for layout and interaction (Stack, Typography, IconButton, Tooltip)React Icons (MdOpenInNew) for the external link iconContext API to access platform domain configuration ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Viewing Access Logs​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#viewing-access-logs","content":"Navigate to Admin → Access Logs from the main navigationThe embedded Grafana dashboard loads automatically showing recent OAuth2 Proxy logsReview access patterns in the default view ","version":"Next","tagName":"h3"},{"title":"Advanced Filtering and Analysis​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#advanced-filtering-and-analysis","content":"From the Access Logs panel, click the &quot;Open in new tab&quot; button (icon with tooltip)The full Grafana dashboard opens in a new browser tabUse Grafana's native filtering, time range selection, and query capabilitiesExport or share specific views as needed ","version":"Next","tagName":"h3"},{"title":"Security Audit Investigation​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#security-audit-investigation","content":"Access the SecOps panel to get an overview of recent access activityIdentify any suspicious patterns or unauthorized access attemptsOpen the full Grafana dashboard for detailed investigationUse time range filters to narrow down specific incidentsCross-reference with user management and authentication systems ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#related-features","content":"Users Panel - Manage user accounts and rolesKeycloak Integration - Authentication and authorization system that generates the OAuth2 Proxy logsAdmin Panels - Other administrative features requiring dashboard-admin role ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Access Logs (SecOps)","url":"/developer-docs/features/secops#notes--tips","content":"The embedded Grafana dashboard uses kiosk mode (?kiosk parameter) to provide a cleaner, full-screen view without Grafana's standard navigationFor complex filtering operations, always use the full Grafana dashboard (click the open in new tab button)Access to this panel requires both the dashboard-admin role and the secOpsEnabled feature flagThe Grafana dashboard URL is dynamically constructed based on the platform's domain configurationThe panel is lazy-loaded for performance optimizationTime handling libraries (dayjs with plugins) are imported but not currently used in the component, suggesting potential future enhancements for time-based filtering or display ","version":"Next","tagName":"h2"},{"title":"Secrets Management","type":0,"sectionRef":"#","url":"/developer-docs/features/secrets","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#overview","content":"The Secrets panel provides secure management of sensitive configuration data (credentials, API keys, tokens, etc.) that can be accessed by workloads, services, and development sessions in the Shakudo platform. Secrets are stored as Kubernetes secrets and can be scoped to specific namespaces to control where they are accessible. The feature supports both primary cluster secrets and satellite cluster secrets, with comprehensive CRUD operations and role-based access control. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#access--location","content":"Route: ?panel=secretsNavigation: Main Dashboard → Secrets PanelAccess Requirements: View: Available to all authenticated users (can see own secrets by default)Create/Edit/Delete: Role-based permissions (SecretRoles) or secret ownershipAdmin users can view and manage all secrets Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Create Secrets​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#create-secrets","content":"Create new secrets with key-value pairs that can be accessed by jobs, services, and sessions. Secrets can be scoped to specific purposes: Workloads: Available in jobs and microservices (hyperplane-pipelines namespace)Development: Available in development sessions (hyperplane-jhub namespace)Development &amp; Workloads: Available in both environments ","version":"Next","tagName":"h3"},{"title":"View Secret Details​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#view-secret-details","content":"View comprehensive information about secrets including: Secret metadata (name, ID, description, creator, timestamps)Key-value pairs stored in the secretEnvironment variable names generated for each keyCreation and update history ","version":"Next","tagName":"h3"},{"title":"Edit Secrets​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#edit-secrets","content":"Update existing secrets by: Modifying the descriptionAdding new key-value pairsUpdating existing key valuesRemoving keys from the secret ","version":"Next","tagName":"h3"},{"title":"Delete Secrets​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#delete-secrets","content":"Permanently remove secrets from both the Kubernetes cluster and database. This action cannot be undone and will affect any workloads currently using the secret. ","version":"Next","tagName":"h3"},{"title":"Deactivate Secrets​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#deactivate-secrets","content":"Temporarily deactivate secrets without permanently deleting them. Deactivated secrets remain in the database but are not accessible to workloads. ","version":"Next","tagName":"h3"},{"title":"Satellite Cluster Support​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#satellite-cluster-support","content":"Manage secrets on satellite clusters (remote Kubernetes clusters) with the same capabilities available for the primary cluster. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#main-view","content":"The panel features a tabbed interface with two main sections: Primary Cluster Tab: Data grid displaying all accessible secrets on the primary clusterColumns: Actions, Name, ID, Purpose, Created By, Created On, DescriptionSearch/filter functionality across all fieldsRefresh button to reload dataColumn visibility controls Satellite Clusters Tab: Dropdown to select a satellite clusterData grid showing secrets for the selected satellite clusterSimilar columns and functionality as primary cluster view ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#dialogs--modals","content":"Create Secret Dialog (Full Screen) Purpose: Create new secrets with multiple key-value pairsFields: Name (required, validated for uniqueness within namespace)Description (optional)Purpose (required, dropdown: Workloads/Development/Both)Data (key-value pairs, at least one required) Features: Tabbed interface (General tab and Data tab)Real-time validationSummary panel showing secret configurationCreates secrets in multiple namespaces when &quot;Development &amp; Workloads&quot; is selected Secret Details Dialog Purpose: View secret metadata and key-value pairsShows: Secret name and IDDescriptionCreator email and creation timestampLast updated informationPurpose/namespace labelTable of keys with corresponding environment variable names Actions: Copy environment variable names to clipboardClose dialog Edit Secret Dialog (Half Screen) Purpose: Modify existing secret properties and dataFields: Description (editable text area)Data section with table of existing keys Actions: Edit individual key values (opens nested dialog)Add new keys (opens nested dialog)Delete keys (with confirmation)Save changes (updates description and/or data) Features: Shows creation and last update metadataValidates key names for Kubernetes compatibilityPrevents duplicate key names Delete Secret Dialog Purpose: Permanently remove a secretConfirmation prompt with warning messageActions: Delete or CancelNote: Only available for activated secrets Deactivate Secret Dialog Purpose: Temporarily disable a secretConfirmation promptActions: Deactivate or CancelNote: Only available for activated secrets Edit Key Dialog (Nested) Purpose: Update the value of a specific keyFields: Key name (display only)Secret content (multiline text input) Actions: Save or Cancel Add Key Dialog (Nested) Purpose: Add a new key-value pair to an existing secretFields: Key (text input, validated)Content (multiline text input) Actions: Save or CancelValidation: Ensures key is valid Kubernetes secret key and doesn't already exist ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#tables--data-grids","content":"Primary Cluster Secrets Table Columns: Actions (Edit and Delete icons)Name (clickable, opens details dialog)ID (copyable, shows first 6 characters)Purpose (copyable, shows label: Workloads/Development)Created By (user email)Created On (formatted timestamp)Description (truncated with ellipsis) Actions: Click name to view detailsClick edit icon to modify secretClick delete icon to remove secret Filtering: Search across all columns (ID, name, namespace, description, user)Real-time search with 300ms debounce Features: Row styling based on activation statusSortable columnsColumn visibility togglePagination (20 items per page) Satellite Cluster Secrets Table Similar structure to primary cluster tableAdditional deactivation action (no delete)Requires satellite cluster selection via dropdownShows &quot;deactivated&quot; chip for inactive secrets ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#graphql-operations","content":"Queries: getHyperplaneSecrets - Fetches list of secrets with filtering, pagination, and orderinggetSecretContent - Retrieves key-value pairs for a specific secret (by name and namespace)GetHyperplaneSecretDocument - Gets detailed metadata for a single secret by IDgetSatelliteSecrets - Fetches secrets from a satellite cluster Mutations: createHyperplaneSecret - Creates a new secret with name, namespace, description, and dataupdateHyperplaneSecret - Updates secret description and/or modifies keys (add/update/remove)deleteHyperplaneSecret - Permanently removes a secret from cluster and databasedeactivateHyperplaneSecret - Marks a secret as inactive without deleting itcreateSatelliteSecret - Creates a secret on a satellite clusterdeactivateSatelliteSecret - Deactivates a secret on a satellite cluster Subscriptions: None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#component-structure","content":"Main Component: /components/HyperplaneSecret/HyperplaneSecretPanel.tsxTable Component: /components/HyperplaneSecret/HyperplaneSecretTables.tsxDialogs: /components/HyperplaneSecret/Dialogs/HyperplaneSecretCreateDialog.tsx/components/HyperplaneSecret/Dialogs/HyperplaneSecretDetailsDialog.tsx/components/HyperplaneSecret/Dialogs/HyperplaneSecretEditDialog.tsx/components/HyperplaneSecret/Dialogs/HyperplaneSecretDeleteDialog.tsx/components/HyperplaneSecret/Dialogs/HyperplaneSecretDeactivateDialog.tsx Satellite Components: /components/HyperplaneSecret/Satellite/ ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#state-management","content":"Uses Jotai atoms for panel-level state: SecretsPanelSectionAtom - Controls which view is displayed (table/create/createSatellite)SecretsTabAtom - Tracks active tab (primary cluster vs satellite)selectedSatelliteClusterAtom - Stores selected satellite cluster nameselectedSatelliteClusterIdAtom - Stores selected satellite cluster ID ","version":"Next","tagName":"h3"},{"title":"Data Transformation​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#data-transformation","content":"Secret values are Base64 encoded before storage and decoded for displayKey names are transformed to environment variable format: HYPERPLANE_CUSTOM_SECRET_KEY_&lt;KEY_NAME&gt;Non-alphanumeric characters in keys are replaced with underscores and converted to uppercaseKubernetes secret key validation ensures keys match pattern: ^[a-zA-Z0-9][-._a-zA-Z0-9]*[a-zA-Z0-9]$ ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Creating a Secret for Workloads​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#creating-a-secret-for-workloads","content":"Click &quot;Create Secret&quot; button in the Primary Cluster tabEnter a unique name for the secretAdd a description (optional but recommended)Select &quot;Workloads&quot; as the PurposeNavigate to the &quot;Data&quot; tabAdd one or more key-value pairs (e.g., API_KEY: abc123)Review the summary panel on the rightClick &quot;Create Secret&quot;Secret is created and available to jobs and services ","version":"Next","tagName":"h3"},{"title":"Viewing Secret Details and Environment Variables​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#viewing-secret-details-and-environment-variables","content":"Locate the secret in the tableClick on the secret nameView the secret metadata and key-value pairsCopy environment variable names using the copy buttonUse these variable names in your code (e.g., os.environ['HYPERPLANE_CUSTOM_SECRET_KEY_API_KEY']) ","version":"Next","tagName":"h3"},{"title":"Updating a Secret​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#updating-a-secret","content":"Find the secret in the tableClick the edit icon (pencil)Modify the description if neededTo update a key value: Click the pencil icon next to the key, enter new value, saveTo add a key: Click &quot;Add Data&quot; button, enter key and value, saveTo remove a key: Click the trash icon next to the key, confirm deletionClick &quot;Save&quot; to apply all changes ","version":"Next","tagName":"h3"},{"title":"Deleting a Secret​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#deleting-a-secret","content":"Locate the secret in the tableClick the delete icon (trash can)Read the warning message carefullyClick &quot;Delete&quot; to confirm permanent removalSecret is removed from both Kubernetes and the database ","version":"Next","tagName":"h3"},{"title":"Managing Satellite Cluster Secrets​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#managing-satellite-cluster-secrets","content":"Switch to the &quot;Satellite Clusters&quot; tabSelect a satellite cluster from the dropdownView secrets specific to that clusterCreate, view, edit, or deactivate secrets as neededNote: Satellite secrets can only be deactivated, not permanently deleted ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#related-features","content":"Jobs - Pipeline jobs can access secrets via environment variablesServices - Microservices can access secrets for API credentialsSessions - Development sessions can access secrets in the Development namespaceSatellite Clusters - Remote cluster management with dedicated secret storage ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Security Best Practices​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#security-best-practices","content":"Never hardcode sensitive values in your code - always use secretsUse descriptive names for secrets to make them easy to identifyAdd descriptions to document the purpose and usage of each secretRegularly review and remove unused secretsOnly grant secret access to users who need it ","version":"Next","tagName":"h3"},{"title":"Environment Variable Access​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#environment-variable-access","content":"In Python: os.environ['HYPERPLANE_CUSTOM_SECRET_KEY_&lt;YOUR_KEY&gt;']In Shell scripts: $HYPERPLANE_CUSTOM_SECRET_KEY_&lt;YOUR_KEY&gt;Key names are automatically transformed: lowercase → uppercase, special chars → underscoresExample: api-key-v2 becomes HYPERPLANE_CUSTOM_SECRET_KEY_API_KEY_V2 ","version":"Next","tagName":"h3"},{"title":"Namespace Scoping​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#namespace-scoping","content":"Workloads secrets are available to scheduled jobs, immediate jobs, and microservicesDevelopment secrets are available in JupyterHub sessions and development environmentsDevelopment &amp; Workloads creates separate secrets in both namespaces for maximum availabilitySecrets with the same name can exist in different namespaces ","version":"Next","tagName":"h3"},{"title":"Permissions and Ownership​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#permissions-and-ownership","content":"Users can always view, edit, and delete their own secretsAdmin roles can manage all secrets across the organizationRole-based access control determines who can create/edit/delete secretsSecret ownership is tracked by creator email and last updater email ","version":"Next","tagName":"h3"},{"title":"Key Validation​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#key-validation","content":"Keys must start and end with alphanumeric charactersKeys can contain hyphens, periods, and underscores in the middleMaximum key length is 253 charactersDuplicate keys within the same secret are not allowed ","version":"Next","tagName":"h3"},{"title":"Satellite Clusters​","type":1,"pageTitle":"Secrets Management","url":"/developer-docs/features/secrets#satellite-clusters","content":"Satellite secrets are managed independently from primary cluster secretsMust select a satellite cluster before viewing or creating secretsDeactivation (rather than deletion) is the recommended approach for satellite secretsSecrets are not automatically synchronized between primary and satellite clusters ","version":"Next","tagName":"h3"},{"title":"Service Accounts","type":0,"sectionRef":"#","url":"/developer-docs/features/service-accounts","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#overview","content":"The Service Accounts feature provides identity management for processes running in Kubernetes pods, enabling secure and manageable role-based access control (RBAC) within the Shakudo platform. This feature allows users to create service accounts for both the primary cluster and satellite clusters, ensuring proper isolation and access control for workloads and development environments. Service accounts in Kubernetes provide an identity for processes that run in pods, allowing them to interact with the Kubernetes API and other cluster resources. In Shakudo, service accounts can be configured with default roles or custom permissions depending on the namespace and requirements. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#access--location","content":"Route: ?panel=service-accountsNavigation: Shakudo Objects → Service AccountsAccess Requirements: View service accounts: All authenticated users can view their own service accountsView all service accounts: Requires DASHBOARD_ADMIN_ROLE or DASHBOARD_MAINTAINER_ROLEDeactivate service accounts: Requires DASHBOARD_ADMIN_ROLE or the service account must be owned by the user Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Create Primary Cluster Service Account​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#create-primary-cluster-service-account","content":"Create a service account for the primary Shakudo cluster with customizable name, namespace, and permissions. Service accounts can be configured with either default roles (in managed namespaces) or custom role definitions for specific access control needs. ","version":"Next","tagName":"h3"},{"title":"Create Satellite Cluster Service Account​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#create-satellite-cluster-service-account","content":"Create a service account for a satellite cluster, extending the platform's capabilities to remote or distributed environments. Requires selecting a target satellite cluster in addition to the standard service account configuration. ","version":"Next","tagName":"h3"},{"title":"Deactivate Service Account​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#deactivate-service-account","content":"Deactivate an existing service account when it's no longer needed. This is a soft-delete operation that marks the account as inactive while preserving the record for audit purposes. Only administrators or the service account owner can perform this action. ","version":"Next","tagName":"h3"},{"title":"Search and Filter​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#search-and-filter","content":"Search across service accounts by ID, name, namespace, or user email. The search functionality helps quickly locate specific service accounts in large deployments with many accounts. ","version":"Next","tagName":"h3"},{"title":"Copy Service Account Details​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#copy-service-account-details","content":"Copy important service account information (ID, name, purpose) directly to clipboard for use in configurations, scripts, or documentation. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#main-view","content":"The Service Accounts panel features a tabbed interface with two sections: Primary Cluster Tab: Displays service accounts for the main Shakudo clusterSatellite Clusters Tab: Displays service accounts for satellite clusters, with a cluster selector dropdown Each tab shows a data grid with the following information: Service account ID (shortened with copy-to-clipboard functionality)Name (with deactivated status indicator if applicable)Purpose (Workloads or Development)Created By (user email)Created On (timestamp)DescriptionQuick deactivate action button The panel header includes: Panel title with description explaining service account purposeCreate button to open the creation dialogSearch/filter input with column visibility controls and refresh button ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#dialogs--modals","content":"Create Service Account Dialog Purpose: Create a new service account in the primary clusterFields: Use Default Role (checkbox): Toggle to use predefined Shakudo managed permissionsName (required): Unique identifier for the service account (alphanumeric with dashes, underscores, dots; max 128 characters)Description (optional): Descriptive text explaining the service account's purposePurpose (required): Dropdown to select namespace - Workloads (hyperplane-pipelines) or Development (hyperplane-jhub) Actions: Cancel or CreateValidation: Name must be unique within the namespace and follow naming conventions Create Satellite Service Account Dialog Purpose: Create a service account for a satellite clusterFields: Use Default Role (checkbox): Toggle for default permissionsName (required): Service account nameSatellite Cluster (required): Dropdown to select target satellite clusterDescription (optional): Purpose descriptionPurpose (required): Namespace selection (Workloads or Development) Actions: Cancel or CreateValidation: Same as primary cluster, plus requires valid satellite cluster selection Deactivate Service Account Confirmation Purpose: Confirm deactivation of a service accountPrompt: &quot;Are you sure you want to deactivate service account '{name}'?&quot;Actions: Cancel or Deactivate ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#tables--data-grids","content":"Primary Cluster Service Accounts Table Columns: Empty (40px): Deactivate action buttonID (medium width): Service account identifier with copy functionalityName (extra-large width): Account name with deactivated chip if inactivePurpose (large width): Namespace display (Workloads/Development)Created By (medium width): User email who created the accountCreated On (date width): Creation timestampDescription (flex): Full description text Actions: Row-level deactivate button (only for active accounts)Copy to clipboard for ID, name, and purpose Filtering: Real-time search across all visible columnsSorting: Activated accounts shown first, then alphabetically by name Satellite Cluster Service Accounts Table Columns: Identical to primary cluster tableAdditional: Requires satellite cluster selection before displaying dataActions: Same as primary cluster with satellite-specific deactivation logic ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#graphql-operations","content":"Queries: getHyperplaneServiceAccounts - Retrieves service accounts from the primary cluster with filtering, ordering, pagination support. Returns account details including activation status, role configuration, and metadatagetSatelliteServiceAccounts - Retrieves service accounts from a specified satellite cluster. Requires satellite cluster name parameter Mutations: createHyperplaneServiceAccount - Creates a new service account in the primary cluster with name, namespace, description, default role flag, and user emailcreateSatelliteServiceAccount - Creates a new service account in a satellite cluster, includes satellite cluster name parameterdeleteHyperplaneServiceAccount - Deactivates (soft deletes) a service account in the primary cluster by name and namespacedeleteSatelliteServiceAccount - Deactivates a service account in a satellite cluster ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#component-structure","content":"Main Component: /components/HyperplaneServiceAccount/HyperplaneServiceAccountPanel.tsxDialogs: /components/HyperplaneServiceAccount/HyperplaneServiceAccountCreate.tsx/components/HyperplaneServiceAccount/HyperplaneServiceAccountDeactivate.tsx/components/HyperplaneServiceAccount/satellite/SatelliteServiceAccountCreate.tsx/components/HyperplaneServiceAccount/satellite/SatelliteServiceAccountDeactivate.tsx Hooks: /hooks/useHyperplaneServiceAccounts.ts - Primary cluster operations/hooks/useSatelliteServiceAccounts.ts - Satellite cluster operations Search Results: /components/Search/ServiceAccountsSearchResults.tsx ","version":"Next","tagName":"h3"},{"title":"Key Implementation Details​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#key-implementation-details","content":"Naming Validation: Service account names must start and end with alphanumeric characters, can contain dashes, underscores, and dots, with a maximum length of 128 charactersNamespace Mapping: Purpose field maps to Kubernetes namespaces: &quot;Workloads&quot; → hyperplane-pipelines (for jobs and microservices)&quot;Development&quot; → hyperplane-jhub (for development sessions) Role Configuration: Default roles are only available for Shakudo managed namespaces (Workloads and Development). Custom namespaces require manual role definitionSoft Delete: Deactivation sets activated flag to false and records deactivationDate rather than permanently deleting the recordAccess Control: Non-admin users can only see and manage their own service accounts unless they have MAINTAINER or ADMIN roles ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Creating a Service Account for Workloads​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#creating-a-service-account-for-workloads","content":"Navigate to Service Accounts panel (Shakudo Objects → Service Accounts)Ensure &quot;Primary Cluster&quot; tab is selectedClick &quot;Create Service Account&quot; buttonCheck &quot;Use Default Role&quot; (recommended for managed namespaces)Enter a descriptive name (e.g., &quot;data-pipeline-prod&quot;)Select &quot;Workloads&quot; from Purpose dropdownAdd optional description (e.g., &quot;Service account for production data pipelines&quot;)Click &quot;Create&quot;Copy the service account name from the table for use in pipeline configurations ","version":"Next","tagName":"h3"},{"title":"Creating a Service Account for Satellite Cluster​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#creating-a-service-account-for-satellite-cluster","content":"Navigate to Service Accounts panelSelect &quot;Satellite Clusters&quot; tabSelect target satellite cluster from dropdownClick &quot;Create Satellite Service Account&quot; buttonConfigure service account (name, purpose, description)Verify satellite cluster selection is correctClick &quot;Create&quot;Note the service account details for remote cluster configuration ","version":"Next","tagName":"h3"},{"title":"Deactivating an Unused Service Account​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#deactivating-an-unused-service-account","content":"Locate the service account in the appropriate tab (Primary or Satellite)Use search if needed to find the specific accountClick the deactivate icon (red X) in the leftmost columnReview the service account name in the confirmation dialogClick &quot;Deactivate&quot; to confirmThe account will be marked as deactivated and appear with a &quot;deactivated&quot; chip ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#related-features","content":"Secrets - Service accounts often need associated secrets for authenticationSessions - Development sessions may use service accounts for cluster accessJobs - Workload jobs can be configured to run with specific service accountsMicroservices - Microservices use service accounts for API access and resource permissions ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Service Accounts","url":"/developer-docs/features/service-accounts#notes--tips","content":"Default Roles: Always use default roles for managed namespaces (Workloads and Development) unless you have specific RBAC requirements that aren't met by the defaultsNaming Convention: Use descriptive names that indicate the service account's purpose and environment (e.g., &quot;ml-training-dev&quot;, &quot;api-service-prod&quot;)Security: Regularly audit service accounts and deactivate those that are no longer in use to maintain security hygieneSatellite Clusters: Satellite cluster service accounts are managed independently and require the satellite cluster to be accessible and properly configuredSearch Performance: For large deployments with many service accounts, use specific search terms rather than browsing to improve load timesRole Permissions: If you uncheck &quot;Use Default Role&quot;, you'll need to manually configure RBAC permissions using Kubernetes role and rolebinding resourcesDeactivation vs Deletion: Service accounts are deactivated rather than deleted to maintain audit trails. Deactivated accounts can be identified by the gray &quot;deactivated&quot; chipMulti-user Environments: Administrators can see all service accounts across users, while regular users only see their own accounts by default ","version":"Next","tagName":"h2"},{"title":"Microservices (Services Panel)","type":0,"sectionRef":"#","url":"/developer-docs/features/services","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#overview","content":"The Services panel provides comprehensive management capabilities for deploying and maintaining long-running microservices on the Shakudo platform. This feature enables users to deploy containerized applications with custom endpoints, configure autoscaling, set up health monitoring, and manage the complete lifecycle of production-ready services. Microservices differ from regular pipeline jobs by running continuously with exposed network ports, horizontal pod autoscaling, and dedicated endpoints for external access. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#access--location","content":"Route: ?panel=servicesNavigation: Workloads → MicroservicesAccess Requirements: None (basic user access)Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Create Microservices​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#create-microservices","content":"Deploy new long-running services with customizable configurations including environment settings, Git integration, networking options, and resource scaling. Services can be created from scratch, cloned from existing services, or deployed from custom pod specifications. ","version":"Next","tagName":"h3"},{"title":"Manage Service Lifecycle​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#manage-service-lifecycle","content":"Control running microservices with operations including: Start/stop services (scaling to 0 replicas)Restart services to apply configuration changesScale services manually or configure horizontal pod autoscalingMonitor service health and replica statusView real-time logs and Kubernetes events ","version":"Next","tagName":"h3"},{"title":"Configure Service Networking​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#configure-service-networking","content":"Set up external access through: Custom subdomains for public access (e.g., myservice.dev.hyperplane.dev)Subpath routing under the main domainIn-cluster service URLs for internal Kubernetes communicationPublic webhooks with special subdomain suffixes (-webhook, -public)URL rewriting and request routing options ","version":"Next","tagName":"h3"},{"title":"Health Monitoring & Alerting​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#health-monitoring--alerting","content":"Configure proactive monitoring with: Automatic health checks on exposed portsConfigurable failure thresholds before alertingIntegration with alert notification targetsCooldown periods between notificationsGrafana integration for detailed metrics ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#main-view","content":"The Services panel presents two tabbed views for managing microservices: Available Tab - Displays currently running and pending microservices with: Service status (healthy, not ready, off, failed)Replica counts (desired vs ready replicas)Scaling configuration (fixed replicas or HPA range)Public endpoint URLs with copy-to-clipboardIn-cluster service URLs for Kubernetes accessQuick access to logs, events, and service details History Tab - Shows completed, failed, or stopped microservices with: Historical service recordsCompletion times and durationsStatus reasons for failuresSame filtering and search capabilities as Available tab Both views support: Real-time filtering by status, name, owner, type, timeframe, and custom parametersQuick search across name, endpoint, pipeline path, ID, and authorPagination for large service listsColumn customization and sortingPin favorite services for quick access ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#dialogs--modals","content":"Create/Edit/Clone Microservice Dialog Purpose: Full-featured form for creating new services or modifying existing onesSections: General Tab: Name, endpoint configuration, environment config, pipeline type (Shell/Multi-step), Git settingsAdvanced Tab: Worker config, autoscaling, failure alerts, Cloud SQL proxies, secrets, service accounts, billing projectsParameters Tab: Key-value parameters passed to the serviceReadme Tab: Markdown documentation for the service Features: Subdomain/subpath configuration with availability checkingEnvironment config selection with resource specificationsGit repository integration (branch/commit selection)Horizontal pod autoscaling (min/max replicas)Health check configuration with alert targetsCustom pod YAML editing for advanced usersGraphQL mutation previewJob summary panel with configuration overview Service Details Dialog Purpose: Comprehensive view of a running or completed serviceInformation: Full job metadata, configuration, logs, events, metricsActions: Edit, clone, restart, scale, cancel operations Events and Logs Dialog Purpose: View Kubernetes pod events and container logsFeatures: Real-time log streaming, event history, container selection Scale Up/Down Dialog Purpose: Adjust service replica countsOptions: Start service (scale up), stop service (scale to 0), custom replica count Service Status Dialog Purpose: Show detailed service condition informationDetails: Pod conditions, readiness status, health check results Customize Pod YAML Dialog Purpose: Advanced users can directly edit the Kubernetes pod specificationFeatures: YAML editor with validation, diff view for edited services, generate from form values ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#tables--data-grids","content":"Current Services Table (Available Tab) Purpose: Display active and pending microservicesColumns: Actions: Dropdown menu with clone, edit, restart, scale, cancel optionsName: Service name with pin button, start button (if off), and link to detailsStatus: Visual status indicator (healthy, not ready, off, failed with reason)Scaling: Replica information (current/min/max with autoscaling indicator)Logs: Quick access to events/logs dialog and Grafana dashboardEndpoint: Public URL with copy button and iframe previewIn-cluster URL: Internal Kubernetes service URLReady Replicas: Number of healthy podsPort: Exposed service portID: Unique service identifierStart Time: When service was startedHidden columns: Desired replicas, max replicas, env config, custom image URL, pipeline path, billing project, service account, Git branch/commit, owner, status reason Actions: Click name to view full detailsPin/unpin services for favoritesStart services that are scaled to 0Clone existing services with same configurationEdit service configuration (creates new version)Restart services to apply changesScale services up/downCancel/delete services Filtering: Quick search across multiple fieldsStatus filter (with &quot;off&quot; option for stopped services)Owner/user filterType/environment config filterCustom parameter filtersTimeframe filters Past Services Table (History Tab) Purpose: Display historical microservice recordsColumns: Similar to Current Services, with additional completion time and durationFiltering: Same filtering capabilities as Current ServicesActions: View details, clone (recreate with same config) ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#graphql-operations","content":"Queries: GetServices - Retrieves paginated list of microservices with filtering and sorting Filters by status (in progress/pending for current, completed/failed for past)Includes replica counts, scaling configuration, endpointsReturns port, timeout=-1, activeTimeout=-1 (identifies as service vs job)Supports filtering by user, group, billing project, parameters, Git details GetUserServicePodSpec - Generates Kubernetes pod specification from form inputsIsUserServiceUrlAvailable - Validates subdomain/subpath availability before creationIsPipelineJobYamlEdited - Checks if service YAML has been manually customizedCountJobs - Returns total count for pagination Mutations: CreateService (createPipelineJobWithAlerting) - Creates new microservice Sets timeout=-1 and activeTimeout=-1 (distinguishes from regular jobs)Sets exposedPort to enable network accessConfigures minReplicas/maxHpaRange for autoscalingSupports notification targets for health alertsAccepts custom pod YAML or generates from form EditService (editMicroserviceWithAlerting) - Updates existing microservice Can modify all service parametersReturns whether service was automatically restartedChanges to core config (image, YAML path) trigger restart RestartService - Restarts service pods to apply configuration changesCancelService - Stops and deletes the microserviceUpdateServiceReplicas (ScaleService) - Adjusts min/max replica counts for scalingPinJob - Pins/unpins service for quick access Subscriptions:None (uses polling via refetch for real-time updates) ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#component-structure","content":"Main Component: /components/Jobs/Services/ServicesPanel.tsxService Dialog: /components/Jobs/Services/ServiceDialog.tsxTables: /components/Jobs/Services/Tables/CurrentServicesTable.tsx/components/Jobs/Services/Tables/PastServicesTable.tsx Table Toolbar: /components/Jobs/Services/ServicesTableToolbar.tsxAction Menu: /components/Jobs/Services/ServicesActionMenu.tsx (inferred)Supporting Dialogs: /components/Jobs/Services/ServiceCreateFromSpecDialog.tsx/components/Jobs/Services/EditServiceYamlDiffModal.tsx/components/Jobs/Services/ServicesStatusDialog.tsx/components/Jobs/Services/ScaleUpDownDialogContainer.tsx/components/Jobs/shared/EventsAndLogsDialog.tsx ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#state-management","content":"The panel uses Jotai atoms for state management: ServicesPanelSectionAtom - Current view (table/create/clone/edit/details)ServicesCloneDefaultValuesAtom - Values when cloning a serviceServiceIdToShowAtom - ID of service to display in detailsServicesDetailsValueAtom - Cached service detailsCurrentServicesAtom / PastServicesAtom - Service data for each tabCurrentServicesTablePageAtom / PastServicesTablePageAtom - Pagination stateCurrentServicesFiltersAtom / PastServicesFiltersAtom - Active filtersCurrentServicesDataLoadingAtom / PastServicesDataLoadingAtom - Loading stateServicesTableTabAtom - Active tab (0=Available, 1=History) Filters persist in URL query parameters for bookmarking and sharing. ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Deploy a New Microservice​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#deploy-a-new-microservice","content":"Navigate to Workloads → MicroservicesClick &quot;Create Microservice&quot; buttonGeneral Tab: Enter a unique service name (alphanumeric with dashes/dots)Choose subdomain (recommended) or subpath for the endpointSelect an Environment Config (resource template)Choose pipeline type: Shell (single script) or Multi-step (YAML pipeline)Specify the path to your startup script or YAMLOptionally configure Git repository, branch, and commit Advanced Tab (optional): Enable/disable default command executionConfigure working directoryEnable horizontal pod autoscaling with min/max replicasSet up failure alerts with notification targetsConfigure health check thresholds and cooldown periodsAdd Cloud SQL proxy for database connectionsAttach secrets and service accountsSelect billing project for cost tracking Parameters Tab (optional): Add key-value parameters accessible to your service Readme Tab (optional): Add markdown documentation for the service Click &quot;Create Microservice&quot; to deployService starts automatically and endpoint becomes available when healthy ","version":"Next","tagName":"h3"},{"title":"Clone and Modify an Existing Service​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#clone-and-modify-an-existing-service","content":"Find the service you want to clone in the Available or History tabClick the Actions menu (three dots) for the serviceSelect &quot;Clone&quot;The create dialog opens with all values pre-filled from the originalModify any settings (name, endpoint, configuration, etc.)Click &quot;Create Microservice&quot; to deploy the cloned serviceOriginal service continues running unchanged ","version":"Next","tagName":"h3"},{"title":"Scale a Running Service​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#scale-a-running-service","content":"Locate the running service in the Available tabCheck current scaling in the &quot;Scaling&quot; columnOption A - Quick Scale via Actions Menu: Click Actions → &quot;Scale Up/Down&quot;Enter new min/max replica countsClick &quot;Scale&quot; to apply Option B - Enable Autoscaling via Edit: Click Actions → &quot;Edit&quot;Navigate to Advanced tabEnable &quot;Horizontal Pod Autoscaling&quot;Set min and max replica countsSave changes (triggers restart) Monitor the &quot;Scaling&quot; column for replica status updates ","version":"Next","tagName":"h3"},{"title":"Stop and Start a Service​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#stop-and-start-a-service","content":"To Stop a Service: Find the running service in the Available tabClick Actions → &quot;Scale Up/Down&quot;Enter 0 for both min and max replicasClick &quot;Scale&quot;Service shows &quot;off&quot; status and stops consuming resources To Start a Stopped Service: Find the stopped service (shows &quot;off&quot; status)Click the start icon next to the service name, ORClick Actions → &quot;Scale Up/Down&quot;Enter desired replica count (e.g., min=1, max=1)Click &quot;Scale&quot;Service starts and endpoint becomes available when healthy ","version":"Next","tagName":"h3"},{"title":"Monitor Service Health and Debug Issues​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#monitor-service-health-and-debug-issues","content":"Find the service in the Available tabCheck the Status column for health indicators: &quot;healthy&quot; - Service is running normally&quot;not ready&quot; - Service starting or pods not ready&quot;off&quot; - Service scaled to 0&quot;failed&quot; - Service encountered errors (hover for reason) View Logs: Click the logs icon in the &quot;Logs&quot; columnSelect &quot;Events&quot; tab for Kubernetes eventsSelect &quot;Logs&quot; tab for container outputUse Grafana link for detailed metrics and dashboards View Full Details: Click on the service nameReview complete configurationCheck replica status and conditionsView startup parameters and environment Common Issues: &quot;not ready&quot; for extended time: Check logs for application errors&quot;failed&quot; status: Check status reason in hidden column or details0 ready replicas: Service may be crashing, check logsIf issues persist, use Edit to adjust configuration or Restart to apply fixes ","version":"Next","tagName":"h3"},{"title":"Configure Public Webhook Endpoints​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#configure-public-webhook-endpoints","content":"Create new microservice or edit existing oneIn the General tab, configure the subdomain field: For webhook: use name ending in -webhook (e.g., payment-processor-webhook)For public service: use name ending in -public (e.g., api-public) This makes the endpoint accessible without authenticationComplete other configuration as neededAfter deployment, share the public URL with external servicesNote: Regular subdomains and subpaths require authentication ","version":"Next","tagName":"h3"},{"title":"Edit Service Configuration​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#edit-service-configuration","content":"Find the service to edit (can be in Available or History tab)Click Actions → &quot;Edit&quot;Modify desired settings in the edit dialogReview the &quot;Customized YAML&quot; badge if present (indicates manual YAML edits)Click &quot;Save Changes&quot;If configuration changes affect runtime (image, script, etc.), service automatically restartsMonitor status to confirm successful restart ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#related-features","content":"Immediate Jobs - One-time pipeline executions without exposed portsScheduled Jobs - Recurring pipeline jobs that run on a scheduleEnvironment Configs - Define resource templates and container images for servicesStack Components - Pin frequently-used services for quick accessGit Repositories - Sync code from Git servers for service deploymentBilling Projects - Track costs and resource usage by project ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Microservices (Services Panel)","url":"/developer-docs/features/services#notes--tips","content":"Services vs Jobs: Microservices run continuously with timeout=-1 and activeTimeout=-1, while regular jobs have finite execution times. Services expose network ports; jobs typically don't.Endpoint Configuration: Prefer subdomains over subpaths for better URL structure and routing. Subdomains provide cleaner URLs and avoid path conflicts.Public Endpoints: Services with subdomains ending in -webhook or -public are accessible without authentication - useful for webhooks and public APIs.In-cluster URLs: Use the in-cluster URL pattern http://hyperplane-service-{id-prefix}.{namespace}.svc.cluster.local:8787 for service-to-service communication within Kubernetes.Stopping Services: Scaling to 0 replicas stops the service but preserves configuration. This is useful for cost savings during periods of non-use.Autoscaling: When HPA (Horizontal Pod Autoscaling) is enabled, Kubernetes automatically adjusts replicas based on CPU utilization between min and max bounds.Health Checks: Default TCP liveness (30s delay) and readiness probes (10s delay) can be enabled. Advanced users can customize via the pod YAML editor.Custom Pod YAML: Services with customized YAML show a badge indicator. Editing the service form with customized YAML may override manual changes - review the diff dialog before saving.Pinning Services: Pin frequently-accessed services to keep them at the top of the list for quick access.Default Port: Services default to port 8787, but this can be changed. Ensure your application listens on the configured port.Restart After Edit: Some configuration changes (environment config, pipeline path, image) trigger automatic restart. Others require manual restart to apply.Pipeline Types: Shell (BASH) runs a single startup script; Multi-step (YAML) supports complex multi-stage pipelines with dependencies.Git Integration: Non-custom environment configs require Git integration. Custom images can optionally disable Git sync.Status Polling: The UI automatically refreshes service status. Manual refresh button available in toolbar.Filtering: Filters persist in the URL, allowing you to bookmark or share specific filtered views.Grafana Integration: If configured, services automatically get Grafana dashboards for detailed metrics and logging. ","version":"Next","tagName":"h2"},{"title":"Sessions","type":0,"sectionRef":"#","url":"/developer-docs/features/sessions","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#overview","content":"The Sessions feature provides a comprehensive interface for managing interactive development environments (JupyterLab, VS Code, and Shell) in the Shakudo platform. Users can create, monitor, and manage containerized sessions with customizable compute resources, storage drives, and environment configurations. Sessions are ephemeral development environments that automatically shut down after a configurable idle timeout period. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#access--location","content":"Route: ?panel=sessionsNavigation: Development → SessionsAccess Requirements: None (basic authenticated user access)Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Create New Sessions​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#create-new-sessions","content":"Launch containerized development environments with customizable configurations including environment type, idle timeout, storage drive, billing project, service accounts, secrets, Cloud SQL proxies, and custom ports. Sessions support both public (shared) and private modes. ","version":"Next","tagName":"h3"},{"title":"View Active Sessions​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#view-active-sessions","content":"Monitor all currently running sessions with real-time status updates and quick access links to JupyterLab, VS Code, and Shell interfaces. Active sessions display connection status and automatically poll for server readiness. ","version":"Next","tagName":"h3"},{"title":"Browse Session History​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#browse-session-history","content":"Review past sessions with detailed information about session duration, resources used, status outcomes, and associated metadata. History can be filtered by status (cancelled, failed, done, etc.). ","version":"Next","tagName":"h3"},{"title":"Clone Sessions​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#clone-sessions","content":"Quickly duplicate existing session configurations to launch new sessions with identical settings, including environment configs, drives, service accounts, secrets, and billing projects. ","version":"Next","tagName":"h3"},{"title":"Stop Running Sessions​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#stop-running-sessions","content":"Terminate active sessions to free up compute resources. This action cancels the underlying pods and stops all associated services. ","version":"Next","tagName":"h3"},{"title":"Restart Failed Sessions​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#restart-failed-sessions","content":"Relaunch sessions that have failed or completed, recreating them with the same configuration and resources. ","version":"Next","tagName":"h3"},{"title":"Publish/Unpublish Sessions​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#publishunpublish-sessions","content":"Toggle session visibility between private (owner-only) and public (all platform users) modes. Published sessions can be accessed by any authenticated platform user. ","version":"Next","tagName":"h3"},{"title":"Add Ports to Active Sessions​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#add-ports-to-active-sessions","content":"Dynamically open additional network ports on running sessions to expose custom services or applications. ","version":"Next","tagName":"h3"},{"title":"SSH Access​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#ssh-access","content":"Copy SSH connection commands for direct terminal access to sessions via SSH proxy. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#main-view","content":"The Sessions panel features a tabbed interface with two main views: Active Sessions Tab - Displays currently running or pending sessions with: Real-time status indicators (in progress, pending)Quick access buttons for JupyterLab, VS Code, and ShellSSH connection command copy buttonSession metadata (ID, drive, environment config, start time)Published/private indicators History Tab - Shows completed, failed, or cancelled sessions with: Session lifecycle information (start time, end time, status)Historical session metadataAbility to restart or clone past sessions Both tabs support: Real-time filtering by email, drive name, environment config, and billing projectQuick search across drive names and email addressesPagination with 20 sessions per pageAuto-refresh every 30 secondsColumn visibility customization ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#dialogs--modals","content":"Session Create Dialog Purpose: Create new development sessions with full configuration controlFields: General Tab: Environment Config (required): Pre-configured development environmentCustom Image URL (optional): Override default image with custom container imageIdle Timeout (required): Auto-shutdown timer (presets: 15min, 30min, 1hr, 2hr, 12hr, 24hr, unlimited)Drive (required): Storage volume to mount (default or custom drives) Advanced Tab: Cloud SQL Proxy: Enable Google Cloud SQL sidecar connectionExternal Dependencies: Attach Kubernetes secrets and service accountsBilling Project: Associate session costs with specific billing projectPublish Session: Make session accessible to all platform usersOpen Additional Ports: Expose custom network ports Actions: Create Session, Customize YAML, View GraphQL MutationFeatures: Real-time session summary, YAML customization for advanced users Session Create From Spec Dialog Purpose: Create sessions from custom Kubernetes pod specificationsFields: YAML editor for full pod spec customizationActions: Create session from YAML, validate YAML syntaxFeatures: Pre-populated with default spec, syntax highlighting Session Status Dialog Purpose: View real-time status of session creation processShows: Pod events, container statuses, startup progressAccess: Click loading spinner on pending sessions Port Management Dialog Purpose: Configure custom network ports for sessionsFields: Port number, protocol (TCP/UDP), port nameActions: Add/remove ports dynamically Drive Management Dialog Purpose: Manage persistent storage drivesAccess: Storage icon next to Drive field in create dialog ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#tables--data-grids","content":"Active Sessions Table Columns: Actions (pinned right): Actions menu dropdownAccess: Quick links to JupyterLab, VS Code, Shell, SSH commandSession ID: Copyable unique identifierDrive: Storage volume name (shows &quot;user's default&quot; for default drives)Status: Visual status indicator with &quot;published&quot; chip for public sessionsEnv Config: Environment configuration nameCustom Image URL (hidden by default): Custom container imageEmail (hidden by default): Session owner emailBilling Project ID (hidden by default): Copyable project IDService Account ID (hidden by default): Copyable service account IDStart: Session creation timestampOpened Ports (hidden by default): List of exposed ports Actions: Clone sessionAdd ports (active sessions only)Stop sessionPublish/unpublish session Filtering: Email, environment config, drive name, billing project, quick searchFeatures: Auto-refresh, real-time status polling, link health checking History Sessions Table Columns: Actions (pinned right): Actions menu dropdownSession ID: Copyable unique identifierEnv Config: Environment configuration nameCustom Image URL (hidden by default): Custom container imageDrive: Storage volume nameStatus (hidden by default): Final session statusEmail (hidden by default): Session owner emailBilling Project ID (hidden by default): Copyable project IDService Account ID (hidden by default): Copyable service account IDStart: Session creation timestampEnd: Session completion/termination timestampOpened Ports (hidden by default): List of exposed ports Actions: Clone sessionRestart session (failed/done sessions only) Filtering: Email, status (cancelling, cancelled, failed, failing), environment config, drive name, billing project, quick searchFeatures: Auto-refresh, historical records ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#graphql-operations","content":"Queries: getSessions - Retrieves paginated list of sessions with filtering support (email, status, imageType, driveName, billingProjectId, quickSearch). Supports both active and historical queries via status filters. Returns session metadata including URLs, resource limits, costs, and pod specs.countSessions - Returns total count of sessions matching filter criteria for paginationgetSessionById - Fetches detailed information for a specific session by IDgetHyperhubSessionPodSpec - Generates Kubernetes pod specification JSON for session creationgetSessionResourceLimits - Retrieves resource quota information for sessions Mutations: createOneHyperHubSession - Creates a new session with specified configuration (imageType, timeout, drive, billing project, service account, secrets, Cloud SQL proxy, visibility, ports)cancelSession - Terminates a running session by IDrestartHyperhubSession - Relaunches a failed or completed session by IDtoggleSessionVisibility - Switches session between public and private modesupdateOneHyperHubSession - Updates session properties (primarily used for publishing/unpublishing) Subscriptions: None (uses polling for real-time updates) ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#component-structure","content":"Main Component: components/Sessions/SessionsPanel.tsxTable: components/Sessions/SessionsTable.tsxToolbar: components/Sessions/SessionsTableToolbar.tsxCreate Dialog: components/Sessions/Dialog/SessionCreateDialog.tsxCreate From Spec: components/Sessions/Dialog/SessionCreateFromSpecDialog.tsxActions Menu: components/Sessions/SessionsActionsMenu.tsxSession URLs: components/Sessions/SessionUrls.tsxMenu Items: components/Sessions/CloneSessionMenuItem.tsxcomponents/Sessions/RestartSessionMenuItem.tsxcomponents/Sessions/TogglePublishSessionMenuItem.tsxcomponents/Sessions/Dialog/StopSessionMenuItem.tsx Ports: components/Sessions/Ports/PortManagementDialog.tsx, components/Sessions/Ports/SessionAddPortsDialog.tsx ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#state-management","content":"Uses Jotai atoms for global state: SessionsPanelSectionAtom: Controls view mode (table, create, clone)SessionIdToShowAtom: Tracks selected session IDPastSessionsFiltersAtom: Stores history tab filtersCurrentSessionsFiltersAtom: Stores active tab filtersSessionsCloneDefaultValuesAtom: Holds default values for cloningSessionsTableTabAtom: Tracks active tab index (0=Active, 1=History) Uses React Context for: SessionLinks: Manages embedded/opened session URLsSessionsFrameContext: Controls session iframe display stateHyperplaneUserContext: Current user informationPodSpecsContext: Available environment configurationsKeycloakRBACContext: Permission checking ","version":"Next","tagName":"h3"},{"title":"URL Generation​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#url-generation","content":"Sessions support three access methods with automatic URL generation: JupyterLab: &lt;protocol&gt;://&lt;domain&gt;/&lt;hash&gt;/jupyter/lab?token=hyperhubVS Code: &lt;protocol&gt;://&lt;domain&gt;/&lt;hash&gt;/code/Shell: &lt;protocol&gt;://&lt;domain&gt;/&lt;hash&gt;/bash/ URLs are derived from the legacy jLabUrl field and converted using getNewJLabAndCodeServerUrlFromOldJLabUrl(). ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Creating a Basic Session​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#creating-a-basic-session","content":"Click &quot;Create Session&quot; button in the Sessions panelSelect an Environment Config (e.g., &quot;basic-ai-tools&quot;, &quot;python-data-science&quot;)Choose Idle Timeout (default: 15 minutes)Select storage Drive (defaults to user's default drive)Click &quot;Create Session&quot; buttonWait for session to start (status changes from &quot;pending&quot; to &quot;in progress&quot;)Access via JupyterLab, VS Code, or Shell icons when ready ","version":"Next","tagName":"h3"},{"title":"Creating a Session with Advanced Options​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#creating-a-session-with-advanced-options","content":"Click &quot;Create Session&quot; buttonConfigure general settings (environment, timeout, drive)Click &quot;Advanced&quot; button or switch to Advanced tabEnable Cloud SQL Proxy if needed and select proxyAttach any required Kubernetes secretsSelect service account for cloud provider authenticationAssign to billing project for cost trackingEnable &quot;Publish&quot; to make session accessible to all usersAdd custom ports if exposing additional servicesReview Session Summary panelClick &quot;Create Session&quot; ","version":"Next","tagName":"h3"},{"title":"Cloning an Existing Session​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#cloning-an-existing-session","content":"Locate the session to clone in Active or History tabClick &quot;Actions&quot; dropdown on the session rowSelect &quot;Clone&quot; from the menuReview and modify pre-populated configurationClick &quot;Create Session&quot; to launch the cloneNew session inherits all settings from original (environment, drive, service account, secrets, ports, billing project) ","version":"Next","tagName":"h3"},{"title":"Stopping a Running Session​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#stopping-a-running-session","content":"Find active session in Active Sessions tabClick &quot;Actions&quot; dropdownSelect &quot;Stop Session&quot;Session status changes to &quot;cancelling&quot; then &quot;cancelled&quot;Resources are freed and session appears in History tab ","version":"Next","tagName":"h3"},{"title":"Restarting a Failed Session​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#restarting-a-failed-session","content":"Navigate to History tabFind failed/completed sessionClick &quot;Actions&quot; dropdownSelect &quot;Restart&quot;Session is recreated with identical configurationNew session appears in Active tab ","version":"Next","tagName":"h3"},{"title":"Publishing a Session for Team Access​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#publishing-a-session-for-team-access","content":"Locate session in Active Sessions tabClick &quot;Actions&quot; dropdownSelect &quot;Publish Session&quot; (or &quot;Unpublish&quot; to revert)Session displays &quot;published&quot; chip next to statusAll platform users can now access this session's interfaces ","version":"Next","tagName":"h3"},{"title":"Adding Ports to Running Session​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#adding-ports-to-running-session","content":"Find running session in Active Sessions tabClick &quot;Actions&quot; dropdownSelect &quot;Add Ports&quot;In Port Management dialog, specify: Port number (e.g., 8080)Protocol (TCP/UDP)Port name (optional label) Click SavePort becomes accessible on session ","version":"Next","tagName":"h3"},{"title":"Accessing Session via SSH​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#accessing-session-via-ssh","content":"Find running session in Active Sessions tabLocate SSH terminal icon in Access columnClick icon to copy SSH connection commandPaste command in local terminalAuthenticate via SSH proxy to access session shell ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#related-features","content":"Environment Configs - Pre-configured development environments for sessionsPersistent Volumes - Persistent storage volumes mounted to sessionsBilling Projects - Cost tracking for session compute resourcesService Accounts - Cloud provider authentication for sessionsSecrets - Kubernetes secrets for external dependencies ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Session Lifecycle​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#session-lifecycle","content":"Sessions automatically stop after the configured idle timeout periodIdle timeout is calculated from last user activity (keyboard/mouse input)Timeout options: 15min, 30min, 1hr, 2hr, 12hr, 24hr, unlimited (-1)Default timeout is 15 minutes (900 seconds)Sessions in &quot;pending&quot; status are starting up and not yet readySessions in &quot;in progress&quot; status are fully running and accessibleFailed sessions can be restarted to retry with same configuration ","version":"Next","tagName":"h3"},{"title":"Resource Management​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#resource-management","content":"Each session consumes compute resources (CPU, RAM, GPU if configured)Sessions count against user or team resource quotasStopping unused sessions frees resources for other workloadsDefault drives are automatically created per user if not existsCustom drives can be shared across multiple sessionsDrive selection determines which files/folders are accessible in session ","version":"Next","tagName":"h3"},{"title":"Access Patterns​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#access-patterns","content":"JupyterLab provides notebook interface and file browserVS Code provides full IDE with extensions and terminalShell provides direct terminal access via browserAll three interfaces share the same filesystem (mounted drive)Sessions auto-detect when servers are ready and enable access buttonsServer readiness is polled every 5 seconds during startupPublished sessions bypass permission checks for access URLs ","version":"Next","tagName":"h3"},{"title":"Filtering & Search​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#filtering--search","content":"Quick search supports partial matches on email and drive name (case-insensitive)Multiple filters can be combined (AND logic)Email filter is automatically applied based on user rolesUsers without admin role only see their own sessions by defaultFilter state persists in URL query parameters for sharing/bookmarkingActive and History tabs maintain independent filter states ","version":"Next","tagName":"h3"},{"title":"GraphQL Mutation Visibility​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#graphql-mutation-visibility","content":"&quot;GraphQL Mutation&quot; button shows the exact API call for session creationUseful for debugging, API integration, or automation scriptsMutation updates in real-time as form fields changeCan be copied and executed in GraphQL Playground (/graphql endpoint) ","version":"Next","tagName":"h3"},{"title":"YAML Customization​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#yaml-customization","content":"&quot;Customize YAML&quot; allows direct editing of Kubernetes pod specificationAdvanced users can modify resource requests, node selectors, environment variablesYAML editor includes syntax highlighting and validationChanges override form-based configurationUseful for special pod configurations not exposed in UI ","version":"Next","tagName":"h3"},{"title":"Public Sessions​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#public-sessions","content":"Published sessions are accessible by all authenticated platform usersOwner retains full control (stop, restart, publish/unpublish)Public sessions show &quot;published&quot; chip in status columnUse for shared demonstrations, tutorials, or collaborative workConsider resource usage when publishing long-running sessions ","version":"Next","tagName":"h3"},{"title":"Clone vs Restart​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#clone-vs-restart","content":"Clone: Creates new session with copied configuration, allows modifications before launchRestart: Immediately recreates session with exact same configurationClone is available for all sessions (active and historical)Restart is only available for completed/failed sessionsBoth preserve environment config, drive, service account, secrets, billing project, and ports ","version":"Next","tagName":"h3"},{"title":"Performance​","type":1,"pageTitle":"Sessions","url":"/developer-docs/features/sessions#performance","content":"Tables auto-refresh every 30 seconds to show latest session statesActive sessions poll for server readiness independentlyPagination limits to 20 sessions per page for performanceColumn visibility can be customized to reduce table widthActions column is pinned to right for consistent access ","version":"Next","tagName":"h3"},{"title":"Shakudo Sync","type":0,"sectionRef":"#","url":"/developer-docs/features/shakudo-sync","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#overview","content":"Shakudo Sync is an administrative feature that enables synchronization and migration of applications between different Dify instances. It provides a centralized interface for importing Dify AI applications from a source Dify instance (exporter) to a target Dify instance (importer), making it easy to manage and replicate Dify apps across different environments. The feature performs health checks on both the source and target Dify instances, displays available applications from the source instance, and allows administrators to selectively import applications with real-time status tracking. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#access--location","content":"Route: ?panel=shakudo-syncNavigation: Admin → Shakudo SyncAccess Requirements: dashboard-admin role (RBAC level 0)Must be a Keycloak admin or have admin privileges Feature Flags: shakudoSyncEnabled must be enabled in platform parameters ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Health Monitoring​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#health-monitoring","content":"Automatically checks the connectivity and health status of both the exporter (source) and importer (target) Dify instances. Displays connection status with visual indicators (check mark for healthy, X for unhealthy) and provides the URLs of both instances. ","version":"Next","tagName":"h3"},{"title":"Application Discovery​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#application-discovery","content":"Fetches and displays all available Dify applications from the source instance, including application metadata such as name, icon, mode, creation date, last update timestamp, and previous import history. ","version":"Next","tagName":"h3"},{"title":"Selective Import​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#selective-import","content":"Allows administrators to select specific applications for import, either individually or in bulk using a &quot;Select All&quot; checkbox. The interface provides granular control over which applications to migrate. ","version":"Next","tagName":"h3"},{"title":"Batch Import Operations​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#batch-import-operations","content":"Supports importing multiple applications simultaneously with parallel processing. Each import operation is tracked independently with loading states, success confirmations, and error handling. ","version":"Next","tagName":"h3"},{"title":"Import History Tracking​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#import-history-tracking","content":"Displays the timestamp and ID of the most recent successful import for each application, allowing administrators to track when applications were last synchronized. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#main-view","content":"The main panel displays a grid layout with cards for different sync integrations. Currently, only Dify integration is implemented, showing: Dify logo and brandingHealth status indicator (loading, healthy, or error state)Source Dify instance URLClickable card to open the import dialog ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#dialogs--modals","content":"Import Dify Apps Dialog Purpose: Display available Dify applications and manage import operationsFields: Select All checkbox with application countSource/Target Dify instance URLsIndividual application cards with selection checkboxes Actions: Select/deselect individual applicationsSelect/deselect all applicationsImport selected applicationsCancel operation ","version":"Next","tagName":"h3"},{"title":"Application Cards​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#application-cards","content":"Each application card displays: Columns/Information: Application icon (emoji or default robot icon)Application name (truncated with ellipsis if too long)Application mode (e.g., chat, workflow)Application ID (first 8 characters, linked to source Dify instance)Created timestamp (formatted as YYYY-MM-DD HH:mm:ss)Updated timestamp (formatted as YYYY-MM-DD HH:mm:ss)Latest Import timestamp (when last synchronized)Latest Imported App ID (linked to target Dify instance) Actions: Checkbox for selectionLoading spinner during importLink to view app in source Dify instanceLink to view imported app in target Dify instance Filtering: None (displays all available applications) ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"API Endpoints​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#api-endpoints","content":"Health Check: GET /api/shakudo-sync/importer/apps/dify/health Checks connectivity to both exporter and importer Dify instancesReturns status, URLs, and error messages for both instancesBackend service: http://shakudo-sync-importer.hyperplane-core:8000/v1/dify/health Fetch Applications: GET /api/shakudo-sync/importer/apps/dify/fetch-apps Retrieves list of all available Dify applications from source instanceReturns application metadata including name, icon, mode, timestamps, and sync historyBackend service: http://shakudo-sync-importer.hyperplane-core:8000/v1/dify/apps Import Application: POST /api/shakudo-sync/importer/apps/dify/import-app Imports a single Dify application by IDRequest body: { id: string }Backend service: http://shakudo-sync-importer.hyperplane-core:8000/v1/dify/apps/migrate Latest Sync Info: GET /api/shakudo-sync/importer/apps/dify/latest-sync?id={app_id} Retrieves the most recent import timestamp and ID for a specific applicationBackend service: http://shakudo-sync-importer.hyperplane-core:8000/v1/dify/apps/{id}/latest_sync ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#component-structure","content":"Main Component: components/ShakudoSync/Panel.tsxApp Container: components/ShakudoSync/Apps/AppContainer.tsxDify Integration: components/ShakudoSync/Apps/Dify/DifySyncContainer.tsx - Main Dify cardcomponents/ShakudoSync/Apps/Dify/DifySyncDialog.tsx - Import dialogcomponents/ShakudoSync/Apps/Dify/DifyHealthCheck.tsx - Health status displaycomponents/ShakudoSync/Apps/Dify/DifyAllAppsGrid.tsx - Application listcomponents/ShakudoSync/Apps/Dify/DifyAppCard.tsx - Individual app cardcomponents/ShakudoSync/Apps/Dify/DifyAppCardLatestImport.tsx - Import history displaycomponents/ShakudoSync/Apps/Dify/DifySyncAppsButton.tsx - Import action buttoncomponents/ShakudoSync/Apps/Dify/DifyExporterTitle.tsx - Source/target URL display State Management: components/ShakudoSync/atoms.ts ","version":"Next","tagName":"h3"},{"title":"State Management (Jotai Atoms)​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#state-management-jotai-atoms","content":"ExporterDetails Stores health status, URL, and error messages for the source Dify instanceFields: { health: boolean | null, url: string, message: string } ImporterDetails Stores health status, URL, and error messages for the target Dify instanceFields: { health: boolean | null, url: string, message: string } SelectedDifyApps Array of selected application IDs for importType: string[] AllDifyAppsSelected Boolean flag indicating whether all applications are selectedType: boolean AllDifyApps Array of all available Dify applications from the source instanceType: Array&lt;DifyApp&gt; AppSyncStatesAtom Reducer-based atom tracking the sync state of each applicationStates per application: loading: Import operation in progresssuccess: Import completed successfullyerror: Import failed with error detailslatest_sync_attempt: Timestamp of most recent sync attempt ","version":"Next","tagName":"h3"},{"title":"Backend Service​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#backend-service","content":"The feature relies on a microservice called shakudo-sync-importer running in the hyperplane-core namespace on port 8000. This service handles the actual communication with both Dify instances and performs the application migration logic. ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Importing Dify Applications​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#importing-dify-applications","content":"Navigate to Admin → Shakudo Sync in the dashboard sidebarWait for health checks to complete (green check mark indicates healthy connection)Click on the Dify card to open the Import dialogReview the source and target Dify instance URLs at the top of the dialogSelect individual applications by clicking their checkboxes, or click &quot;Select All&quot; to choose all applicationsReview the application details (name, mode, creation date, last sync date)Click the &quot;Import&quot; button to begin the import processMonitor the import progress (loading spinners appear on selected apps)Wait for import completion notifications: Success: &quot;Imported all selected apps successfully!&quot;Partial success: &quot;Imported X apps successfully. Failed to sync Y apps.&quot;Failure: &quot;Failed to import any of the selected apps.&quot; Check the &quot;Latest Import&quot; column to verify successful importsClick the app ID links to view the imported applications in the target Dify instance ","version":"Next","tagName":"h3"},{"title":"Checking Import History​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#checking-import-history","content":"Open the Shakudo Sync panelClick on the Dify card to open the dialogReview the &quot;Latest Import&quot; column for each applicationThe timestamp shows when the application was last importedThe &quot;Latest Imported App&quot; link (app ID) navigates to the imported app in the target Dify instance ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Connection Issues​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#troubleshooting-connection-issues","content":"Open the Shakudo Sync panelCheck the health status indicator on the Dify cardIf the indicator shows an X (unhealthy): Hover over the X icon to see the error messageVerify the source Dify URL is accessibleContact the administrator to check the shakudo-sync-importer service Review the source URL displayed below the health indicatorEnsure both exporter and importer Dify instances are running and accessible ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#related-features","content":"Stack Components - For managing other platform integrationsService Accounts - For authentication credentials used by the sync serviceSecrets - For storing Dify API keys and connection credentials ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Shakudo Sync","url":"/developer-docs/features/shakudo-sync#notes--tips","content":"The feature currently supports only Dify application synchronization, but the architecture is designed to support additional integration types in the futureImport operations are performed in parallel for better performance when selecting multiple applicationsThe sync service maintains a history of imports, allowing you to track when each application was last synchronizedApplication IDs are truncated to 8 characters in the UI but full IDs are used for operationsThe dialog automatically resets selected applications when closed and reopenedFailed imports do not prevent other imports from completing - each import operation is independentHealth checks are performed automatically when opening the panelThe feature uses the shakudo-sync-importer microservice which must be running and properly configured for the feature to workApplication icons default to a robot emoji if the source application doesn't have a custom icon or emoji ","version":"Next","tagName":"h2"},{"title":"Stack Components","type":0,"sectionRef":"#","url":"/developer-docs/features/stack-components","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#overview","content":"The Stack Components panel provides a centralized interface for managing pre-configured data tools and services in the Shakudo platform. It allows users to browse, install, activate, pause, and monitor various data infrastructure components like databases, monitoring tools, and other third-party applications that are packaged as Helm charts. Users can access component dashboards, view logs, check service URLs, and manage the lifecycle of these tools. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#access--location","content":"Route: ?panel=stack-components (also accessible via ?panel=apps)Navigation: Main Dashboard → Stack Components (or Apps)Access Requirements: Basic viewing: Available to all usersInstall/Uninstall/Pause/Activate: Requires admin privileges (RBAC permissions)Shakudo Core components: Cannot be modified (system-managed) Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Browse Stack Components​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#browse-stack-components","content":"View all available data infrastructure components organized by status (Active/Inactive) with real-time status updates. Components display their name, logo, description, category, license type, and version information. The grid layout automatically adapts to show all components with smooth animations and transitions. ","version":"Next","tagName":"h3"},{"title":"Install New Components​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#install-new-components","content":"Install pre-configured tools from the catalogue into your Shakudo environment. When installing, components are deployed to dedicated Kubernetes namespaces with appropriate resource configurations. Installation progress can be tracked through real-time logs. ","version":"Next","tagName":"h3"},{"title":"Activate/Pause Components​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#activatepause-components","content":"Control resource usage by pausing inactive components (scales down to 0 replicas) or activating paused components (scales back to default replica count). This helps optimize cluster resources while maintaining configuration state. Status changes are reflected immediately with visual feedback. ","version":"Next","tagName":"h3"},{"title":"Pin Favorite Components​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#pin-favorite-components","content":"Pin frequently-used components to keep them at the top of the Active tab for quick access. Pins are user-specific and persist across sessions, allowing each team member to customize their view. ","version":"Next","tagName":"h3"},{"title":"Access Component Dashboards​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#access-component-dashboards","content":"Click on active components to open their native web interfaces. Components with custom dashboard pages open in dedicated views, while others open in an embedded iframe dialog. If a component has no UI, users are notified appropriately. ","version":"Next","tagName":"h3"},{"title":"Monitor Component Logs​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#monitor-component-logs","content":"View real-time Kubernetes event logs for any active component to troubleshoot issues, monitor startup progress, or verify operational status. Logs auto-refresh every 2 seconds and show the most recent 20 lines of events. ","version":"Next","tagName":"h3"},{"title":"View Internal Service URLs​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#view-internal-service-urls","content":"Access the in-cluster service URLs and port information for components, useful for connecting services together or debugging network connectivity. This dialog also displays component usage documentation and license information when available. ","version":"Next","tagName":"h3"},{"title":"Search and Filter​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#search-and-filter","content":"Quickly find components using the search bar, which filters by component name or category. Results update in real-time as you type, searching across all components in the selected tab. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#main-view","content":"The main panel displays a grid of component cards organized into two tabs: Active Tab: Shows components with ACTIVE, SCALING, or PAUSING status. Pinned components appear first, followed by others alphabetically.Inactive Tab: Shows components with PAUSED status that can be reactivated. Each card includes: Component logo/iconComponent name with version badge (if applicable)License typeDescription preview (truncated to 2 lines)Category tagAction buttons (logs, monitoring, install/pause/activate, info)Pin button (Active tab only) The grid uses responsive layout with cards sized at minimum 300px width, automatically adjusting columns based on viewport size. Cards feature hover effects and smooth animations when filtering or status changes occur. ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#dialogs--modals","content":"Component Dashboard Dialog Purpose: View the component's web interface in an embedded iframeDisplays: Full-screen iframe loading the component's appUrlActions: Close dialogUsed when: Clicking on a component that has a web UI but no custom dashboard page Component Details Dialog Purpose: View internal service information, usage documentation, and license detailsTabs: Internal Services: Lists Kubernetes services in the component's namespace with ClusterIP and port detailsComponent Details: Displays markdown-formatted usage documentation (if available)License: Shows license terms and conditions (if available) Actions: Close dialogUsed when: Clicking the info icon on a component card Component Logs Dialog Purpose: Monitor real-time Kubernetes event logs for the componentDisplays: Scrollable text area with the last 20 lines of namespace/pod eventsRefresh: Auto-polls every 2 seconds while openActions: Close dialogUsed when: Clicking the logs icon on an active component Install/Pause/Activate Confirmation Purpose: Confirm lifecycle actions before executionDisplays: Action description and component nameActions: Confirm or CancelUsed when: Clicking install, pause, or activate buttons ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#tables--data-grids","content":"The panel uses a card-based grid layout rather than traditional tables. The grid displays stack components as individual cards with the following structure: Active Components Grid Layout: Responsive CSS grid with auto-fill columns (minimum 300px)Sorting: Pinned components first, then alphabetical by nameFiltering: Search by name/category, real-time updatesAnimation: Smooth transitions when cards appear/disappear Inactive Components Grid Layout: Same responsive grid layoutSorting: Alphabetical by nameFiltering: Search by name/categoryNo pinning functionality ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#graphql-operations","content":"Queries: GetApps - Fetches all platform apps with their metadata (name, status, namespace, URLs, images, descriptions, categories, licenses). Supports optional WHERE clause filtering for search.GetAppVersions - Retrieves version information for all deployed Helm applications across namespaces (returns getHelmAllAppVersions as a nested object)GetPinnedApps - Fetches the current user's pinned platform apps (filtered by ACTIVE status and show=true)GetNamespaceResourceServices - Gets Kubernetes service details (ClusterIP, ports) for a specific namespace and label selectorGetNamespaceEventLogs - Retrieves Kubernetes event logs for resources in a namespace matching a label selector (last N lines)installStackComponent - Installs a stack component by name and namespace, returns installation statusscaleNamespaceResourcesToDefault - Scales up resources in a namespace (with label selector) to their default replica countsscaleDownNamespaceResources - Scales down resources in a namespace (with label selector) to 0 replicas Mutations: PinApp - Connects a platform app to the current user's pinned apps listUnpinApp - Disconnects a platform app from the current user's pinned apps listinstallOnePlatformApp - Creates and installs a new platform app (used for catalogue installations)deleteOnePlatformApp - Permanently removes a platform app by name Subscriptions: None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#component-structure","content":"Main Component: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformApps/PlatformAppsPanel.tsxGrid View: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformApps/PlatformAppsGrid.tsxCard Component: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformApps/PlatformAppsCard.tsxInstall/Control: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformApps/PlatformAppsInstall.tsxDialogs: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformApps/PlatformAppsDetailsDialog.tsx/root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformApps/PlatformAppsLogsDialog.tsx/root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformApps/PlatformAppIframeDialog.tsx Pin Button: /root/gitrepos/monorepo/apps/hyperplane-dashboard/components/PlatformApps/PlatformAppsPinButton.tsx ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#state-management","content":"Jotai Atom: PlatformAppPageAtom - Stores currently selected platform app for detail pagesApollo Client: Polling interval of 5000ms (5 seconds) for GetApps query to keep status updatedLocal State: Component-level state for dialogs, search filters, and tab selection ","version":"Next","tagName":"h3"},{"title":"Component Categories​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#component-categories","content":"Components are organized by platformAppCategoryName with a special category &quot;SHAKUDO_CORE&quot; for system-managed infrastructure (JupyterHub, Keycloak, etc.) that cannot be modified by users. ","version":"Next","tagName":"h3"},{"title":"Install Status Types​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#install-status-types","content":"ACTIVE: Component is running and accessiblePAUSED: Component is scaled down (0 replicas) but configuration preservedSCALING: Component is currently scaling up (transitioning to ACTIVE)PAUSING: Component is currently scaling down (transitioning to PAUSED)CATALOGUED: Component is available for installation but not yet installedARCHIVED: Component is deprecated/archivedUNAVAILABLE: Component status unknown or unavailableUNKNOWN: Component status cannot be determined ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Installing a New Stack Component​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#installing-a-new-stack-component","content":"Navigate to Stack Components panelBrowse available components in the Inactive or catalogue viewClick the Install icon (download arrow) on the desired component cardConfirm installation in the dialogWait for installation to complete (status changes from CATALOGUED to ACTIVE)Click the logs icon to monitor installation progressOnce active, click the component card to access its dashboard ","version":"Next","tagName":"h3"},{"title":"Pausing an Active Component​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#pausing-an-active-component","content":"Locate the active component in the Active tabClick the Pause icon (pause symbol) in the component card footerConfirm the pause action in the dialogComponent status changes to PAUSING, then PAUSEDComponent moves to the Inactive tabResources are scaled down to save cluster capacity ","version":"Next","tagName":"h3"},{"title":"Reactivating a Paused Component​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#reactivating-a-paused-component","content":"Switch to the Inactive tabLocate the paused componentClick the Activate icon (play arrow) in the component card footerConfirm the activationComponent status changes to SCALING, then ACTIVEComponent moves back to the Active tabClick the component card to verify it's accessible ","version":"Next","tagName":"h3"},{"title":"Pinning Frequently-Used Components​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#pinning-frequently-used-components","content":"Hover over an active component cardClick the pin icon in the top-right cornerComponent immediately moves to the top of the Active tabPin state persists across browser sessionsClick the pin icon again to unpin ","version":"Next","tagName":"h3"},{"title":"Viewing Component Service URLs​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#viewing-component-service-urls","content":"Click the info icon (i) on any component cardView the &quot;Internal Services&quot; tab showing Kubernetes service detailsCopy ClusterIP and port information for connecting other servicesSwitch to &quot;Component Details&quot; tab for usage documentation (if available)Check &quot;License&quot; tab for license terms (if available) ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Component Issues​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#troubleshooting-component-issues","content":"Click the logs icon (radio waves) on the component cardReview real-time Kubernetes event logs in the dialogLogs auto-refresh every 2 secondsLook for error messages, pod crashes, or resource issuesClick info icon to check service URLs and verify network connectivityIf needed, pause and reactivate the component to force a restart ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#related-features","content":"Sessions - User development environments that may connect to stack componentsJobs - Data pipeline jobs that may use stack components as data sourcesServices - Custom services that may integrate with stack componentsApps Catalogue - Browse and install additional stack components ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Best Practices​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#best-practices","content":"Pin your most frequently-used components (databases, monitoring tools) for quick accessPause components you're not actively using to save cluster resourcesCheck component logs immediately after installation to verify successful deploymentUse the search bar to quickly locate components in large installationsReview component details dialog for usage instructions and connection information ","version":"Next","tagName":"h3"},{"title":"Important Limitations​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#important-limitations","content":"Shakudo Core components (marked with &quot;SHAKUDO_CORE&quot; category) cannot be paused, activated, or uninstalled as they are essential system servicesOnly users with admin privileges can install, uninstall, pause, or activate componentsComponents in SCALING or PAUSING status cannot have actions performed on them until the transition completesSome components may not have web dashboards - check the info dialog for service URLs instead ","version":"Next","tagName":"h3"},{"title":"Performance Considerations​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#performance-considerations","content":"The panel polls for updates every 5 seconds to show real-time status changesLog dialogs poll every 2 seconds while open - close them when not needed to reduce loadPinned components are stored per-user and require an additional GraphQL queryComponent version information is fetched once at panel load for all namespaces ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Common Issues​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#troubleshooting-common-issues","content":"&quot;Component has no dashboard&quot; message: Not all components provide web UIs - use the info dialog to find service URLs insteadPin button disabled with error icon: Failed to load user pin preferences - refresh the pageComponent stuck in SCALING status: Check logs for errors, may need manual interventionSearch not finding components: Ensure component names or categories match your search termsVersion information missing: Not all components report versions, especially custom deployments ","version":"Next","tagName":"h3"},{"title":"Version Information Display​","type":1,"pageTitle":"Stack Components","url":"/developer-docs/features/stack-components#version-information-display","content":"Single version: Shows as a small badge next to component nameMultiple versions: Shows &quot;View versions&quot; chip that expands to list all detected versionsVersion detection: Attempts to match based on URL, app name, or explicit release nameShakudo Core components: Do not display version badges ","version":"Next","tagName":"h3"},{"title":"Traffic Shifting","type":0,"sectionRef":"#","url":"/developer-docs/features/traffic-shifting","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#overview","content":"Traffic Shifting (also called Traffic Management) is a feature that enables users to distribute network traffic across multiple microservices using weighted routing. It creates a virtual service (traffic shifter) that acts as a single endpoint and intelligently distributes incoming requests to multiple backend microservices based on configurable percentage weights. This is useful for A/B testing, canary deployments, blue-green deployments, and gradual rollouts of new service versions. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#access--location","content":"Route: ?panel=traffic-shiftingNavigation: Network → Traffic ShiftingAccess Requirements: Users can view their own traffic shiftersAdmin roles can view all traffic shifters (controlled by TrafficShiftingRoles.filters.email)Cancel action requires ownership or specific admin role (TrafficShiftingRoles.actions.cancel) Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Create Traffic Shifter​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#create-traffic-shifter","content":"Create a virtual service endpoint that distributes traffic across multiple microservices with configurable weight percentages. Users can add up to 5 microservices and define how traffic is split between them (e.g., 70% to Service A, 30% to Service B). ","version":"Next","tagName":"h3"},{"title":"Update Traffic Distribution​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#update-traffic-distribution","content":"Modify the traffic weight distribution for existing traffic shifters. Users can add or remove microservices, adjust percentage weights, and update descriptions without recreating the traffic shifter. ","version":"Next","tagName":"h3"},{"title":"Cancel Traffic Shifter​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#cancel-traffic-shifter","content":"Deactivate a traffic shifter to stop routing traffic through the virtual service. This is a permanent action that removes the traffic splitting configuration. ","version":"Next","tagName":"h3"},{"title":"Monitor Traffic Distribution​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#monitor-traffic-distribution","content":"View active traffic shifters with their associated microservices, current traffic distribution percentages, status, and endpoint URLs. The interface uses a hierarchical tree view where each traffic shifter can be expanded to show its constituent microservices. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#main-view","content":"The main panel displays a hierarchical data grid showing all traffic shifters with expandable rows revealing the microservices behind each shifter. The table shows: Traffic shifter name and subdomainOverall status (Active/Inactive)Distribution percentages for each microserviceEndpoint URLsIndividual microservice status and logsUser ownership information ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#dialogs--modals","content":"Traffic Shifting Create Dialog Purpose: Create a new traffic shifter with multiple microservicesFields: Traffic Shifter Name (required): Display name for the traffic shifterName (subdomain) (required): Subdomain where the traffic shifter will be hosted (alphanumeric, dashes, dots, max 63 chars)Description: Optional descriptionServices: Select up to 5 microservices to includeTraffic Weights: Percentage distribution for each service (must total 100%) Actions: Add Service: Add another microservice to the distributionEvenly Distribute Traffic: Automatically balance weights equally across all servicesPin/Unpin Weight: Lock a service's weight percentage to prevent automatic adjustmentsCreate Traffic Shifter: Submit and create the traffic shifter Features: Random webhook URL generator (creates public URLs ending in -webhook or -public)Live summary panel showing configuration before creationSmart weight adjustment: When adjusting one service's weight, others are automatically recalculated Cancel Traffic Shift Dialog Purpose: Permanently deactivate a traffic shifterFields: Confirmation messageActions: Cancel (deactivate) or Close (abort) Traffic Shifter Detail Panel Purpose: View and edit an existing traffic shifterFields: Description (editable)Service list with weight sliders Actions: Adjust weights using sliders or numeric inputAdd/remove servicesEvenly distribute trafficReset to original valuesConfirm changes Traffic Shifting Input Component Purpose: Manage the list of microservices and their traffic weightsFeatures: Service autocomplete for selecting microservicesWeight slider (0-100%) with numeric inputPin functionality to lock specific weights during adjustmentsDelete service button (disabled when pinned)Intelligent weight redistribution algorithm Traffic Shifter Weight Input Slider Purpose: Fine-tune individual service traffic percentagesFeatures: Slider control (0-100%)Numeric text input with percentage symbolAutomatic clamping to valid range (0-100)Integrated with smart redistribution logic Traffic Table Filter Dialog Purpose: Filter traffic shifters by various criteriaFields: Status: Active/InactiveTraffic Shifter ID or namePipeline YAML PathTimeline filters (Started/Completed with date range) Actions: Apply filters or Cancel ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#tables--data-grids","content":"Current Traffic Shifting Table Columns: Actions: Menu with Cancel option (for traffic shifters), Events/Logs (for microservices)Name (subdomain/service): Display name or job name (clickable to view details)Status: Active/Inactive for traffic shifters, standard status for microservicesLogs: Grafana link and events/logs dialog for microservicesDistribution: Traffic percentage (100% for parent, specific % for children)ID: Traffic shifter or microservice ID (hidden by default)Endpoint: Host URL for traffic shifter, service URL for microservicesPort: Exposed port for microservicesPipeline YAML Path: Path to the pipeline definitionStart: Start timestampUser Email: Owner email Actions: Click name to view detailsExpand/collapse traffic shifter rows to show microservicesCancel traffic shifter from actions menu Filtering: Filter by status, ID, name, timeline, YAML pathEmail filter automatically applied based on user roleFilter chips display active filtersReset filters button ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#graphql-operations","content":"Queries: getTrafficSplit - Retrieves traffic shifters with pagination, filtering, and associated microservice information including weight distribution Mutations: createTrafficSplit - Creates a new traffic shifter with display name, host, service-to-weight mapping, namespace, and descriptionupdateTrafficSplit - Updates an existing traffic shifter's display name, description, and service weight distributiondeactivateTrafficSplit - Deactivates/cancels a traffic shifter by ID, namespace, and name Subscriptions: None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#component-structure","content":"Main Component: components/TrafficShifting/TrafficShiftingPanel.tsxDialogs: components/TrafficShifting/Dialogs/ TrafficShiftingCreateDialog.tsx - Create new traffic shifterCancelTrafficShiftDialog.tsx - Cancel/deactivate traffic shifterTrafficShifterDetailPanel.tsx - View and edit traffic shifter detailsTrafficShiftingInput.tsx - Manage services and weightsTrafficShifterWeightInputSlider.tsx - Weight adjustment sliderTrafficTableFilterDialog.tsx - Filter traffic shifters Tables: components/TrafficShifting/ TrafficShiftingTables.tsx - Query and data managementCurrentTrafficShiftingTable.tsx - Table display and columns Details: components/TrafficShifting/TrafficShiftingDetails.tsx ","version":"Next","tagName":"h3"},{"title":"State Management​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#state-management","content":"The feature uses Jotai atoms for state management: CurrentTrafficAtom - Stores traffic shifter dataCurrentTrafficTablePageAtom - Current table pageCurrentTrafficFiltersAtom - Active filtersCurrentTrafficDataLoadingAtom - Loading stateTrafficPanelSectionAtom - Current panel section (table/create/details)TrafficDetailsValueAtom - Selected traffic shifter for details viewJobsDetailsValueAtom - Selected microservice for details view ","version":"Next","tagName":"h3"},{"title":"Weight Distribution Algorithm​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#weight-distribution-algorithm","content":"The traffic shifter implements an intelligent weight redistribution algorithm: When a user adjusts one service's weight, the system automatically adjusts other services to maintain 100% totalPinned services are excluded from automatic adjustmentsAdjustments prioritize services after the modified service in the listIf no subsequent services exist, adjustments are distributed across all unpinned servicesThe algorithm handles edge cases to ensure weights always total exactly 100% ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Creating a Basic Traffic Shifter​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#creating-a-basic-traffic-shifter","content":"Click &quot;Create Traffic Shifter&quot; buttonEnter a Traffic Shifter NameEnter or generate a subdomain nameAdd 2+ microservices using the &quot;Add Service&quot; buttonSelect microservices from the dropdownAdjust traffic weights using sliders (or use &quot;Evenly Distribute Traffic&quot;)Review the summary panelClick &quot;Create Traffic Shifter&quot; ","version":"Next","tagName":"h3"},{"title":"Performing a Canary Deployment​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#performing-a-canary-deployment","content":"Create a traffic shifter with your stable service at 95% and canary service at 5%Monitor metrics and logs for both servicesView details of the traffic shifterGradually increase the canary service weight (e.g., 10%, 25%, 50%)Use &quot;Confirm&quot; to apply changesContinue monitoring and adjusting until canary is at 100% or rollback if issues occur ","version":"Next","tagName":"h3"},{"title":"A/B Testing Multiple Versions​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#ab-testing-multiple-versions","content":"Create a traffic shifter with two service versionsSet equal weights (50/50) or desired distributionPin both weights to prevent accidental changesMonitor user behavior and metrics through logsAdjust distribution based on resultsRemove underperforming version when test completes ","version":"Next","tagName":"h3"},{"title":"Updating Traffic Distribution​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#updating-traffic-distribution","content":"Click on a traffic shifter name to view detailsAdjust weights using sliders or numeric inputsAdd new services with &quot;+&quot; button if neededRemove services using the delete iconUse &quot;Evenly Distribute Traffic&quot; for equal distributionClick &quot;Confirm&quot; to apply changesUse &quot;Reset&quot; to revert to previous configuration ","version":"Next","tagName":"h3"},{"title":"Filtering Traffic Shifters​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#filtering-traffic-shifters","content":"Click &quot;Filter&quot; button in the toolbarSelect status (Active/Inactive)Enter traffic shifter ID or nameSet timeline ranges (Started/Completed dates)Click &quot;Filter&quot; to applyView active filters as chipsClick &quot;Reset Filters&quot; to clear all filters ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#related-features","content":"Microservices/Pipeline Jobs - The backend microservices that traffic shifters route toNetwork/Ingress Configuration - Traffic shifters create Kubernetes VirtualService resourcesStack Components - Pre-configured services that can be used behind traffic shifters ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Traffic Shifting","url":"/developer-docs/features/traffic-shifting#notes--tips","content":"Subdomain Naming: Subdomains ending in -webhook or -public are publicly accessible without authenticationWeight Pinning: Pin service weights when you want specific services to maintain exact percentages during adjustmentsMaximum Services: Up to 5 microservices can be added to a single traffic shifterWeight Totals: The system automatically ensures weights always total 100%Public URLs: Random webhook URLs can be generated for public-facing traffic shiftersHierarchical View: Expand traffic shifter rows to see individual microservice status and logsGrafana Integration: Each microservice provides a direct link to Grafana logsEmail Filtering: Non-admin users automatically see only their own traffic shiftersNamespace: Traffic shifters are deployed to the hyperplane-pipelines namespaceVirtual Service: Each traffic shifter creates a Kubernetes VirtualService with the name format hyperplane-traffic-shifter-{id-first-6-chars}Smart Distribution: When adjusting weights, unpinned services are automatically recalculated to maintain 100% totalDeactivation: Canceling a traffic shifter is permanent and immediately stops traffic routing ","version":"Next","tagName":"h2"},{"title":"Triton Inference Server","type":0,"sectionRef":"#","url":"/developer-docs/features/triton-inference-server","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#overview","content":"The Triton Inference Server panel provides a comprehensive interface for managing NVIDIA Triton Inference Server deployments within the Shakudo Platform. This feature enables users to monitor server health, manage AI model loading/unloading, and control serving endpoints for production machine learning inference workloads. Triton supports models from any framework (TensorFlow, PyTorch, ONNX, TensorRT, or custom) and can be deployed on GPU or CPU infrastructure. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#access--location","content":"Route: ?panel=triton-inference-serverNavigation: Main Navigation → Triton Inference ServerAccess Requirements: None specified (standard user access)Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Server Health Monitoring​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#server-health-monitoring","content":"The panel continuously monitors the Triton Inference Server health status by checking the /v2/health/ready endpoint. A visual indicator (green/red circle) displays whether the server is healthy and ready to serve requests. ","version":"Next","tagName":"h3"},{"title":"Model Management​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#model-management","content":"Users can view all models in the Triton model repository and control their loading state. Models can be individually loaded or unloaded from the server's memory, or bulk operations can load/unload all models simultaneously. This allows for efficient resource management when multiple models are available. ","version":"Next","tagName":"h3"},{"title":"Endpoint Management​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#endpoint-management","content":"The panel tracks active Triton serving endpoints (pipeline jobs) that are currently running. Users can view endpoint details, check their health status, and cancel endpoints when needed. Each endpoint represents a running service that exposes model inference capabilities via HTTP/HTTPS. ","version":"Next","tagName":"h3"},{"title":"Real-time Logs​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#real-time-logs","content":"Both server-level logs and endpoint-specific logs are available in dedicated panels, providing visibility into model operations, inference requests, and system events. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#main-view","content":"The interface features two primary tabs accessed via chip buttons: Models Tab: Displays the models table with server logs panel on the sideEndpoints Tab: Shows the endpoints table with endpoint-specific logs panel on the side A server health indicator is prominently displayed in the header, showing real-time status of the Triton server. ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#dialogs--modals","content":"Cancel Endpoint Dialog Purpose: Confirm cancellation of a running Triton endpointFields: Confirmation message with endpoint IDActions: Close (abort) or Cancel (confirm deletion) ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#tables--data-grids","content":"Models Table Columns: Model: Model name (clickable to copy)Version: Model version numberBucket Path: Full path to model in cloud storage (clickable to copy)State: Toggle switch showing Loaded/Unloaded status Actions: Load All: Load all available models into server memoryUnload All: Unload all models from server memoryRefresh: Reload the models listIndividual toggle: Load/unload specific models Filtering: NonePagination: 10 items per page Endpoints Table Columns: Name: Endpoint name with cancel button (clickable to copy)Endpoint: Full URL to the serving endpoint (clickable to copy)Health: Real-time health check indicator Actions: Cancel endpoint (X button per row)Refresh: Reload the endpoints listRow click: Select endpoint to view logs Filtering: Automatically filters to only show active Triton endpoints (excludes cancelled, failed, or completed jobs)Pagination: Server-side pagination with 10 items per page ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#graphql-operations","content":"Queries: tritonServices - Fetches active Triton pipeline jobs (endpoints) with filtering for jobType='triton' and excluding cancelled/failed/completed jobs. Returns id, jobName, jobType, status, dashboardPrefix, and daskDashboardUrl. Mutations: cancelEndpoint - Updates a pipeline job status to 'cancelled' by ID, effectively terminating the endpoint. Subscriptions:None ","version":"Next","tagName":"h3"},{"title":"REST API Endpoints​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#rest-api-endpoints","content":"Model Operations: POST /api/triton-dashboard/get-models - Fetches model repository index from Triton server (/v2/repository/index)POST /api/triton-dashboard/load-models - Loads or unloads a specific model (/v2/repository/models/{name}/{action}) Server Monitoring: POST /api/triton-dashboard/check-url - Health check endpoint validatorPOST /api/triton-dashboard/logs - Retrieves Triton server logsPOST /api/triton-dashboard/server-metrics - Fetches server performance metrics Endpoint Operations: POST /api/triton-dashboard/check-endpoint-status - Validates endpoint healthPOST /api/triton-dashboard/endpoint-logs - Retrieves logs for specific endpoints ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#component-structure","content":"Main Component: shakudo-apps/triton-dashboard/components/Panels/TritonPanel.tsxTables: shakudo-apps/triton-dashboard/components/Tables/TritonModels.tsxshakudo-apps/triton-dashboard/components/Tables/TritonEndpoints.tsx Dialogs: shakudo-apps/triton-dashboard/components/Dialogs/CancelEndpoint.tsxToggles: shakudo-apps/triton-dashboard/components/Toggle/LoadUnloadModelToggle.tsxLog Containers: shakudo-apps/triton-dashboard/components/Containers/TritonLogs.tsxshakudo-apps/triton-dashboard/components/Containers/EndpointLogs.tsx ","version":"Next","tagName":"h3"},{"title":"Context & Configuration​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#context--configuration","content":"TritonAppContext: Provides server URL and model repository path configurationEnvironment Variables: TRITON_SERVER: Base URL for the Triton Inference Server ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Deploying a New Model​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#deploying-a-new-model","content":"Upload model checkpoint to the Triton model repository (cloud bucket path: {bucket}/triton-server/model-repository/)Structure the model following Triton model repository formatWait for automatic detection or manually refresh the Models tabToggle the model state from &quot;Unloaded&quot; to &quot;Loaded&quot;Verify the model appears as &quot;Loaded&quot; in the state column ","version":"Next","tagName":"h3"},{"title":"Creating a Model Serving Endpoint​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#creating-a-model-serving-endpoint","content":"Ensure your model is loaded in the Models tabWrite a client application using Triton client librariesWrap the client with FastAPI or FlaskDeploy the client as a pipeline job with jobType='triton'Monitor the endpoint in the Endpoints tabUse the provided URL to make inference requests ","version":"Next","tagName":"h3"},{"title":"Managing Server Resources​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#managing-server-resources","content":"Navigate to the Models tabReview which models are currently loadedUnload unused models to free memory using individual togglesUse &quot;Load All&quot; before batch inference operationsUse &quot;Unload All&quot; to clear server memory completely ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Failed Endpoints​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#troubleshooting-failed-endpoints","content":"Switch to the Endpoints tabIdentify the problematic endpointClick on the endpoint row to view its logs in the side panelReview logs for error messagesIf necessary, cancel the endpoint using the X buttonFix the underlying issue and redeploy ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#related-features","content":"Immediate Jobs - Triton endpoints are managed as pipeline jobsServices - Similar service management for other types of deploymentsEnvironment Configs - Configure compute resources for Triton deployments ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#notes--tips","content":"","version":"Next","tagName":"h2"},{"title":"Model Repository Best Practices​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#model-repository-best-practices","content":"Follow the Triton model repository structure strictly to ensure automatic detectionFor TensorFlow models, config.pbtxt can be auto-generated by TritonModel files are stored at: {cloud_bucket}/triton-server/model-repository/Each model should have its own subdirectory with version subdirectories ","version":"Next","tagName":"h3"},{"title":"Performance Optimization​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#performance-optimization","content":"Only keep frequently-used models loaded to optimize memory usageUnload models during low-traffic periods to free resourcesUse bulk load operations when preparing for batch inference workloadsMonitor server health indicator before deploying new endpoints ","version":"Next","tagName":"h3"},{"title":"Endpoint Configuration​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#endpoint-configuration","content":"Custom URL endpoints can be specified during client deploymentInference endpoints typically follow pattern: https://{domain}/hyperplane.dev/{endpoint_name}/infer/Endpoints require the daskDashboardUrl field to appear in the endpoints table ","version":"Next","tagName":"h3"},{"title":"Multi-Model and Ensemble Serving​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#multi-model-and-ensemble-serving","content":"Multiple models can be loaded simultaneously and served from a single endpointParameterize client inference functions with model_name for multi-model endpointsEnsemble models use the ensemble platform in config.pbtxt with ensemble_scheduling configurationEnsemble models can execute multiple models concurrently using Python backend with asyncio ","version":"Next","tagName":"h3"},{"title":"Log Monitoring​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#log-monitoring","content":"Server logs (left panel in Models tab) show server-level events and model loading operationsEndpoint logs (right panel in Endpoints tab) show request-specific logs for selected endpointsClick on any endpoint row to switch the log view to that specific endpoint ","version":"Next","tagName":"h3"},{"title":"Health Checks​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#health-checks","content":"Server health is checked via /v2/health/ready endpointIndividual endpoint health indicators appear in the Health columnGreen indicator = healthy and ready, Red indicator = unhealthy or not ready ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Triton Inference Server","url":"/developer-docs/features/triton-inference-server#troubleshooting","content":"If models don't appear after upload, check the model repository structure and refreshIf load/unload operations fail, verify server health and check server logsEndpoint cancellation changes job status but doesn't immediately terminate running processesFailed endpoints remain visible until explicitly filtered or cancelled ","version":"Next","tagName":"h3"},{"title":"Tutorials","type":0,"sectionRef":"#","url":"/developer-docs/features/tutorials","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#overview","content":"The Tutorials panel provides users with access to a curated collection of video tutorials and documentation that help them learn how to use different features of the Shakudo Dashboard. Tutorials are fetched from a remote CDN and cover various platform capabilities including sessions, jobs, microservices, and stack components. The panel provides an interactive, searchable gallery of tutorial cards with video previews and links to detailed documentation. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#access--location","content":"Route: ?panel=tutorialsNavigation: Root Menu → TutorialsAccess Requirements: None (available to all authenticated users)Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"Browse Tutorials​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#browse-tutorials","content":"Users can browse through all available tutorials in a responsive grid layout. Each tutorial is displayed as a card with a preview image or video, title, and description. ","version":"Next","tagName":"h3"},{"title":"Search Tutorials​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#search-tutorials","content":"The search functionality allows users to quickly find relevant tutorials by searching through tutorial titles and descriptions. The search uses fuzzy matching to provide flexible results. ","version":"Next","tagName":"h3"},{"title":"Watch Tutorial Videos​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#watch-tutorial-videos","content":"Tutorials that include video content can be watched directly in the dashboard through an interactive video dialog. Videos autoplay on hover and can be expanded to full-screen viewing. ","version":"Next","tagName":"h3"},{"title":"Access Documentation​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#access-documentation","content":"For tutorials that include written documentation, users can click to open the documentation in a new browser tab, providing detailed step-by-step instructions. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#main-view","content":"The tutorials panel displays a grid of tutorial cards (3 columns on desktop, responsive on mobile). Each card shows: Preview image or looping video (160px height on home panel, 220px height on tutorials panel)Tutorial titleBrief description (truncated to 2 lines)Hover interactions that reveal video controls and expand icons At the top of the panel: Panel title: &quot;Tutorials&quot;Search bar with placeholder text &quot;Search Tutorials&quot;Loading spinner during data fetchError/empty state alerts when appropriate ","version":"Next","tagName":"h3"},{"title":"Dialogs & Modals​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#dialogs--modals","content":"Tutorial Video Dialog Purpose: Provides full-screen viewing of tutorial videosFields: Video player with controlsTitle overlay (shown on hover)Description overlay (shown on hover) Actions: Play/pause videoClose dialog (X button in top-right)Hover to reveal title and description overlay ","version":"Next","tagName":"h3"},{"title":"Tables & Data Grids​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#tables--data-grids","content":"No tables are used in this feature. Content is displayed as a grid of cards. ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#graphql-operations","content":"This feature does not use GraphQL operations. Tutorial data is fetched directly from an external CDN. External Data Source: URL: https://raw.githubusercontent.com/devsentient/cdn/main/tutorials-list.jsonFormat: JSON array of tutorial objectsFetched via standard HTTP fetch API Tutorial Object Schema: { tutorial_id: string; title: string; description: string; video_url?: string; // Optional video content preview_image_url: string; // Thumbnail/preview image usage_docs_url?: string; // Link to documentation component?: string; // Associated dashboard component }  ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#component-structure","content":"Main Component: /components/Tutorials/TutorialPanel.tsxCard Component: /components/Tutorials/TutorialCard.tsxVideo Dialog: /components/Tutorials/TutorialVideoDialog.tsxComponent Tutorial: /components/Tutorials/TutorialComponent.tsx (for inline tutorials)Hook: /hooks/useFetchTutorial.tsSearch Integration: /components/Search/TutorialsSearchResults.tsx ","version":"Next","tagName":"h3"},{"title":"Search Implementation​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#search-implementation","content":"The tutorials panel uses two search mechanisms: Panel Search: Simple text filtering on title and description (case-insensitive substring match)Global Search: Fuse.js fuzzy search integration with 0.6 threshold for platform-wide search results ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"Finding a Tutorial​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#finding-a-tutorial","content":"Navigate to Tutorials panel via sidebar menuBrowse the grid of available tutorialsUse the search bar to filter by keywordsClick on a tutorial card to view ","version":"Next","tagName":"h3"},{"title":"Watching a Tutorial Video​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#watching-a-tutorial-video","content":"Find a tutorial with video content (indicated by video preview)Hover over the card to see video preview autoplayClick the card or expand icon to open full-screen video dialogVideo plays with controls availableHover over video to see title/description overlayClose dialog when finished ","version":"Next","tagName":"h3"},{"title":"Accessing Documentation​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#accessing-documentation","content":"Find a tutorial (typically without video content)Click the tutorial cardDocumentation opens in a new browser tabFollow step-by-step instructions in the documentation ","version":"Next","tagName":"h3"},{"title":"Home Panel Integration​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#home-panel-integration","content":"The home panel displays a filtered subset of tutorialsOnly tutorials with documentation links are shown (filtered by usage_docs_url)Search bar is disabled on home panelCards are smaller (160px vs 220px height)Provides quick access to getting started resources ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#related-features","content":"Home Panel - Displays featured tutorials in the &quot;Get Started&quot; sectionStack Components - Many tutorials relate to installed stack componentsComponent-specific tutorials - Inline tutorials shown within specific dashboard panels ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"Tutorials","url":"/developer-docs/features/tutorials#notes--tips","content":"Caching: Tutorial data is fetched fresh on each panel load with a 250ms artificial delay for smooth loading transitionsVideo Performance: Videos on cards use autoPlay and muted attributes to play automatically on hover without soundResponsive Design: Grid adjusts from 3 columns (desktop) to 1 column (mobile) for optimal viewingError Handling: If tutorial data fails to load, users see a friendly error message instead of broken UIEmpty States: When no tutorials match a search query, an informative message is displayedComponent Integration: Tutorials can be associated with specific dashboard components via the component field for contextual helpExternal Content: All tutorial content is managed externally through the CDN, allowing updates without code deploymentsSearch Integration: Tutorials are searchable through both the panel-specific search bar and the global platform search (Cmd+K) ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/developer-docs/intro","content":"","keywords":"","version":"Next"},{"title":"Platform Features​","type":1,"pageTitle":"Overview","url":"/developer-docs/intro#platform-features","content":"The Platform Features section provides detailed documentation for all 29 Shakudo dashboard panels and features, including: Core Features: Home, Sessions, Jobs, Scheduled Jobs, ServicesData &amp; Infrastructure: Datalake, Persistent Volumes, Container Images, Git RepositoriesPlatform Management: Stack Components, Apps Catalogue, Environment Configs, PluginsSecurity &amp; Access: Users, Service Accounts, Secrets, Authorization Policies, Outbound Traffic AccessMonitoring &amp; Operations: Distributed Workloads Dashboard, SecOps, Alert TargetsAdvanced Features: Satellite Clusters, Traffic Shifting, Triton Inference Server, Cloud SQL ProxyIntegrations: Shakudo Sync, Data Stack Graph, Billing ProjectsLearning Resources: Tutorials Each feature document includes: Overview and key capabilitiesAccess requirements and navigationUser interface detailsTechnical implementation (GraphQL operations, components, data flow)Common workflows and usage examplesRelated features and cross-referencesTips and best practices ","version":"Next","tagName":"h2"},{"title":"User Management","type":0,"sectionRef":"#","url":"/developer-docs/features/users","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#overview","content":"The User Management panel provides Keycloak administrators with the ability to view and manage platform users. This feature displays comprehensive user information including usernames, names, emails, assigned groups, and roles. It enables administrators to maintain proper access control by managing user role assignments through integration with Keycloak's admin console. ","version":"Next","tagName":"h2"},{"title":"Access & Location​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#access--location","content":"Route: ?panel=usersNavigation: Admin → User ManagementAccess Requirements: keycloak-admin role (required)Only visible to users with Keycloak administrator privileges Feature Flags: None ","version":"Next","tagName":"h2"},{"title":"Key Capabilities​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#key-capabilities","content":"","version":"Next","tagName":"h2"},{"title":"View User Information​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#view-user-information","content":"Display a comprehensive list of all users in the Keycloak realm with their profile information, group memberships, and assigned roles. The table provides a searchable, paginated interface for easy navigation through the user base. ","version":"Next","tagName":"h3"},{"title":"Search Users​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#search-users","content":"Filter the user list by username using the built-in search functionality. The search performs real-time filtering on the username field, making it easy to locate specific users in large organizations. ","version":"Next","tagName":"h3"},{"title":"Manage User Roles​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#manage-user-roles","content":"Access Keycloak's role management interface directly from the dashboard to assign or modify roles for specific users. This action opens the Keycloak admin console in a new tab, providing full role management capabilities. ","version":"Next","tagName":"h3"},{"title":"User Interface​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#user-interface","content":"","version":"Next","tagName":"h2"},{"title":"Main View​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#main-view","content":"The main panel displays a data grid table with the following information: Username: The user's login identifierName: The user's full name (first and last name combined)Email: The user's email addressGroups: Visual chips displaying all groups the user belongs toRoles: Visual chips displaying all roles assigned to the userActions: A dropdown menu with available management actions The table includes: Client-side pagination (10 users per page)Search toolbar with filter iconSortable columnsPinned actions column on the right side ","version":"Next","tagName":"h3"},{"title":"Search Toolbar​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#search-toolbar","content":"Located at the top of the data grid, the search toolbar includes: Filter icon indicatorText input field with placeholder &quot;Enter property value&quot;Clear button (X icon) to reset searchReal-time filtering as you type ","version":"Next","tagName":"h3"},{"title":"Actions Menu​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#actions-menu","content":"Each user row includes an &quot;Actions&quot; dropdown button that provides: Add Role: Opens the Keycloak admin console role-mapping page for the specific user in a new browser tab Only enabled for users with manage-users and query-users rolesDisabled with tooltip &quot;Keycloak Admin or owner only&quot; for unauthorized users ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#technical-details","content":"","version":"Next","tagName":"h2"},{"title":"GraphQL Operations​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#graphql-operations","content":"Queries: getKeycloakUsers - Retrieves all user information from Keycloak including usernames, names, emails, groups, and roles Mutations:None - Role management is performed through Keycloak's native admin interface Subscriptions:None ","version":"Next","tagName":"h3"},{"title":"Component Structure​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#component-structure","content":"Main Component: components/KeycloakUsers/KeycloakUsersPanel.tsxTable Component: components/KeycloakUsers/KeycloakUsersTable.tsxActions Menu: components/KeycloakUsers/KeycloakUsersActionMenu.tsxHook: hooks/useKeycloakUsers.ts ","version":"Next","tagName":"h3"},{"title":"Access Control​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#access-control","content":"The panel uses multiple layers of access control: Panel visibility: Only shown when isKeycloakAdmin is true (checked in PanelWindow.tsx)Navigation visibility: Only appears in Admin navigation group when user has keycloak-admin role (checked in PanelNav.tsx)Action permissions: Role management actions require both manage-users and query-users roles, stored in KeycloakRBACContext as userRBAC[2] ","version":"Next","tagName":"h3"},{"title":"Keycloak Integration​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#keycloak-integration","content":"The component integrates with Keycloak through: Authentication: Uses Keycloak token for GraphQL API authenticationData Fetching: Queries Keycloak user data via GraphQL backendRole Management: Deep links to Keycloak admin console for role assignmentsContext: Leverages KeycloakContext for protocol, domain, and realm information ","version":"Next","tagName":"h3"},{"title":"URL Construction​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#url-construction","content":"Role management links are constructed using the format: {protocol}://{kcDomain}/auth/admin/{realm}/console/#/{realm}/users/{userId}/role-mapping  ","version":"Next","tagName":"h3"},{"title":"Common Workflows​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#common-workflows","content":"","version":"Next","tagName":"h2"},{"title":"View All Users​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#view-all-users","content":"Navigate to Admin → User Management from the sidebarThe panel loads and displays all users in a paginated tableScroll through pages to view additional users (10 per page) ","version":"Next","tagName":"h3"},{"title":"Search for a Specific User​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#search-for-a-specific-user","content":"Open the User Management panelClick in the search field at the top of the tableType the username you're looking forThe table filters in real-time to show matching usersClick the X button to clear the search and show all users ","version":"Next","tagName":"h3"},{"title":"Add Roles to a User​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#add-roles-to-a-user","content":"Locate the user in the table (use search if needed)Click the &quot;Actions&quot; dropdown button in the user's rowSelect &quot;Add Role&quot; from the menuKeycloak admin console opens in a new tab at the role-mapping pageUse Keycloak's interface to assign or remove rolesReturn to the dashboard (changes will be reflected on next data refresh) ","version":"Next","tagName":"h3"},{"title":"Related Features​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#related-features","content":"Billing Projects - Manage project-based resource allocationAuthorization Policies - Configure service-level access policiesService Accounts - Manage non-human authentication credentials ","version":"Next","tagName":"h2"},{"title":"Notes & Tips​","type":1,"pageTitle":"User Management","url":"/developer-docs/features/users#notes--tips","content":"The search functionality only filters by username, not by name, email, groups, or rolesRole assignments are performed through Keycloak's native interface, not directly in the dashboardThe panel automatically refetches user data when openedUsers with keycloak-admin role can see this panel, but need additional manage-users and query-users roles to perform role management actionsThe data grid shows roles and groups as visual chips for easy scanningAll role management actions open in a new browser tab to maintain dashboard stateClient-side pagination means all user data is loaded at once (suitable for moderate user bases) ","version":"Next","tagName":"h2"},{"title":"Installation","type":0,"sectionRef":"#","url":"/Getting started/installation","content":"Installation Shakudo is deployed through several helm charts on your Kubernetes cluster. It can be deployed on GCP, AWS, Azure, Oracle, or on VMs or Baremetal. To deploy Shakudo to your cluster, Shakudo will provide a key and a guide for installation, or terraform scripts for your cloud environment. Shakudo deployments include Kubernetes cluster setup, although we can use existing clusters that meet specific requirements. Shakudo typically works closely with the customer team to deploy the platform on their infrastructure. The installation process generally consists of these steps: Creating a Kubernetes cluster on your cloud or machineActivating your key to pull Shakudo container imagesPulling and installing the Shakudo helm chartSetting up and mapping users/rolesSetting up optional stack components","keywords":"","version":"Next"},{"title":"Example: Setting up Single Sign-On (SSO) from Google to Shakudo Keycloak","type":0,"sectionRef":"#","url":"/Getting started/Sign in with external provider/example-1-google","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Google to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-1-google#introduction","content":"Single Sign-On allows users to authenticate once and gain access to multiple applications without logging in separately. Integrating Google with Shakudo Keycloak can enhance user experience by allowing them to authenticate using their Google accounts. ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Google to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-1-google#prerequisites","content":"Access to a Google accountAdmin access to your Shakudo Keycloak instance ","version":"Next","tagName":"h2"},{"title":"Step 1: Configuring Shakudo Keycloak​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Google to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-1-google#step-1-configuring-shakudo-keycloak","content":"Log into your Shakudo Keycloak admin console.Select the appropriate realm.Navigate to Identity Providers and select Google from the list.  Copy the redirect URI, you will need this in the next step.   ","version":"Next","tagName":"h2"},{"title":"Step 2: Setting up a Google Project​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Google to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-1-google#step-2-setting-up-a-google-project","content":"Go to the Google Cloud Console.Select or create a new project.Enable the Google+ API and OAuth Consent Screen. ","version":"Next","tagName":"h2"},{"title":"Step 3: Configuring OAuth Consent Screen​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Google to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-1-google#step-3-configuring-oauth-consent-screen","content":"In the Google Cloud Console, navigate to the OAuth Consent Screen tab.Select the user type (External/Internal) and provide required information like App name, email, etc.Save and move to the next step. ","version":"Next","tagName":"h2"},{"title":"Step 4: Setting up Credentials​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Google to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-1-google#step-4-setting-up-credentials","content":"Under the Credentials tab, click on Create Credentials and select OAuth client ID.Configure the application type (usually Web Application).Enter your Authorized redirect URIs: Obtain this from your Shakudo Keycloak setup panel. Typically, it looks like https://&lt;keycloak-domain&gt;/auth/realms/&lt;realm-name&gt;/broker/google/endpoint. Click Create to obtain the client ID and client secret.  ","version":"Next","tagName":"h2"},{"title":"Step 5: Configuring Shakudo Keycloak​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Google to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-1-google#step-5-configuring-shakudo-keycloak","content":" Enter the client ID and secret obtained from the Google Developer Console.Save your settings. ","version":"Next","tagName":"h2"},{"title":"Step 6: Testing the Setup​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Google to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-1-google#step-6-testing-the-setup","content":"Navigate to the Login page provided by Shakudo Keycloak.Choose Sign in with Google.Follow the prompts to authenticate via Google. ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Google to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-1-google#conclusion","content":"Congratulations! You've successfully configured SSO from Google to Shakudo Keycloak. Your users can now authenticate using their Google accounts to access the applications managed by your Shakudo Keycloak instance. ","version":"Next","tagName":"h2"},{"title":"Example: Setting up Single Sign-On (SSO) from Microsoft Azure AD to Shakudo Keycloak","type":0,"sectionRef":"#","url":"/Getting started/Sign in with external provider/example-2-microsoft","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Microsoft Azure AD to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-2-microsoft#introduction","content":"Single Sign-On allows users to authenticate once and gain access to multiple applications without logging in separately. Integrating Microsoft with Shakudo Keycloak can enhance user experience by allowing them to authenticate using their Microsoft accounts. ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Microsoft Azure AD to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-2-microsoft#prerequisites","content":"Access to a Microsoft accountAdmin access to your Shakudo Keycloak instance ","version":"Next","tagName":"h2"},{"title":"Step 1: Configuring Shakudo Keycloak​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Microsoft Azure AD to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-2-microsoft#step-1-configuring-shakudo-keycloak","content":"Log into your Shakudo Keycloak admin console.Select the appropriate realm.Navigate to Identity Providers and select Microsoft from the list.  Copy the redirect URI, you will need this in the next step.   ","version":"Next","tagName":"h2"},{"title":"Step 2: Setting up a Microsoft Project​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Microsoft Azure AD to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-2-microsoft#step-2-setting-up-a-microsoft-project","content":"Go to the Azure Portal.Select or create a new Azure AD application.Enable the Microsoft Graph API and OAuth Consent Screen. ","version":"Next","tagName":"h2"},{"title":"Step 3: Configuring OAuth Consent Screen​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Microsoft Azure AD to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-2-microsoft#step-3-configuring-oauth-consent-screen","content":"In the Azure Portal, navigate to the OAuth Consent Screen tab.Select the user type (External/Internal) and provide required information like App name, email, etc.Save and move to the next step. ","version":"Next","tagName":"h2"},{"title":"Step 4: Setting up Credentials​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Microsoft Azure AD to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-2-microsoft#step-4-setting-up-credentials","content":"Under the Certificates &amp; Secrets tab, click on New client secret.Register the application type (Web Application). Enter your Authorized redirect URIs: Obtain this from your Shakudo Keycloak setup panel. Typically, it looks like https://&lt;keycloak-domain&gt;/auth/realms/&lt;realm-name&gt;/broker/Microsoft/endpoint. Click Create to obtain the client ID and client secret.  ","version":"Next","tagName":"h2"},{"title":"Step 5: Configuring Shakudo Keycloak​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Microsoft Azure AD to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-2-microsoft#step-5-configuring-shakudo-keycloak","content":" Enter the client ID and secret obtained from the Azure Portal.(Optional) If you wish to use multi-tenants login leave the Tenant ID field empty.Save your settings. ","version":"Next","tagName":"h2"},{"title":"Step 6: Testing the Setup​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Microsoft Azure AD to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-2-microsoft#step-6-testing-the-setup","content":"Navigate to the Login page provided by Shakudo Keycloak.Choose Sign in with Microsoft.Follow the prompts to authenticate via Microsoft. ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Example: Setting up Single Sign-On (SSO) from Microsoft Azure AD to Shakudo Keycloak","url":"/Getting started/Sign in with external provider/example-2-microsoft#conclusion","content":"Congratulations! You've successfully configured SSO from Microsoft to Shakudo Keycloak. Your users can now authenticate using their Microsoft accounts to access the applications managed by your Shakudo Keycloak instance. ","version":"Next","tagName":"h2"},{"title":"IDP Mapper Setup in Shakudo Keycloak","type":0,"sectionRef":"#","url":"/Getting started/Sign in with external provider/idp-mapper-setup-doc","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"IDP Mapper Setup in Shakudo Keycloak","url":"/Getting started/Sign in with external provider/idp-mapper-setup-doc#prerequisites","content":"Before proceeding with this setup, ensure that you have: A running Shakudo Keycloak instance.Administrative access to the Shakudo Keycloak Admin Console. ","version":"Next","tagName":"h2"},{"title":"Steps to Set Up IDP Mapper in Shakudo Keycloak​","type":1,"pageTitle":"IDP Mapper Setup in Shakudo Keycloak","url":"/Getting started/Sign in with external provider/idp-mapper-setup-doc#steps-to-set-up-idp-mapper-in-shakudo-keycloak","content":"Log in to the Shakudo Keycloak Admin Console Open your Shakudo Keycloak Admin Console.Enter your administrator credentials to access the dashboard. Navigate to Identity Providers In the left-hand navigation panel, click on Identity Providers to view the list of configured identity providers. Select Your Identity Provider Click on the name of the identity provider you wish to configure. This will take you to the settings for that particular provider. Add a Mapper Go to the Mappers tab.Click Add to create a new mapper.Fill in the required fields such as Name, Mapper Type, Attribute Name, etc.Configure the other settings, including claim mapping, attribute mapping, etc. Save the Configuration After configuring the settings, click Save to apply changes. Test the Configuration Ensure that the mapper works as expected by testing it through a login flow. Reference these steps during the configuration process to ensure correct setup and integration across your services. ","version":"Next","tagName":"h2"},{"title":"Example: Azure AD Integration​","type":1,"pageTitle":"IDP Mapper Setup in Shakudo Keycloak","url":"/Getting started/Sign in with external provider/idp-mapper-setup-doc#example-azure-ad-integration","content":"This section provides a step-by-step example for configuring Azure AD to correctly import preferred_username as both email and username in Shakudo Keycloak using IDP mappers. ","version":"Next","tagName":"h2"},{"title":"Step-by-Step Configuration​","type":1,"pageTitle":"IDP Mapper Setup in Shakudo Keycloak","url":"/Getting started/Sign in with external provider/idp-mapper-setup-doc#step-by-step-configuration","content":"Access Azure AD Configuration: Log in to the Azure portal.Navigate to Azure Active Directory and select App registrations. Shakudo Keycloak IDP Mapper Setup: Log in to the Shakudo Keycloak Admin Console.Navigate to Identity Providers and select Azure AD from your configured providers.Go to the Mappers tab and click Add. Create Mapper for preferred_username: Set the Name to &quot;Preferred Username to Email&quot;.Choose Mapper Type as Attribute Importer.Set Attribute Name to preferred_username.Map this attribute to the email and username fields in Shakudo Keycloak by setting User Attribute Name to email, then create another mapper setting it to username. Save the Configuration: Click Save for both mappers to save your configurations. Test the Integration: Perform a test login using Azure AD credentials.Verify that the preferred_username is correctly mapped in Shakudo Keycloak as both email and username. By following these steps, you can ensure that Azure AD is configured to pass the preferred_username correctly to Shakudo Keycloak, where it will be mapped to the email and username fields for user accounts. ","version":"Next","tagName":"h3"},{"title":"Use Shakudo Keycloak with an External OIDC Provider","type":0,"sectionRef":"#","url":"/Getting started/Sign in with external provider/oidcSetup","content":"","keywords":"","version":"Next"},{"title":"Introduction to OpenID Connect (OIDC)​","type":1,"pageTitle":"Use Shakudo Keycloak with an External OIDC Provider","url":"/Getting started/Sign in with external provider/oidcSetup#introduction-to-openid-connect-oidc","content":"OpenID Connect (OIDC) is an identity layer on top of the OAuth 2.0 protocol. It enables clients to verify the identity of an end-user based on the authentication performed by an Authorization Server, as well as to obtain basic profile information about the end-user in an interoperable and REST-like manner. This guide will assist you in configuring Shakudo Keycloak to use an external OIDC Identity Provider (IDP). This guide will allow you to log into Shakudo using external IDPs such as Microsoft or Google through Shakudo Keycloak. ","version":"Next","tagName":"h3"},{"title":"Information Table​","type":1,"pageTitle":"Use Shakudo Keycloak with an External OIDC Provider","url":"/Getting started/Sign in with external provider/oidcSetup#information-table","content":"Information\tExample Value\tSource\tHow to ObtainRedirect URL\thttps://your-app.com/callback\tYour Application\tProvided by your application. Typically found in the application's OAuth settings. Client ID\texternal-idp-client-id\tExternal IDP\tProvided by your external identity provider when you register the application/client. Secret ID\txyz123secureVeryLongSecret\tExternal IDP\tProvided by your external identity provider in the client's security credentials. OIDC Discovery Endpoint\thttps://idp.example.com/.well-known/openid-configuration\tExternal IDP\tCheck your external identity provider's documentation or admin console. ","version":"Next","tagName":"h3"},{"title":"Prerequisites for External OIDC Provider Setup in Shakudo Keycloak​","type":1,"pageTitle":"Use Shakudo Keycloak with an External OIDC Provider","url":"/Getting started/Sign in with external provider/oidcSetup#prerequisites-for-external-oidc-provider-setup-in-shakudo-keycloak","content":"Shakudo Keycloak Server Installation Ensure that you have a running instance of Shakudo Keycloak server.You can visit the Shakudo Keycloak admin console with &lt;Domain&gt;/auth/ Administrative Access Access to the Shakudo Keycloak Admin Console with administrative privileges to manage external identity providers.Ensure that you have the necessary credentials for accessing your external IDP. Knowledge of OIDC Protocol Familiarity with OpenID Connect (OIDC) protocol and its concepts like tokens, scopes, and claims. Configuring Shakudo Keycloak with an External OIDC Provider To configure Shakudo Keycloak to use an external OpenID Connect (OIDC) Identity Provider, follow these steps: Log in to Shakudo Keycloak Admin Console Open your Shakudo Keycloak Admin Console in your web browser.Enter your administrator credentials to log in. Add an External Identity Provider Go to the &quot;Identity Providers&quot; section in your realm settings.Click on &quot;Create&quot; and choose &quot;OpenID Connect v1.0&quot; from the list of available providers. Enter the External IDP Details Alias: Provide a unique alias for your identity provider.Client ID: Enter the client ID obtained from your external IDP.Client Secret: Provide the client secret associated with your application on the external IDP.Discovery URL: Enter the OIDC discovery URL for your external identity provider.Save the configuration. Advanced Settings (Optional) Configure scopes and claims mapping if needed, based on the specific requirements of your external IDP.Adjust token settings and user attribute synchronization as required.  For more information on how to set up IDP mappers in Shakudo Keycloak, please refer to the IDP Mapper Setup. ","version":"Next","tagName":"h3"},{"title":"SAML SSO Setup for Shakudo Hyperplane Platform","type":0,"sectionRef":"#","url":"/Getting started/Sign in with external provider/SamlSetUp","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#prerequisites","content":"Admin access to Azure Active DirectoryAdmin access to Keycloak instanceKeycloak server reachable by AAD (public DNS or reverse proxy)  ","version":"Next","tagName":"h2"},{"title":"Step 1: Create Enterprise Application in Azure AD​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#step-1-create-enterprise-application-in-azure-ad","content":"Log in to Azure Portal.Navigate to Azure Active Directory &gt; Enterprise Applications.Click + New Application &gt; Create your own application.Name it something like Shakudo Hyperplane SSO, select Integrate any other application you don't find in the gallery (Non-gallery), and click Create.  ","version":"Next","tagName":"h2"},{"title":"Step 2: Configure SAML in Azure AD​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#step-2-configure-saml-in-azure-ad","content":"In the created application, go to Single sign-on &gt; SAML. Under Basic SAML Configuration, click Edit and fill in: Identifier (Entity ID): https://&lt;your-keycloak-domain&gt;/auth/realms/Hyperplane Reply URL (Assertion Consumer Service URL): https://&lt;your-keycloak-domain&gt;/auth/realms/Hyperplane/broker/azuread/endpoint &gt; Some of the app require you to put https://&lt;your-keycloak-domain&gt; so the Application will redirect you into correct entry page of Shakudo platform Click Save.  ","version":"Next","tagName":"h2"},{"title":"Step 3: Download SAML Metadata​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#step-3-download-saml-metadata","content":"In the Azure AD SAML configuration page, download the Federation Metadata XML, or copy the URL.Save it locally for import into Keycloak.  ","version":"Next","tagName":"h2"},{"title":"Step 4: Configure Keycloak Identity Provider​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#step-4-configure-keycloak-identity-provider","content":"Log in to Keycloak Admin Console (https://&lt;your-keycloak-domain&gt;/auth/admin).Select the realm Hyperplane.Navigate to Identity Providers &gt; Add provider &gt; SAML v2.0.Set: Alias: azureadImport from URL / XML: Upload the XML metadata from Azure, or paste the XML url from the Step 3.First Login Flow: first broker loginSync Mode: force Click Save.  ","version":"Next","tagName":"h2"},{"title":"Step 5: Configure Attribute Mappers in Keycloak​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#step-5-configure-attribute-mappers-in-keycloak","content":"In the Identity Provider (azuread) configuration page, scroll to the Mappers tab.Click Create to add each of the following mappers: ","version":"Next","tagName":"h2"},{"title":"Mapper: username​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#mapper-username","content":"Name: usernameMapper Type: Attribute ImporterAttribute Name: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddressUser Attribute Name: username ","version":"Next","tagName":"h3"},{"title":"Mapper: email​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#mapper-email","content":"Name: emailMapper Type: Attribute ImporterAttribute Name: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddressUser Attribute Name: email ","version":"Next","tagName":"h3"},{"title":"Mapper: given name​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#mapper-given-name","content":"Name: givenNameMapper Type: Attribute ImporterAttribute Name: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givennameUser Attribute Name: firstName ","version":"Next","tagName":"h3"},{"title":"Mapper: surname​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#mapper-surname","content":"Name: surnameMapper Type: Attribute ImporterAttribute Name: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/surnameUser Attribute Name: lastName ","version":"Next","tagName":"h3"},{"title":"Optional: groups (if configured in Azure AD)​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#optional-groups-if-configured-in-azure-ad","content":"Name: groupsMapper Type: Attribute ImporterAttribute Name: http://schemas.xmlsoap.org/claims/GroupUser Attribute Name: groups Click Save for each.  ","version":"Next","tagName":"h3"},{"title":"Step 6: Test the SSO​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#step-6-test-the-sso","content":"Go to your Shakudo platform.You should be redirected to Azure AD for login.After login, you’ll be redirected back to the platform and logged in.  ","version":"Next","tagName":"h2"},{"title":"Notes​","type":1,"pageTitle":"SAML SSO Setup for Shakudo Hyperplane Platform","url":"/Getting started/Sign in with external provider/SamlSetUp#notes","content":"If group/role mapping is needed, ensure Azure AD is sending the correct claims, and use mappers in Keycloak.Ensure the clock time between Keycloak and Azure AD is synchronized to avoid token issues. Enjoy seamless identity federation with Azure AD and the Shakudo Hyperplane platform 🚀 ","version":"Next","tagName":"h2"},{"title":"What is Shakudo","type":0,"sectionRef":"#","url":"/home","content":"What is Shakudo Shakudo is the data and AI operating system on your VPC. Shakudo creates compatibility across the best-of-breed data tools for a more reliable, performant, and cost effective data and AI operating system. On Shakudo, data teams can choose and mix and match best-of-breed software and try out new emerging tools without DevOps overhead. On Shakudo, the workflow is simplified with the Shakudo components: Sessions are development environments on Shakudo. They come with pre-configured resources, environment variables, mounted credentials, network connections, and connections to databases, so users can start development without any additional setup. Jobs run code -- from a single step bash script to a multi-step pipeline -- from beginning to end. Users can connect a git repository and run pushed code, or deploy custom Docker images. Jobs can be triggered on demand, on a schedule, on with Kafka. Microservices run frontend or backend applications with frameworks like Flask, fastAPI, Django, NextJS, React, and more. Similar to jobs, you can use code in a linked git repositories, or simply use a pre-built Docker image. A Microservice exposes an endpoint, which can be a dashboard, a website or an API endpoint. Shakudo Stack Components are best-of-breed, production-ready data tools and frameworks preconfigured to work seamlessly on the Shakudo Platform. Shakudo manages the networking, credentials, SSO, security, and interconnectivity between components so you can install and use Stack Components with one click. Shakudo adds new integrations every day. Visit our integration page to see the latest list. If you can't find the tool you are looking for, please send us an integration request.","keywords":"","version":"Next"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/legacy/faq","content":"","keywords":"","version":"Next"},{"title":"Out of memory​","type":1,"pageTitle":"FAQ","url":"/legacy/faq#out-of-memory","content":"Out of memory errors may occur if one of your chunks/partitions or persisted data is too large to fit into RAM. OOM errors can also occur if one of your operations requires more RAM than what is available. Increasing the available memory on your workers can usually solve this issue. OOMs show up as any of the following: killed worker, key error, canceled, http error. ","version":"Next","tagName":"h2"},{"title":"Module not found​","type":1,"pageTitle":"FAQ","url":"/legacy/faq#module-not-found","content":"When add code to your pipeline, some references to .py modules may not be found. Ensure you are importing from the correct directory (relative to the top level of your repository). ","version":"Next","tagName":"h2"},{"title":"Timeouts when spinning up Dask workers​","type":1,"pageTitle":"FAQ","url":"/legacy/faq#timeouts-when-spinning-up-dask-workers","content":"You may receive an error if the Dask nodes are taking too long to scale up (due to resource availability, resource limits for your project, etc.). You can retry by re-running your cell or script when you see this error. ","version":"Next","tagName":"h2"},{"title":"Timeout when using Sessions​","type":1,"pageTitle":"FAQ","url":"/legacy/faq#timeout-when-using-sessions","content":"In your browser Session, if you get a popup that says Directory not found it means your Session has timed out. ","version":"Next","tagName":"h2"},{"title":"Preempted nodes​","type":1,"pageTitle":"FAQ","url":"/legacy/faq#preempted-nodes","content":"If a node is preempted, you will see a canceled error. For this reason, we do not recommend long jobs, but rather split your long-running functions into multiple jobs to avoid http error or canceled error. ","version":"Next","tagName":"h2"},{"title":"Public Logo CDN Setup","type":0,"sectionRef":"#","url":"/legacy/public-logo-cdn-setup","content":"","keywords":"","version":"Next"},{"title":"Why This Matters​","type":1,"pageTitle":"Public Logo CDN Setup","url":"/legacy/public-logo-cdn-setup#why-this-matters","content":"Stack Component Icon Issue: The Stack Component page currently cannot display icons for newly uploaded SVG or PNG files.Ephemeral URLs: GitHub no longer generates direct public URLs for images uploaded in comments or issues. Now, it generates links like https://private-user-images.githubusercontent.com/..., which are restricted and may expire.Inconsistent Access: These URLs are not reliable for documentation or production use since they can change or stop working over time. To ensure consistency and reliability, we host public logos directly in our repository and access them via GitHub's raw CDN. ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Public Logo CDN Setup","url":"/legacy/public-logo-cdn-setup#prerequisites","content":"Write access to the CDN repository (no fork needed).Git installed and configured with your GitHub credentials. ","version":"Next","tagName":"h2"},{"title":"Icon Requirements​","type":1,"pageTitle":"Public Logo CDN Setup","url":"/legacy/public-logo-cdn-setup#icon-requirements","content":"File Format: Prefer SVG for sharp, scalable logos. PNG files also work.File Naming: Use lowercase letters with hyphens (e.g., my-company-logo.svg) for consistency.Updates: When updating the logo, the URL remains unchanged. Simply open a new PR to overwrite the existing file. ","version":"Next","tagName":"h2"},{"title":"Uploading a Public Logo​","type":1,"pageTitle":"Public Logo CDN Setup","url":"/legacy/public-logo-cdn-setup#uploading-a-public-logo","content":"Follow the steps below to upload a logo and use it as a CDN asset. Clone the repository and create a feature branch git clone https://github.com/devsentient/cdn.git cd cdn git checkout -b add-my-logo Copy your logo into the repository cp ~/Downloads/my-logo.svg stack-component-logos/ Commit and push your changes git add stack-component-logos/my-logo.svg git commit -m &quot;Add my logo to stack-component-logos&quot; git push -u origin add-my-logo Open a pull request Go to https://github.com/devsentient/cdn/pulls and open a new PR from add-my-logo into main. Provide a clear title and description so reviewers understand the change. Merge and Publish Once your PR is merged, your logo will be publicly accessible at: https://github.com/devsentient/cdn/blob/main/stack-component-logos/my-logo.svg Get the Raw URL When you view the file on GitHub, you'll see a Raw button near the top right of the file preview. Clicking it will load the file's contents directly (with the browser URL updated): https://raw.githubusercontent.com/devsentient/cdn/main/stack-component-logos/my-logo.svg This is the URL you should use for embedding the image as a CDN asset.  ","version":"Next","tagName":"h2"},{"title":"Tips​","type":1,"pageTitle":"Public Logo CDN Setup","url":"/legacy/public-logo-cdn-setup#tips","content":"Prefer SVG for crisp, infinitely‑scalable logos. PNG also works.If you ever update the file, the URL stays the same—just overwrite the asset in a new PR.Keep filenames lowercase and hyphen‑separated for consistency (e.g., my-company-logo.svg). ","version":"Next","tagName":"h3"},{"title":"Quick Start","type":0,"sectionRef":"#","url":"/legacy/quickstart","content":"","keywords":"","version":"Next"},{"title":"1. Start a Session​","type":1,"pageTitle":"Quick Start","url":"/legacy/quickstart#1-start-a-session","content":"Sessions are pre-configured virtual machines (VMs) with connections to all the tools and resources necessary for development. To begin development, navigate to the Sessions tab on the dashboard and click the +Start a Session button. You will see a dialog window to start a session like the image below.  Image: Choose the image type to use in the session. To add new image to the dropdown permanently, you can add a new Environment Config under the admin settings. Please checkout EC for more details. In this example, we are going to use the Basic image in the dropdown. For more information on Session Types and other configurations check out the Guide on Sessions ImageURL: You can paste any image URL in the Image Url field. This will overwrite the Image above field that we have chosen and use the ImageURL instead. This is useful for quick testing, we need to make sure the image registry credentials are added to the shakudo environment. This is usually setup at installation time. If you'd like to add more image registry access, please contact your Kubernets cluster admin or Shakudo support. Timeout: Choose the idle timeout for the session. Idle timeout is defined as the number of seconds from which the session has been continuously idling. It's default to 15 minutes. Drive: Drive is the persistent volume that this session will use. Persistent volumes is a Kubernetes term, imaging it as a hard drive in a laptop. You can have multiple drives and manage your drives by clicking on the icon to the right of the Drive field. ","version":"Next","tagName":"h2"},{"title":"2. Access the Session​","type":1,"pageTitle":"Quick Start","url":"/legacy/quickstart#2-access-the-session","content":"Once the Session is ready, you'll see a Jupyterlab icon and a SSH icon. If your image has CodeServer, you will also see a VsCode icon. To see the different ways of accessing the session, checkout the Guide on Sessions. In this example, we'll use the Jupyterlab option.  ","version":"Next","tagName":"h2"},{"title":"3. Process with Pandas​","type":1,"pageTitle":"Quick Start","url":"/legacy/quickstart#3-process-with-pandas","content":"The dataset we are using is the flight dataset in a public AWS S3 bucket. The dataset has 22 CSV files and has a total size of 11 GB. The objective is to identify the airport with the most frequently delayed arriving flights. This can be achieved through a few simple Pandas DataFrame transformations. Your session should already be set up to connect to your internal data source if this is a local storage bucket. Let's first use the groupby and nlargest function in Pandas to benchmark. ## loop through 22 files and groupby one by one, on the 16vCPU 16 GB RAM node that the session is on from tqdm.notebook import tqdm results = [] for file in tqdm(files): df = pd.read_csv( f&quot;s3://{file}&quot;, encoding = &quot;ISO-8859-1&quot;, usecols = ['DepTime','FlightNum','DepDelay','Origin', 'Dest','Distance'] ) df['isDelayed'] = df.DepDelay.fillna(16) &gt; 15 df_delayed = df.groupby(['Origin','Dest']).isDelayed.mean() results.append(df_delayed) df_results = pd.concat(results).reset_index() df_results = df_results.groupby(['Origin','Dest']).mean() df_results = df_results[df_results.isDelayed==1].reset_index() print(f'found {len(df_results)} most delayed flights Origin and destination pairs')  The processing time of the above operation is 6 minutes and 26 seconds on a 16 vCPU 16GB RAM machine. There are many ways to speed things up, here we use distributed Dask. ","version":"Next","tagName":"h2"},{"title":"4. Spin up a Dask cluster​","type":1,"pageTitle":"Quick Start","url":"/legacy/quickstart#4-spin-up-a-dask-cluster","content":"Dask is a powerful distributed computing framework and can scale from multi-core local machine to large distributed clusters. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, Scikit-learn and NumPy, leading to a shallow learning curve. We can use the Shakudo package notebook_common to spin up a fully configured Dask cluster with preemptible nodes. You can specify the number of workers with argument num_workers or specify more specs to better fit the computation. Shakudo will automatically choose a cluster configuration for you and provides a Dask dashboard link to monitor progress. from hyperplane import notebook_common as nc num_workers = 2 ## number of worker nodes total_memory = 12 ## total memory size for the worker nodes in GB cors_per_worker = 15 ## total number of cores for the worker nodes nprocs = 3 ## number of processes for each worker node ram_gb_per_proc = total_memory/nprocs ## calculated memory size per processes in GB nthreads = int(cors_per_worker/nprocs) ## calculated number of threads per processes client, cluster = nc.initialize_cluster( num_workers = num_workers, nprocs = nprocs, nthreads = nthreads, ram_gb_per_proc = ram_gb_per_proc, cores_per_worker = cors_per_worker, node_selector = {} )  You will be able to see the spinning up logs of the Dask cluster and the link to the Dask dashboard. 👉 Shakudo Platform: selecting worker node pool 👉 Shakudo Platform: selecting scheduler node pool Creating scheduler pod on cluster. This may take some time. 👉 Shakudo Platform: spinning up a dask cluster with a scheduler as a standalone container. 👉 Shakudo Platform: In a few minutes you'll be able to access the dashboard at https://ds.hyperplane.dev/dask-cluster-e002f3d0-b18d-4027-81c5-bed613eb63a4/status 👉 Shakudo Platform: to get logs from all workers, do `cluster.get_logs()`  By clicking on the link above you'll see the unique distributed Dask dashboard. ","version":"Next","tagName":"h2"},{"title":"5. Process data in Dask​","type":1,"pageTitle":"Quick Start","url":"/legacy/quickstart#5-process-data-in-dask","content":"To run the code from step 3 on a Dask cluster, we just need to swap the Pandas API to the Dask API. Dask does lazy computation, the last line .compute() function triggers the actual computation. You can find information on the Dask concepts and Dask best practices page. Check out the Dask official documentation for more. import dask.dataframe as dd df = dd.read_csv([f&quot;s3://{file}&quot; for file in files], encoding = &quot;ISO-8859-1&quot;, usecols = ['DepTime','FlightNum','DepDelay','Origin', 'Dest','Distance'], dtype={'Distance': 'float64'} ) df['isDelayed'] = df.DepDelay.fillna(16) &gt; 15 df_delayed = df.groupby(['Origin','Dest']).isDelayed.mean() df_results = df_delayed[df_delayed==1].compute().reset_index() print(f'found {len(df_results)} most delayed flights Origin and destination pairs')  The Dask operation took 20 seconds using 2 remote 16 vCPU 16GB RAM Dask nodes. Comparing to Pandas, that's a ~20x speed up with only 2 extra nodes! After using Dask it's good practice to close the cluster after the computation. Add the line below at the end of your notebook: client.shutdown()  ","version":"Next","tagName":"h2"},{"title":"6. Creating the YAML file for deployment​","type":1,"pageTitle":"Quick Start","url":"/legacy/quickstart#6-creating-the-yaml-file-for-deployment","content":"Now the data processing notebook is developed and tested, to automatically run this notebook on a schedule as in most production setups, we can simply add a pipeline.yaml file to build a pipeline. To read more on pipeline YAML files please visit the create a pipeline job page. Open a text file on your browser Session by clicking on the blue + button on the top left of the side bar. Copy and paste the content below: pipeline: name: &quot;data prep pipeline&quot; tasks: - name: &quot;dask processing data&quot; type: &quot;jupyter notebook&quot; notebook_path: &quot;example_notebooks/doc_demo/quick_start/dask.ipynb&quot; # path to the script to run notebook_output_path: &quot;dask_output.ipynb&quot;  In this YAML file, we'll need to change the notebook_path to the actual path in your repository. This YAML is all we need to deploy the pipeline. Now let's commit the notebook and the YAML file to a GIT repository and run the job. Shakudo maintains live syncs of the repositories that are connect and make the code available for deployment immediately after code is synced. You can check the status of the git sync at Admin Settings.  ","version":"Next","tagName":"h2"},{"title":"7. Deploy to a pipeline job​","type":1,"pageTitle":"Quick Start","url":"/legacy/quickstart#7-deploy-to-a-pipeline-job","content":"Now we are one step away to put the job in production! To launch a pipeline job, we can go to the Shakudo Platform dashboard's Jobs tab and click Create.   In the job dialogue, we need to fill the following: Name: Job name or use the randomized name Image: Choose the image that we developed the code on to maintain environment consistency YAML path: Paste the path of the YAML file that we created above, from the root of the repository  Click the Create Immediate Job button on the top right corner to create the job. ","version":"Next","tagName":"h2"},{"title":"8. Check job status​","type":1,"pageTitle":"Quick Start","url":"/legacy/quickstart#8-check-job-status","content":"Once the job is submitted, you are redirected to the immediate jobs dashboard where our job is at the top of the job list. To see the live log of the job, click on the file button. You can pin the job to the top with the pin button, or checkout more functions in side the three dots.  Congratulations on developed and deployed your first Shakudo job pipeline with distributed computing! ","version":"Next","tagName":"h2"},{"title":"9. Additional Steps​","type":1,"pageTitle":"Quick Start","url":"/legacy/quickstart#9-additional-steps","content":"Shakudo Platform offers a variety of other functionalities for more advanced workflows. Some additional uses include the following: Run pipelines on a scheduleTriggering pipeline jobs programmatically ","version":"Next","tagName":"h2"},{"title":"Shakudo Cloud Shell","type":0,"sectionRef":"#","url":"/shakudo-platform-core/adminSettings/cloudshell","content":"","keywords":"","version":"Next"},{"title":"Installing Stack Components using their Shakudo Helm Charts​","type":1,"pageTitle":"Shakudo Cloud Shell","url":"/shakudo-platform-core/adminSettings/cloudshell#installing-stack-components-using-their-shakudo-helm-charts","content":"","version":"Next","tagName":"h2"},{"title":"What are Helm Charts?​","type":1,"pageTitle":"Shakudo Cloud Shell","url":"/shakudo-platform-core/adminSettings/cloudshell#what-are-helm-charts","content":"Helm is a package manager for Kubernetes that simplifies the deployment and management of applications. A Helm chart is a collection of YAML templates and configurations that describe a set of Kubernetes resources required to deploy an application. ","version":"Next","tagName":"h3"},{"title":"Tools and Environment​","type":1,"pageTitle":"Shakudo Cloud Shell","url":"/shakudo-platform-core/adminSettings/cloudshell#tools-and-environment","content":"You will be using the Shakudo Cloud Shell, which comes pre-installed with Helm, Kubernetes CLI (kubectl), and other essential tools. Since permissions are already configured, you can directly install charts from the Shakudo registry. ","version":"Next","tagName":"h3"},{"title":"Step-by-Step Guide of installing a Stack Component​","type":1,"pageTitle":"Shakudo Cloud Shell","url":"/shakudo-platform-core/adminSettings/cloudshell#step-by-step-guide-of-installing-a-stack-component","content":"1. Dry Run the Installation (Helm --dry-run=server)​ Before performing the actual installation, it’s a good practice to execute a dry run to ensure that the command will succeed without errors: helm upgrade --install n8n oci://us-docker.pkg.dev/shakudo-417723/charts/n8n --namespace hyperplane-n8n-test --version 1.2.4 --create-namespace --dry-run=server --debug  This will simulate the installation and output what changes will be made, helping you verify that the installation parameters are correct.  2. Installing the n8n Stack Component (Example)​ Let’s assume you want to install the n8n stack component, version 1.2.4, in the hyperplane-n8n-test namespace. Run the following command to install or upgrade the n8n chart: helm upgrade --install n8n oci://us-docker.pkg.dev/shakudo-417723/charts/n8n --namespace hyperplane-n8n-test --version 1.2.4 --create-namespace   Explanation: upgrade --install: Ensures that if the release does not exist, it will be installed; otherwise, it will be upgraded.n8n: This is the release name you assign. You can choose any meaningful name.oci://us-docker.pkg.dev/shakudo-417723/charts/n8n: Refers to the n8n Helm chart stored in the Shakudo registry.--namespace hyperplane-n8n-test: Specifies the Kubernetes namespace where the component will be installed.--version 1.2.4: Ensures that version 1.2.4 of the n8n chart is installed.--create-namespace: Creates the namespace if it does not already exist. 3. Verifying Installation​ After the installation completes, verify the deployed resources: List Helm releases: helm list --namespace hyperplane-n8n-test   Check Kubernetes resources: kubectl get all --namespace hyperplane-n8n-test  This will display all the resources associated with the n8n stack component, such as pods, services, and deployments.  4. Customizing Installation with valuesOverride.yaml​ If you need to customize the deployment, you can use a valuesOverride.yaml file to override default chart values. Here’s an example valuesOverride.yaml file for n8n: # valuesOverride.yaml - you may want to name this different as there may be multiple values file for different stack components resources: limits: cpu: &quot;5000m&quot; memory: &quot;12Gi&quot;  To install the chart with custom values, use: helm upgrade --install n8n oci://us-docker.pkg.dev/shakudo-417723/charts/n8n --namespace hyperplane-n8n-test -f valuesOverride.yaml  ","version":"Next","tagName":"h3"},{"title":"Important Notes​","type":1,"pageTitle":"Shakudo Cloud Shell","url":"/shakudo-platform-core/adminSettings/cloudshell#important-notes","content":"Permissions: Shakudo Cloud Shell is pre-configured with the necessary permissions, so you do not need to set up additional roles or service accounts.Namespace Management: Always specify the correct namespace during installation to avoid conflicts.Version Control: It’s important to specify the exact version of the Helm chart you want to deploy to ensure consistency across environments.Dry Run Before Install: Using --dry-run before actual installation helps catch potential issues in advance and ensures a smooth deployment. By following these steps, even users new to Kubernetes and Helm can successfully deploy and manage Stack Components using the Shakudo Cloud Shell. ","version":"Next","tagName":"h3"},{"title":"Shakudo API","type":0,"sectionRef":"#","url":"/legacy/shakudoApi","content":"","keywords":"","version":"Next"},{"title":"Notebook_Common​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#notebook_common","content":"notebook_common is part of the Shakudo Platform Hyperplane API that contains convenience functions for Dask and pipeline jobs. It contains functions to manage Dask clusters, pipeline jobs, and Slack messages, and GraphQL operations. ","version":"Next","tagName":"h2"},{"title":"Dask​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#dask","content":"Dask is a flexible open source distributed framework for parallel computing. It has similar APIs to NumPy and Pandas, is an ideal choice for parallelizing NumPy, Pandas and List based code. ","version":"Next","tagName":"h3"},{"title":"quickstart_dask()​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#quickstart_dask","content":"Use quickstart_dask to quickly spin up a Dask cluster using t-shirt sizes. Returns a tuple [Client, KubeCluster]. from hyperplane.notebook_common import quickstart_dask client, cluster = quickstart_dask( num_workers = 4, size = 'hyperplane-med-high-mem' )  Parameters Name\tType\tDescriptionnum_workers Required integer\tNumber of workers size Required string\tPre-configured worker pools   Pre-configured Worker pools The preconfigured worker pools are the following Name\tWorker Pool\tAllocatable cores\tAllocatable ramhyperplane-xs-high-mem\tPOOL_4_32\t3.5\t7.0 hyperplane-small\tPOOL_8_8\t7.0\t5.0 hyperplane-small-mid-mem\tPOOL_8_16\t7.5\t12.0 hyperplane-small-high-mem\tPOOL_8_64\t7.5\t58.0 hyperplane-med\tPOOL_16_16\t15.0\t12.0 hyperplane-med-mid-mem\tPOOL_16_32\t15.0\t27.0 hyperplane-med-high-mem\tPOOL_16_128\t15.0\t110.0 hyperplane-large\tPOOL_32_32\t28.0\t27.0 hyperplane-xxl-high-mem\tPOOL_96_768\t94.0\t675.0  ","version":"Next","tagName":"h3"},{"title":"initialize_dask_cluster()​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#initialize_dask_cluster","content":"Initialize a distributed DASK cluster. Returns a tuple [Client, KubeCluster]. You may use the returned client and cluster like any other dask cluster. from hyperplane.notebook_common import initialize_dask_cluster client, cluster = initialize_dask_cluster( num_workers:int=2, local_mode:bool=False, worker_spec_yaml:str=WORKER_SPEC_TEMPLATE_1_1, timeout:int=1200, nthreads:int=1, nprocs:int=15, ram_gb_per_proc:float=0.7, cores_per_worker:int=15, scheduler_deploy_mode:str=&quot;remote&quot;, dashboard_port:str=&quot;random&quot;, logging:str=&quot;quiet&quot; )   Parameters  Name\tType\tDescriptionnum_workers\tinteger\t(Default value: 2) Number of Dask worker nodes. local_mode\tbool\tWhether to use local cluster or distributed KubeCluster worker_spec_yaml\tstring\tA string YAML for cluster configs timeout\tinteger\tTime limit (seconds) for a scheduler to wait to connect before returning a timeout error nthreads\tinteger\tNumber of threads per worker in your cluster nprocs\tinteger\tNumber of processes per worker in your cluster ram_gb_per_proc\tfloat\tGB of Ram per process, per worker cores_per_worker\tinteger\tNumber of cores per worker scheduler_deploy_mode\tstring\tWhere to deploy the scheduler (remote in its own worker, or locally in jhub). Choose remote when the Dask graph dashboard_port\tstring\tChoose a port number for your dashboard, or leave as &quot;random&quot; to have a random port, which will not conflict logging\tstring\tLogging level for printouts when initializing. Available options are verbose or quiet. note The number of dask workers in the cluster will be the num_workers x num_procs. Shakudo platform will automatically choose the closest pool from the pre-configured node pool based on the combination of parameters specified. Example from hyperplane import notebook_common as nc client, cluster = nc.initialize_dask_cluster(num_workers=2)  from hyperplane import notebook_common as nc client, cluster = nc.initialize__dask_cluster( num_workers=2, nthreads=1, nprocs=15, ram_gb_per_proc=0.7, cores_per_worker=15 )   ","version":"Next","tagName":"h3"},{"title":"daskpool_candidates​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#daskpool_candidates","content":"Use daskpool_candidates when you'd like to access the list of available dask pools to choose from to spin up a Dask cluster. candidates = nc.daskpool_candidates candidates   ","version":"Next","tagName":"h3"},{"title":"get_dask_cluster()​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#get_dask_cluster","content":"Retrieve a Dask cluster. Use this function if there's a Dask cluster that's already spun up that you would like to connect. from hyperplane import notebook_common as nc client = nc.get_dask_cluster(&quot;dask_cluster_name&quot;) client   Parameters  Name\tType\tDescriptiondask_cluster_name\tstring\tName of Dask cluster To retrieve the Dask cluster name, navigate to the Ray &amp; Dask tab on the platform and click the copy button in the table column Cluster Name.  ","version":"Next","tagName":"h3"},{"title":"cluster.close() & client.close()​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#clusterclose--clientclose","content":"Use cluster.close() and client.close() to destroy or shut down a dask cluster after it is no longer needed to free up resources. The platform comes with an automatic garbage collection functionality - if you forget to close the cluster the platform will automatically close it after a few minutes of idle time. Starting a cluster and shutting it down: from hyperplane import notebook_common as nc client, cluster = nc.initialize_dask_cluster(num_workers=2) cluster.close() client.close()  Retrieving a forgotten Dask cluster and closing it: from hyperplane import notebook_common as nc client = nc.get_dask_cluster(&quot;dask-cluster-with-some-random-hash&quot;) cluster.close() client.close()   ","version":"Next","tagName":"h3"},{"title":"client.restart()​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#clientrestart","content":"Use client.restart whenever you want to clean up dask memory.  client.restart()  note Dask remembers every line of code that was run since initializing the cluster. If you'd like to edit a line of code after it's already been run once, then restart the dask client to ensure that the script runs smoothly.  ","version":"Next","tagName":"h3"},{"title":"Pipeline Jobs​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#pipeline-jobs","content":"There are many ways pipeline jobs can be controlled: dashboard interface, GraphQL Playground, and Hyperplane API notebook_commons. You can submit, cancel, get output, and check status on jobs from your Sessions. ","version":"Next","tagName":"h3"},{"title":"submit_pipeline_job​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#submit_pipeline_job","content":"Use this function to submit a job from your Sessions. See Create a pipeline for details on submission query fields. The function returns a job id and runID. Navigate to the Shakudo Platform Jobs tab and filter by the returned job ID if you'd like to check that the job has been successfully submitted. from hyperplane import notebook_common as nc newjob = await nc.submit_pipeline_job( jobName = 'name_of_newjob', pipelineYamlPath = 'yaml_path.yaml', jobType = 'basic', timeout = 1800, active_timeout = 1800, max_retries = 2, parameters = { &quot;a&quot;:1, &quot;b&quot;:1 } ) newjob   Parameters  Name\tType\tDescriptionjobName\tstring\tcustom name for your job pipelineYamlPath Required integer\t(Default value: 2) Number of Dask worker nodes. jobType\tstring\tJob (EC) type to use timeout\tinteger\tMaximum time that the pipeline may run, starting from the moment of job submission active_timeout\tinteger\tMaximum time that the pipeline may run once it is picked up max_retries\tinteger\tNumber of times to retry the job if the job run has failed or timed out. parameters\tdictionary\tKey value pairs for any parameters in your script you'd like to overwrite for this pipeline job  ","version":"Next","tagName":"h3"},{"title":"checkjobs()​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#checkjobs","content":"Check status on jobs using job IDs. Returns a summary of job status and links to Dask dashboards if Dask is used. ids = ['a-job-id', 'another-job-id'] res = await nc.checkjobs(ids, loop = True) print(res)  Output will look like the following: #### Jobs summary 0 / 2 in progress 0 / 2 pending 2 / 2 processed 2 done | 0 timed out | 0 cancelled | 0 failed Progress: 100.0% #### Dask dashboards a-job-id done None another-job-id done None   Parameters  Name\tType\tDescriptionids Required list\tList of job IDs to check status loop\tboolean\t(Default value: False) True will refresh the output every 5 seconds until all jobs are processed interval\tinteger\tRefresh frequency for loop = True  ","version":"Next","tagName":"h3"},{"title":"cancel_pipeline_jobs()​","type":1,"pageTitle":"Shakudo API","url":"/legacy/shakudoApi#cancel_pipeline_jobs","content":"Use cancel_pipeline_jobs() to cancel a pipeline job from Sessions. Returns {'id': 'job-id', 'status': 'cancelled} await nc.cancel_pipeline_job('job-id')   Parameters  Name\tType\tDescriptionjobID Required string\tID of pipeline job to cancel ","version":"Next","tagName":"h3"},{"title":"Linking Git Repos","type":0,"sectionRef":"#","url":"/shakudo-platform-core/adminSettings/gitrepos","content":"","keywords":"","version":"Next"},{"title":"Adding a Git Repository​","type":1,"pageTitle":"Linking Git Repos","url":"/shakudo-platform-core/adminSettings/gitrepos#adding-a-git-repository","content":"First, navigate to the Git Repositories panel on the black sidebar. You may need to scroll down. Next, click the orange &quot;Link Git Repository&quot; button on the top right, above the table. You will need to fill in the form below.  The Name is an arbitrary string to display the entry in the list of Git repositories listed on the page. The Git repository SSH URL is the URL of the Git repository to link to. HTTPS is not supported, only SSH. The link format looks like user@server.com:repo.git. For example, Shakudo's public examples repository's SSH URL is git@github.com:devsentient/examples.git. In some cases, like with GitLab, you will need to add &quot;ssh://&quot; in front of your URL.The Default Branch is the branch to use when none is specified in a job or service. This can be the repository's actual default branch (for example, main), or any other branch to be considered the default on Shakudo. This branch will be synced in Shakudo. When you create jobs, you will have an opportunity to choose another branch for a specific job. In most cases, you should leave the &quot;Use default SSH key&quot; unchecked, unless you have registered the SSH key during deployment to your own personal account (not recommended). The Git repository that was added through the steps above will be displayed in the table on the Git Repositories page.  You may notice that the status for the branch we added says &quot;remote not found&quot;. As suggested in the tooltip when hovering over the status icon, the problem is that we haven't yet added Shakudo's key to our repository. To do this, copy the SSH key by selecting the key icon on the left side of the table entry. If the popup is blank, wait up to 30 seconds before pressing the key icon again.  Add it as a deploy key for the repo, or as a personal key if needed. Exact procedure depends on your Git developer platform (e.g. GitLab, Bitbucket, GitHub, Azure DevOps, etc.). For example, on GitHub, you will need to add the displayed SSH key through the &quot;Deploy Keys&quot; section.  ","version":"Next","tagName":"h2"},{"title":"Unlinking a Repository​","type":1,"pageTitle":"Linking Git Repos","url":"/shakudo-platform-core/adminSettings/gitrepos#unlinking-a-repository","content":"To unlink a Git repository, simply click the &quot;x&quot; button in the actions menu next to the branch from a repository that you wish to unlink. ","version":"Next","tagName":"h2"},{"title":"Image Builder","type":0,"sectionRef":"#","url":"/shakudo-platform-core/adminSettings/image_builder","content":"","keywords":"","version":"Next"},{"title":"Steps:​","type":1,"pageTitle":"Image Builder","url":"/shakudo-platform-core/adminSettings/image_builder#steps","content":"Add a git server (shakudo-examples) where we want to fetch the Dockerfile. Go to Container Images tab. Select Create Container Image on the top right corner and fill in the details appropriately. See the example below:  The Dockerfile Path is the file path from the root of the repo. After the job is completed, you can Convert to EC and use it in Sessions, Jobs, or Microservices. You can also view the container image details in Harbor by selecting Open in Harbor to see the images built from Image Builder. ","version":"Next","tagName":"h3"},{"title":"Outbound Traffic Control","type":0,"sectionRef":"#","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess","content":"","keywords":"","version":"Next"},{"title":"Table of Contents​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#table-of-contents","content":"IntroductionAccess Control Options Full AccessNo AccessPartial AccessAir Gap Mode Managing Access via UI Granting Full AccessBlocking Access CompletelyGranting Access to Specific Hosts Air Gap ModeBackend AutomationExample ScenariosConclusion ","version":"Next","tagName":"h2"},{"title":"1. Introduction​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#1-introduction","content":"Istio provides a robust solution for managing egress traffic control in a Kubernetes environment. This guide explains how to use our UI to manage namespace access control, including cluster air-gap mode, full access, no access, and host-specific access ","version":"Next","tagName":"h2"},{"title":"2. Access Control Options​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#2-access-control-options","content":"","version":"Next","tagName":"h2"},{"title":"Air Gap Mode​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#air-gap-mode","content":"Blocks accessing all external services on all namespaces ","version":"Next","tagName":"h3"},{"title":"Full Access​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#full-access","content":"Granting full access allows a namespace to communicate freely with external services without any restrictions. ","version":"Next","tagName":"h3"},{"title":"No Access​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#no-access","content":"Blocking access completely prevents a namespace from communicating with any external services. ","version":"Next","tagName":"h3"},{"title":"Partial Access​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#partial-access","content":"Granting access to specific hosts allows a namespace to communicate only with specified external services. ","version":"Next","tagName":"h3"},{"title":"3. Managing Access via UI​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#3-managing-access-via-ui","content":"","version":"Next","tagName":"h2"},{"title":"Granting Full Access​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#granting-full-access","content":"Open the UI and navigate to Admin &gt; Outbound Traffic AccessSelect the settings of the namespace you want to configure.Choose the &quot;Full Access&quot; option.Click &quot;Save&quot; to save the changes. ","version":"Next","tagName":"h3"},{"title":"Blocking Access Completely​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#blocking-access-completely","content":"Open the UI and navigate to Admin &gt; Outbound Traffic AccessSelect the settings of the namespace you want to configure.Choose the &quot;No Access&quot; option.Click &quot;Save&quot; to save the changes. ","version":"Next","tagName":"h3"},{"title":"Granting Access to Specific Hosts​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#granting-access-to-specific-hosts","content":"Open the UI and navigate to Admin &gt; Outbound Traffic AccessSelect the settings of the namespace you want to configure.Choose the &quot;Partial Access&quot; option.Enter a comma-separated list of valid hosts (.ca, .com, example.com, my.example.com, *.my.example.com - Wildcard only is not allowed).Click &quot;Save&quot; to save the changes. ","version":"Next","tagName":"h3"},{"title":"5. Air Gap Mode​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#5-air-gap-mode","content":"Air Gap mode blocks access to all egress traffic on all namespaces. This mode is useful for environments that require complete isolation from external networks. To enable Air Gap mode: Open the UI and navigate to the Outbound Traffic Access mode section.Enable Air Gap mode.Confirm your change and wait for it to be applied ","version":"Next","tagName":"h2"},{"title":"6. Backend Automation​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#6-backend-automation","content":"Our backend handles the creation of service entries and sidecars based on the UI selections. When a user selects an access control option, the backend will automatically create the necessary Istio configurations to enforce the desired access control. ","version":"Next","tagName":"h2"},{"title":"7. Example Scenarios​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#7-example-scenarios","content":"","version":"Next","tagName":"h2"},{"title":"Scenario 1: Granting Full Access​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#scenario-1-granting-full-access","content":"Namespace: namespace1Action: Grant full accessResult: namespace1 can communicate with all external services. ","version":"Next","tagName":"h3"},{"title":"Scenario 2: Blocking Access Completely​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#scenario-2-blocking-access-completely","content":"Namespace: namespace2Action: Block access completelyResult: namespace2 cannot communicate with any external services. ","version":"Next","tagName":"h3"},{"title":"Scenario 3: Granting Access to Specific Hosts​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#scenario-3-granting-access-to-specific-hosts","content":"Namespace: namespace3Action: Grant access to specific hosts (e.g., example.com, api.example.com)Result: namespace3 can only communicate with example.com and api.example.com. ","version":"Next","tagName":"h3"},{"title":"8. Conclusion​","type":1,"pageTitle":"Outbound Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#8-conclusion","content":"This guide provides detailed instructions for managing namespace access control to egress traffic on Istio using our UI. By following these steps, you can effectively control egress traffic in your cluster. ","version":"Next","tagName":"h2"},{"title":"Environment Configs","type":0,"sectionRef":"#","url":"/shakudo-platform-core/adminSettings/podspecs","content":"","keywords":"","version":"Next"},{"title":"Environment Config details​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#environment-config-details","content":"To check the configurations for each EC, navigate to the Environment Config tab and click on the card you'd like to see the details for.  ","version":"Next","tagName":"h2"},{"title":"Check installed packages​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#check-installed-packages","content":"To check what packages are installed in an EC, you can do so from Sessions. Create a new session and choose the session type for which you'd like to check the packages for. Open up the browser Session and open a Jupyter notebook. Type in pip freeze to see pip packages installed and conda list for conda packages installed.  ","version":"Next","tagName":"h3"},{"title":"Create an EC​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#create-an-ec","content":"","version":"Next","tagName":"h2"},{"title":"Prerequisities​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#prerequisities","content":"These are a few requirements when creating an EC: An image that is accessible to ShakudoVolumes and volume mounts to existAppropriate node pools or resources ","version":"Next","tagName":"h3"},{"title":"Steps to create an EC​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#steps-to-create-an-ec","content":"You can create a new EC to use in your Sessions, Jobs, or Services. Follow the steps below to do so: Navigate to the Environment Config tab and click on the + button to create a new EC Customize the details. For details on each field, see the section below for podspec configurations Click CREATE To view and use the new EC you must press ⌘ + R to hard refresh the page.  ","version":"Next","tagName":"h3"},{"title":"EC configurations​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#ec-configurations","content":"","version":"Next","tagName":"h2"},{"title":"Name​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#name","content":"This is the name that will show up when choosing which EC to use in Sessions, Jobs, and Services. ","version":"Next","tagName":"h3"},{"title":"EC name​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#ec-name","content":"This name acts as a unique ID for the EC. ","version":"Next","tagName":"h3"},{"title":"Description​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#description","content":"Give the EC a one liner description for when someone should pick it. This description appears in the EC page as well as in the Session Type selection.  ","version":"Next","tagName":"h3"},{"title":"Image URL​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#image-url","content":"You can choose to bring your own docker image to build your custom EC with. Paste in the link to the image in this field. ","version":"Next","tagName":"h3"},{"title":"Resources​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#resources","content":"You can specify the CPU request &amp; limit as well as the memory request &amp; limit. If you want GPUs make sure to specify GPUs that are available to your cluster. If you're unsure please contact us and we can provide you the details. ","version":"Next","tagName":"h3"},{"title":"Volumes & volume mounts​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#volumes--volume-mounts","content":"Specify your Volumes and Volume Mounts to attach by clicking on the + button. For more information see the Kubernetes guide on Volumes. ","version":"Next","tagName":"h3"},{"title":"Environment variables​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#environment-variables","content":"Use this field to specify the name and value for environment variables.  ","version":"Next","tagName":"h3"},{"title":"Edit an EC​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#edit-an-ec","content":"You can edit details of an EC by clicking on the pencil icon from the EC tab. By default you can only edit the custom created ECs (not the default ECs created by Shakudo) When an EC is edited and saved, newly created jobs and sessions will use the updated version. Any jobs or sessions currently in progress will use the old version. Any services in progress using the edited EC will be restarted to update. ","version":"Next","tagName":"h2"},{"title":"PVC Management","type":0,"sectionRef":"#","url":"/shakudo-platform-core/adminSettings/pvc-management","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#overview","content":"PVC Management provides administrators with tools to: View all Persistent Volume Claims across the clusterSearch and filter PVCs by namespace, name, and statusMonitor storage usage, capacity, and fullnessResize PVCs (when supported by the storage class)View which pods are using each PVC ","version":"Next","tagName":"h2"},{"title":"Accessing PVC Management​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#accessing-pvc-management","content":"To access PVC Management: Ensure you have the dashboard-admin roleNavigate to the Admin section in the Shakudo dashboard sidebarSelect Persistent Volume tabView the comprehensive list of all PVCs in your cluster ","version":"Next","tagName":"h2"},{"title":"PVC Information​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#pvc-information","content":"The PVC Management interface displays the following information for each Persistent Volume Claim: Field\tDescriptionName\tThe name of the PVC Namespace\tThe Kubernetes namespace where the PVC resides Status\tCurrent status (Bound or not bound) Size\tThe allocated storage capacity Storage Class\tThe storage class used for provisioning Pods Using PVC\tWhich pods are currently using the PVC Created\tWhen the PVC was created ","version":"Next","tagName":"h2"},{"title":"Search and Filter Options​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#search-and-filter-options","content":"The interface provides several filtering and search capabilities: Search by name: Find specific PVCs by their nameFilter by namespace: View PVCs from specific namespacesFilter by status: Show only bound or unbound PVCs  ","version":"Next","tagName":"h2"},{"title":"PVC Details​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#pvc-details","content":"Clicking on any PVC name opens the detailed view, which provides: ","version":"Next","tagName":"h2"},{"title":"Storage Usage Information​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#storage-usage-information","content":"Current fullness: Shows how much of the PVC storage is currently being usedAvailable space: Remaining storage capacity ","version":"Next","tagName":"h3"},{"title":"Resizing PVCs​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#resizing-pvcs","content":"If your storage class supports resizable PVCs, you can: Increase the PVC size directly from the details viewMonitor the resize operation progress  note PVC resizing is only available for storage classes that support volume expansion. The resize operation typically requires the PVC to be in use by a running pod. ","version":"Next","tagName":"h3"},{"title":"Important Notes for Single VM Deployments​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#important-notes-for-single-vm-deployments","content":"When running Shakudo on a singular VM setup: Virtual partitions: The PVC partitions are virtual rather than on separate physical volumesShared storage: All PVCs share the same underlying node storageFullness calculation: The fullness percentage shows what is available on the full node, not just the individual PVC This means that even if a PVC appears to have available space, it may be limited by the total available space on the node. ","version":"Next","tagName":"h2"},{"title":"Best Practices​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#best-practices","content":"","version":"Next","tagName":"h2"},{"title":"Storage Monitoring​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#storage-monitoring","content":"Regularly review PVC usage and fullness to identify storage issues earlyMonitor for PVCs that are approaching capacity limitsKeep track of unbound PVCs that may indicate configuration issues ","version":"Next","tagName":"h3"},{"title":"PVC Management​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#pvc-management-1","content":"Use the search and filter features to quickly locate specific PVCsRegularly review which pods are using each PVC to understand dependenciesConsider resizing PVCs proactively before they reach capacity ","version":"Next","tagName":"h3"},{"title":"Single VM Considerations​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#single-vm-considerations","content":"Monitor overall node storage capacity in addition to individual PVC usageBe aware that all PVCs compete for the same underlying storage spacePlan storage allocation carefully when multiple workloads share the same node ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#troubleshooting","content":"","version":"Next","tagName":"h2"},{"title":"Unbound PVCs​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#unbound-pvcs","content":"If PVCs show as not bound: Check if the specified storage class exists and is availableVerify that there is sufficient storage space on the nodeReview any error messages in the PVC details ","version":"Next","tagName":"h3"},{"title":"Storage Full Issues​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#storage-full-issues","content":"When PVCs or nodes are approaching capacity: Use the details view to identify which PVCs are consuming the most spaceConsider resizing PVCs if the storage class supports itClean up unused data or consider adding more storage capacity ","version":"Next","tagName":"h3"},{"title":"Resize Operations​","type":1,"pageTitle":"PVC Management","url":"/shakudo-platform-core/adminSettings/pvc-management#resize-operations","content":"If PVC resizing fails: Ensure the storage class supports volume expansionVerify that the PVC is currently in use by a running podCheck for sufficient space on the underlying storage note The PVC Management interface provides real-time information about your storage resources. Features and capabilities may vary depending on your Kubernetes version and storage provider configuration. ","version":"Next","tagName":"h3"},{"title":"Secrets","type":0,"sectionRef":"#","url":"/shakudo-platform-core/adminSettings/secrets","content":"","keywords":"","version":"Next"},{"title":"How to create a secret?​","type":1,"pageTitle":"Secrets","url":"/shakudo-platform-core/adminSettings/secrets#how-to-create-a-secret","content":"Go to the secrets tab on the Shakudo dashboard and click on &quot;Create Secret&quot; on the top right corner. Add the name and description of the secret. Note: The name should not contain blank space or underscore, since it's a mount directory name when you attach a secret to jobs/sessions. Adding a purpose: A secret created by a user is often used in the microservice or within sessions. So, adding a purpose allows users to scope the usage of the secret to a job or session or both. Development refers to sessions which creates a secret in the hyperplane-jhub namespace and can only be used in sessions.Workloads refers to jobs/microservice which creates a secret in the hyperplane-pipelines namespace and can only be used in microservices.Workloads &amp; Development helps create a secret in both of the above namespaces and can be used in all of Sessions, Jobs and Microservices. ","version":"Next","tagName":"h3"},{"title":"How to access a secret in your code?​","type":1,"pageTitle":"Secrets","url":"/shakudo-platform-core/adminSettings/secrets#how-to-access-a-secret-in-your-code","content":"You can access the secret attached to your code via the mount directory. For example, if you create a secret minio-creds with the key as username and values as HelloWorld then it will be mounted at /etc/hyperplane/secrets/minio-creds/username file. Alternatively, you can access secrets with environment variables.HYPERPLANE_CUSTOM_SECRET_KEY_USERNAME The key which you define will be added as an environment variable with the prefix HYPERPLANECUSTOM_SECRET_KEY. ","version":"Next","tagName":"h3"},{"title":"pyshakudo to manage secrets.​","type":1,"pageTitle":"Secrets","url":"/shakudo-platform-core/adminSettings/secrets#pyshakudo-to-manage-secrets","content":"Shakudo offers a Python client pyshakudo to manage Shakudo resources. Note: This is still under development and currently capable of managing secrets. pip install --index-url http://pypiserver-pypiserver.hyperplane-pypiserver.svc.cluster.local:8080/simple/ --trusted-host pypiserver-pypiserver.hyperplane-pypiserver.svc.cluster.local pyshakudo==0.1.0 Contact the Shakudo team to get access. In your Python code, use this package to dynamically operate on the secrets. # Example code to use pyshakudo to operate on Shakudo secrets in your sessions or microservice. from pyshakudo.secrets import ShakudoSecretsManager def main(): # Initialize the ShakudoSecretsManager with in-cluster configuration manager = ShakudoSecretsManager() # Get a specific secret print(&quot;Getting specific secret...&quot;) secret_name = &quot;test-secret&quot; secret = manager.get_secret(secret_name) print(&quot;Retrieved Secret Data:&quot;, secret.data) if __name__ == &quot;__main__&quot;: main()  ","version":"Next","tagName":"h3"},{"title":"tsshakudo to manage secrets in your typescript code.​","type":1,"pageTitle":"Secrets","url":"/shakudo-platform-core/adminSettings/secrets#tsshakudo-to-manage-secrets-in-your-typescript-code","content":"tsshakudo is a npm package developed by shakudo team to manage shakudo secrets in your node code. Note: This is still under development and currently capable to managing secrets. npm install npm install tsshakudo@0.1.0 import { ShakudoSecretsManager } from 'tsshakudo'; const main = async () =&gt; { // Initialize the secrets manager with the default namespace and in-cluster config const secretsManager = new ShakudoSecretsManager('hyperplane-jhub'); // Get the created secret const fetchedSecret = await secretsManager.getSecret('test-secret'); console.log('Fetched Secret:', fetchedSecret); main().catch((error) =&gt; { console.error('Error:', error); });  ","version":"Next","tagName":"h3"},{"title":"Traffic Shifter","type":0,"sectionRef":"#","url":"/shakudo-platform-core/adminSettings/traffic-shifter","content":"Traffic Shifter Traffic Shifters are essentially splitters that route traffic to one or more services with specified weights. In the most basic scenario, you can use a Traffic Shifter to give your microservice an alternative domain or to open up a webhook for it. The following video demonstrates how to create a Traffic Shifter with a webhook, and how to delete or cancel it when it is no longer needed:","keywords":"","version":"Next"},{"title":"Jobs and Triggers","type":0,"sectionRef":"#","url":"/shakudo-platform-core/jobs","content":"","keywords":"","version":"Next"},{"title":"Creating a Job​","type":1,"pageTitle":"Jobs and Triggers","url":"/shakudo-platform-core/jobs#creating-a-job","content":"Jobs can be created from the corresponding job menu (immediate or scheduled) depending on the desired job type. As the names imply, immediate jobs will run immediately while scheduled jobs will run on a configured schedule. Shakudo supports various job types, including IPython notebooks and Python scripts. ","version":"Next","tagName":"h2"},{"title":"Immediate Jobs​","type":1,"pageTitle":"Jobs and Triggers","url":"/shakudo-platform-core/jobs#immediate-jobs","content":"Immediate jobs can be created by clicking &quot;start a job&quot; on the Shakudo landing page.  Creating a job involves, at minimum, filling in the job name (automatically generated by default), environment configuration (which works the same way as for Shakudo sessions or services), and specifying the job yaml specification as described above. When using a non-default environment configuration, advanced settings like source git repository, job timeout, and job retry count can be modified to suit job requirements. Immediate Jobs can be rerun as scheduled jobs from the immediate jobs page by using the actions menu (see Scheduled Jobs below), which will clone the job specification and change it into a Scheduled Job.  ","version":"Next","tagName":"h3"},{"title":"Scheduled Jobs​","type":1,"pageTitle":"Jobs and Triggers","url":"/shakudo-platform-core/jobs#scheduled-jobs","content":"Scheduled jobs can be created from the Scheduled Jobs page by first selecting the appropriate menu item  And clicking on the &quot;create scheduled job&quot; button on the Scheduled Jobs page.  Scheduled jobs work like Immediate Jobs, except that a schedule in crontab format must be specified in addition to other basic fields.  ","version":"Next","tagName":"h3"},{"title":"Job Specification (multi-step)​","type":1,"pageTitle":"Jobs and Triggers","url":"/shakudo-platform-core/jobs#job-specification-multi-step","content":"Select &quot;multi-step&quot; under pipeline. Jobs are specified in a pipeline yaml format.  A basic pipeline yaml will resemble the below: pipeline: name: [name of your pipeline job] working_dir: (optional) directory where pipeline is ran from (default is the root of the repository) tasks: - name: “step1” type: type of file notebook_path: path to notebook notebook_output_path: output notebook location depends_on: run steps in parallel or specify dependencies working_dir: (optional) directory where task is ran from (default is the root of the repository)  When filling in a value for this template, enclose it in quotation marks (e.g., &quot;step1&quot;). Paths to files and directories, should be the relative paths in your repository. For the type of file, the following are accepted: VS Code notebook: vs code notebookPython scripts: python script, python, or pyJavaScript: js script, javascript, or jsJupyter notebooks: jupyter notebook, jupyter, notebook, or ipynb Additional steps can be specified as per job requirements simply by adding more step entries within the tasks list. Jobs can be specified for sequential, parallel, or mixed execution. Steps will run sequentially by default. Specifying dependencies using a list of strings in the depends_on field of a step entry allows steps to dispatch in parallel, so long as their dependencies (i.e. jobs, identified by name, which have to have completed execution before this step executes) are fulfilled. An empty list ([]) as a value for depends_on will dispatch the step immediately (since it has no dependencies), in parallel with other jobs that are ready to run. The following example specifies a step to run in parallel:  - name: “parallelStep” type: type of file notebook_path: path to notebook notebook_output_path: output notebook location depends_on : []  For more examples of pipeline yaml files, see the Shakudo examples repository Below are example step specifications for job types supported by Shakudo. To add a Jupyter notebook step:  - name: &quot;[your_step_name]&quot; type: &quot;jupyter notebook&quot; notebook_path: &quot;[notebook/path/relative/to/top/level/of/repo.ipynb]&quot; notebook_output_path: &quot;[some/notebook_output_name.ipynb]&quot;  To add a VS Code notebook step:  - name: &quot;[another_step_name]&quot; type: &quot;vs code notebook&quot; py_path: &quot;[py/file/relative/to/top/level/of/repo.py]&quot;  To add a Node.js JavaScript step:  - name: &quot;[another_step_name]&quot; type: &quot;js script&quot; js_path: &quot;[js/file/relative/to/top/level/of/repo.js]&quot;  To add a bash script step:  - name: &quot;[another_step_name]&quot; type: &quot;bash script&quot; bash_script_path: &quot;[sh/file/relative/to/top/level/of/repo.sh]&quot;  Head to the &quot;Advanced&quot; tab, and remember to select your Git repository. See the platform's tutorial on how to link your repo.  ","version":"Next","tagName":"h3"},{"title":"Bash Script (single step job)​","type":1,"pageTitle":"Jobs and Triggers","url":"/shakudo-platform-core/jobs#bash-script-single-step-job","content":"If you only need to run a single step consisting of a bash script, select &quot;Shell&quot; instead of &quot;Multi-step&quot; in the Job creation dialogue.  As you would for multi-step pipeline YAMLs, specify the path to the bash script relative to the root of your git repo. An example bash script might look like the following: #!/bin/bash set -e PROJECT_DIR=&quot;$(cd -P &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot; cd &quot;$PROJECT_DIR&quot; pip install foo-package==bar-version python ./a-script.py  Note the shebang for bash. The -e flag detects errors in the currently running script, and stops the script when one of the commands within returns a non-zero status. PROJECT_DIR finds the current directory of the current bash script. It is useful when you want to reference things relative to the current bash script.  ","version":"Next","tagName":"h3"},{"title":"Parameterizing Notebooks​","type":1,"pageTitle":"Jobs and Triggers","url":"/shakudo-platform-core/jobs#parameterizing-notebooks","content":"Jupyter notebooks run as jobs can be parameterized. Parameters can then be specified in the job creation menu in the Parameters tab. To prepare a notebook to use parameters in JupyterLab, click on the cog icon on the top right corner of the cell to parameterize and add a tag called parameters. In VS Code, this is the &quot;mark cell as parameters&quot; option in the breadcrumb menu for the cell of interest.  The cell's contents will be overwritten by the parameter's value as specified in the Parameters tab, if applicable. ","version":"Next","tagName":"h2"},{"title":"Microservices","type":0,"sectionRef":"#","url":"/shakudo-platform-core/service","content":"","keywords":"","version":"Next"},{"title":"Creating a Microservice​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#creating-a-microservice","content":"Services can be created from the Shakudo landing page by clicking the &quot;start a service&quot; button.  Alternatively, services can be created from the service page by clicking &quot;create service&quot;.  ","version":"Next","tagName":"h2"},{"title":"Basic settings​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#basic-settings","content":"A basic service can be started by simply providing the service name (automatically generated by default), the desired service endpoint, an environment configuration (which work the same way as in Shakudo Sessions), and a path to the service configuration YAML file, relative to the git repository associated with the service (configurable in the advanced tab) ","version":"Next","tagName":"h3"},{"title":"Advanced settings​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#advanced-settings","content":"The advanced tab allows configuring some more advanced service parameters, described in the table below: Name\tDescriptionPort\tEnter a port to expose between 1 and 65535. The default for Shakudo Platform services is port 8787. Min Replicas\tMinimum amount of replicas running the service simultaneously Max Replicas\tMaximum amount of replicas running the service simultaneously Git repository\tThe git repository associated with the service Branch\tGit branch name for the service Commit\tGit commit ID hash to use for the service The git repository, branch and commit will be used to clone a project into the service environment, and the service YAML will be located and run from the root of the cloned project. Additional arbitrary parameters for the service runner configuration can be set in the Parameters tab. ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#parameters","content":"Similarly to Jobs, you can add Parameters to inject some additional information, which you can use in your code. When you attach Parameters, they will be available as environment variables, in upper case, with all non-alphanumeric characters replaced by _. For example, if you add an parameter with name &quot;model-name&quot;, it will be accessible as the MODEL_NAME in the Service's environment variables. To maintain backward compatibility with legacy services, Parameters are also available with the HYPERPLANE_JOB_PARAMETER_. Using the example above, this would be HYPERPLANE_JOB_PARAMETER_MODEL_NAME. Note that these values are stored in plain text, it's recommended that you use Secrets for API keys, keyfile strings, access credentials, and other secret values. ","version":"Next","tagName":"h3"},{"title":"Secrets​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#secrets","content":"When you attach Secrets, they will be available both as environment variables and as a file. In the case of environment variable, the secret key will be converted to upper case, with all non-alphanumeric characters replaced by _, and prefixed with HYPERPLANE_CUSTOM_SECRET_KEY_. For example, if you add an parameter with name &quot;openai-key&quot;, it will be accessible as the HYPERPLANE_CUSTOM_SECRET_KEY_OPENAI_KEY in the Service's environment variables. Secrets are available as files, in the format of /etc/hyperplane/secrets/{secret_name}/{secret_key}. ","version":"Next","tagName":"h3"},{"title":"Starting with a bash script​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#starting-with-a-bash-script","content":"If you only need to run a single step consisting of a bash script to start your app, select &quot;Shell&quot; instead of &quot;Multi-step&quot; in the Job creation dialogue.  As you would for multi-step pipeline YAMLs, specify the path to the bash script relative to the root of your git repo. An example bash script might look like the following: #!/bin/bash set -e PROJECT_DIR=&quot;$(cd -P &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot; cd &quot;$PROJECT_DIR&quot; pip install foo-package==bar-version python ./app.py  Note the shebang for bash. The -e flag detects errors in the currently running script, and stops the script when one of the commands within returns a non-zero status. PROJECT_DIR finds the current directory of the current bash script. It is useful when you want to reference things relative to the current bash script. This bash script will start your Microservice (e.g. a Flask app) and run until app.py exits with an error. In the case of an error, the pod will be restarted and the script will rerun from the top. ","version":"Next","tagName":"h3"},{"title":"Subdomains and exposing endpoints​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#subdomains-and-exposing-endpoints","content":"The recommended approach is to use Subdomains rather than sub-paths for each application. In versions of Shakudo 3.47.0 and later, Microservices with subdomains that end in &quot;-public&quot; or &quot;-webhook&quot; will be publicly exposed to users who are not logged in, acting as webhooks. We recommend using our random URL generator to protect your webhooks endpoints.  If you already have a running service and would like to open up a separate URL, you can use the Traffic Shifter to create a webhook for an existing Microservice.  ","version":"Next","tagName":"h3"},{"title":"Logs and filtering​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#logs-and-filtering","content":"To view the live logs for a Microservice, go to the Microservices table view and select the logs icon for the live logs modal. If you would like to view historical logs, you can click the Grafana icon in the table entry. The Grafana dashboard is automatically filtered for the Microservice by id. To see logs for a specific service name rather than id, you can use a filter like {hyperplane_dev_app_name=&quot;test-recycle&quot;} in the Grafana dashboard.  ","version":"Next","tagName":"h3"},{"title":"Customize Pod YAML​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#customize-pod-yaml","content":"Customize Pod YAML is an option for advanced users who are familiar with Kubernetes and want to manipulate the specs of a Microservice at a more granular level. The screen starts off with the YAML generated from the Create a Microservice form, but you can edit any of the fields. There is a validation check to ensure that the fields are Kubernetes compatible. For examples on specific ways to modify your custom YAMLs, see the tutorials here.  For previously-created services, if you want to compare the differences between the custom YAML used vs. the auto-generated YAML, you can click on the &quot;Customized YAML&quot; on the Microservice Details page to see a side-by-side comparison.  ","version":"Next","tagName":"h3"},{"title":"Service actions​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#service-actions","content":"The service actions menu can be used to operate on a current or past service.  ","version":"Next","tagName":"h2"},{"title":"Cancel​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#cancel","content":"A current service can be canceled by choosing the &quot;cancel&quot; action in the action menu. ","version":"Next","tagName":"h3"},{"title":"Clone​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#clone","content":"A new service can be created based on the configuration of an existing service by choosing the &quot;clone&quot; action. This can also be used on past services that have been canceled to recreate a service of the same type, or to use the service settings as a template for faster iteration. ","version":"Next","tagName":"h3"},{"title":"Restart​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#restart","content":"Restarting a service (which will cause it to pull from the associated repository as per its settings) can be done by selecting the &quot;restart&quot; action. ","version":"Next","tagName":"h3"},{"title":"Edit​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#edit","content":"If you are on a version of Shakudo that supports editing Microservices and you have permissions to change a specific Microservice, you will see &quot;Edit&quot; in the dropdown. Editing most values, other than the README, will result in a microservice restart upon saving. If your Shakudo version does not yet support a single-action update operation on Microservices, the recommended path to updating specific values are to first stop the existing service and then clone it, with an updated configuration. For updates to the service code from the associated repository which don't require configuration changes, the Microservice should be restarted with the restart action. ","version":"Next","tagName":"h3"},{"title":"Accessing a service from outside the cluster​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#accessing-a-service-from-outside-the-cluster","content":"Creating webhooks using the Traffic Shifter is a standard way to expose a Shakudo Microservice publicly. However, if you prefer not to use webhooks, Microservices can be accessed externally using a JWT using the bearer method in the request headers. Prepare to make a POST request to https://{your_shakudo_domain}/auth/realms/Hyperplane/protocol/openid-connect/token, for example using curl or Postman {your_shakudo_domain} is the domain at which your Keycloak is available, which is usually the same domain at which you access your cluster running Shakudo Set the Content-Type header to application/x-www-form-urlencodedSet the following parameters in the request body: client_id: the client used to get the token (should be istio)grant_type: value should be passwordusername: Your usernamepassword: Your password note The Access Type must be public to obtain a JWT this way The response JWT can then be used to access service endpoints: simply add Authorization: Bearer {token} to your Headers, with {token} being the JWT obtained in the previous step. Note that in some Stack Components, the Authorization: header is needed to use credentials in requests, but in Shakudo, it is reserved for Keycloak. Therefore, if your application or Stack Component requires credentials in the form of an Authorization: header, you should pass the component-specific credentials into the x-custom-auth: header instead.  ","version":"Next","tagName":"h2"},{"title":"Health Check​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#health-check","content":"","version":"Next","tagName":"h2"},{"title":"Liveness and Readiness Probes​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#liveness-and-readiness-probes","content":"In versions v3.48.0 and later of Shakudo, you can enable &quot;Health Probes&quot; through the Microservice creation page. By default, enabling Health Probes through the dashboard adds a simple TCP socket probe, using the value in your exposedPort, to your Microservice:   livenessProbe: tcpSocket: port: 8787 initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: tcpSocket: port: 8787 initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3  If you wish to modify to a more advanced health probe, you can go to the Custom YAML panel to add further customizations. For example, to add a probe checking a /health endpoint, you can use the following:  livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3  For more options, refer to the Kubernetes official docs for probes. ","version":"Next","tagName":"h3"},{"title":"Check All Services as an Admin​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#check-all-services-as-an-admin","content":"Once deployed, you can confirm the Microservice is healthy using the k9s or Cluster Shell (if you have a dashboard-admin role).  Example: Neo4j Streamlit Microservice This example demonstrates how to deploy a simple Streamlit application that connects to a Neo4j database and provides a real-time query interface with the following features: Interactive frontend built with StreamlitConnects to Neo4j using the official Python driverVisualizes Cypher query results in an intuitive, user-friendly graph interfaceFully configurable via environment variables (URI, user, password, etc.)Includes a setup and usage screen recording  ","version":"Next","tagName":"h3"},{"title":"Setup Instructions​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#setup-instructions","content":"Add the repository to your Shakudo workspace. Create a new Microservice using either the landing page or the service dashboard. Set Service Details: Name your service: Example: neo4j-streamlit Subdomain (optional but recommended): Set it to match the name, e.g. neo4j-streamlit, which will expose the service at: neo4j-streamlit.test-dev.canopyhub.io Port: Use the default port: 8787 Environment Config: Select Basic (or another appropriate Environment Config that includes Python) Pipeline: Choose the Shell option Provide the relative path to your shell script, for example: neo4j-microservice/run.sh This script should exist in your Git repository and be executable. Git Repository: Select repository: shakudo-examplesBranch: feature/neo4j-streamlit  ","version":"Next","tagName":"h2"},{"title":"Parameters​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#parameters-1","content":"Add the following Parameters (not Secrets unless sensitive): Parameter Name\tDescription\tExample ValueNEO4J_URI\tBolt connection URI for Neo4j\tbolt://neo4j.hyperplane-neo4j.svc.cluster.local:7687 NEO4J_USER\tNeo4j username\tneo4j NEO4J_PASSWORD\tNeo4j password\tyour_secure_password (use a Secret for real deployments) These will be available inside the service as environment variables: e.g., NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD   ","version":"Next","tagName":"h2"},{"title":"Streamlit app preview​","type":1,"pageTitle":"Microservices","url":"/shakudo-platform-core/service#streamlit-app-preview","content":"  Watch Setup Demo — Watch the setup demo here ","version":"Next","tagName":"h2"},{"title":"Sessions","type":0,"sectionRef":"#","url":"/shakudo-platform-core/sessions/","content":"","keywords":"","version":"Next"},{"title":"Get started with Sessions​","type":1,"pageTitle":"Sessions","url":"/shakudo-platform-core/sessions/#get-started-with-sessions","content":"From the homepage, click the &quot;start a session&quot; button note Alternatively, click the &quot;start a session&quot; button on the Sessions screen Choose which type of session you’d like to use as your development environment note Optionally, setup any other details, like which drive or docker image to use Click &quot;start session&quot; at the bottom of the page note The session spin-up may take a few seconds to a few minutes depending on your session type. Once your session is ready, click an access button to connect to the session. Pictured from left to right: in-browser jupyterlab, in-browser vscode, ssh connection string. note while a session may appear as &quot;running&quot; or &quot;active&quot;, until the access buttons beyond ssh appear (e.g. the jupyterlab and vscode access buttons), it may not yet have initialized all its components. If a connection cannot be established, it may simply be because the session is completing initialization. That is most likely the case if you do not see an error in the session logs.  ","version":"Next","tagName":"h2"},{"title":"Configurations​","type":1,"pageTitle":"Sessions","url":"/shakudo-platform-core/sessions/#configurations","content":"","version":"Next","tagName":"h2"},{"title":"Session Type​","type":1,"pageTitle":"Sessions","url":"/shakudo-platform-core/sessions/#session-type","content":" The session type that you choose in the session creation popup will prepare your Session with a pre-configured development environment. Relevant packages, frameworks and tools will be made available and the environment will expose the selected hardware. Details on what resources, volumes, and YAML are used for each of the session types can be found by navigating to the Environment Configs tab on the dashboard and clicking on the card for more details. Contact us for any images that we do not currently support. ","version":"Next","tagName":"h3"},{"title":"Timeout​","type":1,"pageTitle":"Sessions","url":"/shakudo-platform-core/sessions/#timeout","content":" Set the Session timeout to your desired session lifetime. The time may be specified as an arbitrary value, in seconds. To disable the timeout completely, enter -1. Alternatively, a dropdown list of common expiry times is provided for convenience. In the case of in-browser sessions, this value will be interpreted as an idle timeout and will reset every time activity starts again in the session. For SSH connections, it is an absolute timeout: the session will terminate after the specified amount of time has elapsed since session spin-up. ","version":"Next","tagName":"h3"},{"title":"Drive Name - Spinning up multiple Sessions​","type":1,"pageTitle":"Sessions","url":"/shakudo-platform-core/sessions/#drive-name---spinning-up-multiple-sessions","content":" Each Session is connected to a drive containing your files and folders. Drives may only be used in one session at a time. Each user has a default drive (called &lt;username&gt;'s default drive). Users can spin up multiple sessions by selecting a new drive or creating one if all drives are in use. The default drive is only accessible by the corresponding user, other (created) drives are accessible by anyone. To create a drive, click on the drive management icon next to the Drive selection dropdown, then click the &quot;add drive&quot; button.  ","version":"Next","tagName":"h3"},{"title":"Opening Ports","type":0,"sectionRef":"#","url":"/shakudo-platform-core/sessions/opening_ports","content":"","keywords":"","version":"Next"},{"title":"Default Port Access​","type":1,"pageTitle":"Opening Ports","url":"/shakudo-platform-core/sessions/opening_ports#default-port-access","content":"Port 3000 is automatically open for every Session. When you create an application running on port 3000 (for example, a NextJS app in development mode), you can access it via: https://nextjs-&lt;first-6-chars-of-session-id&gt;.YOUR-DOMAIN.TLD  Replace &lt;first-6-chars-of-session-id&gt; with the first 6 characters of your Session ID, and YOUR-DOMAIN.TLD with your Shakudo deployment domain. note In code-server (VS Code), when you have an application listening on port 3000, you will see a popup notification that provides a link to access your dashboard directly. ","version":"Next","tagName":"h2"},{"title":"Adding Additional Ports​","type":1,"pageTitle":"Opening Ports","url":"/shakudo-platform-core/sessions/opening_ports#adding-additional-ports","content":"Available on Shakudo version 3.52.0+ Starting in version 3.52.0, you can add additional ports to your Session even after the Session has been created. This allows you to expose multiple services or applications from a single Session. ","version":"Next","tagName":"h2"},{"title":"How to Add Ports​","type":1,"pageTitle":"Opening Ports","url":"/shakudo-platform-core/sessions/opening_ports#how-to-add-ports","content":"Navigate to the Sessions table in your Shakudo consoleLocate your Session and click the dropdown menu on the right side under the &quot;Actions&quot; columnSelect Add Ports from the dropdown menuIn the modal that appears, you will see: Currently Open Ports: Displays all ports that are currently exposed for your SessionAdd New Ports: An input field where you can add additional ports  Type your desired port number (between 3000-9999) in the input fieldPress Enter to add the port to the listRepeat steps 5-6 to add multiple ports if neededClick Submit to apply the changes note The port range is restricted to 3000-9999. If you need to expose a service running on a different port, you can configure your application to use a port within this range. ","version":"Next","tagName":"h3"},{"title":"Accessing Your Applications​","type":1,"pageTitle":"Opening Ports","url":"/shakudo-platform-core/sessions/opening_ports#accessing-your-applications","content":"Once ports are added to your Session, you can access applications running on those ports using the following URL pattern: https://nextjs-&lt;first-6-chars-of-session-id&gt;-&lt;port&gt;.YOUR-DOMAIN.TLD  For example, if your Session ID starts with 16231a and you've added port 8888, you would access your application at: https://nextjs-16231a-8888.YOUR-DOMAIN.TLD  Replace &lt;first-6-chars-of-session-id&gt; with the first 6 characters of your Session ID, &lt;port&gt; with the port number, and YOUR-DOMAIN.TLD with your Shakudo deployment domain. ","version":"Next","tagName":"h2"},{"title":"Shakudo CLI (sctl)","type":0,"sectionRef":"#","url":"/shakudo-platform-core/sessions/sctl","content":"","keywords":"","version":"Next"},{"title":"Features​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sessions/sctl#features","content":"List workloadsView workloads logsList microservicesCreate microservicesRestart microservicesView microservices logs ","version":"Next","tagName":"h2"},{"title":"Manage Shakudo Objects​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sessions/sctl#manage-shakudo-objects","content":"","version":"Next","tagName":"h2"},{"title":"Immediate Jobs​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sessions/sctl#immediate-jobs","content":"Manage immediate jobs by listing or viewing their logs. Commands: sctl immediate-jobs ls --limit [number of items] --offset [offset of items]: List all immediate jobs.sctl immediate-jobs logs --id [job-id] --lines [number of lines - defaults to 100]: Tail logs of an existing immediate job by its ID. Examples: # List all immediate jobs $ sctl immediate-jobs ls # Tail logs for a specific immediate job $ sctl immediate-jobs logs --id 12345 # Tail logs for a specific immediate job and set the line limit $ sctl immediate-jobs logs --id 12345 --lines 1000 # Watch logs for a microservice every 2 seconds watch sctl immediate-jobs logs --id 12345  ","version":"Next","tagName":"h3"},{"title":"Scheduled Jobs​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sessions/sctl#scheduled-jobs","content":"View and manage scheduled jobs. Commands: sctl scheduled-jobs ls --limit [number of items] --offset [offset of items]: List all scheduled jobs. Examples: # List all scheduled jobs $ sctl scheduled-jobs ls  ","version":"Next","tagName":"h3"},{"title":"Microservices​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sessions/sctl#microservices","content":"Manage microservices by listing, creating, restarting, and viewing logs. Commands: sctl microservices ls --limit [number of items] --offset [offset of items]: List all microservices.sctl microservices create [options]: Create a new microservice.sctl microservices restart --id [service-id]: Restart an existing microservice by ID.sctl microservices logs --id [service-id] --lines [number of lines - defaults to 100]: Tail logs of a specific microservice by ID. Examples: Basic creation of a microservice: $ sctl microservices create --gitServerName shakudo-dev-repo --ai Creating a custom microservice with job name, subdomain, and script path: $ sctl microservices create --scriptPath fastapi-openapi-example/run.sh \\ --pipelineType BASH --jobName my-custom-ms --subdomain custom-subdomain Creating a microservice with notifications enabled: $ sctl microservices create --gitServerName shakudo-dev-repo \\ --notificationsEnabled \\ --notificationTargetIds 8ebc86f3-c76e-4ce7-99f9-666151923b0c,14960534-15dd-4ffd-8d46-2e00b2f55f36 \\ --healthCheckThreshold 5 --serviceAlertCooldownPeriod 7000 \\ --scriptPath fastapi-openapi-example/run.sh --pipelineType BASH Creating a microservice with autoscaling: $ sctl microservices create --gitServerName shakudo-dev-repo \\ --minReplicas 2 --maxHpaRange 5 Combining multiple flags for advanced microservice creation: $ sctl microservices create \\ --gitServerName shakudo-dev-repo \\ --jobName test-service-example-cli \\ --jobType basic-system \\ --scriptPath fastapi-openapi-example/run.sh \\ --pipelineType BASH \\ --subdomain test-subdomain \\ --notificationsEnabled \\ --notificationTargetIds 8ebc86f3-c76e-4ce7-99f9-666151923b0c,14960534-15dd-4ffd-8d46-2e00b2f55f36 \\ --healthCheckThreshold 2 --serviceAlertCooldownPeriod 2000 \\ --minReplicas 1 --maxHpaRange 3 \\ --port 8888 \\ --billingProjectName prod-bp \\ --serviceAccountName admin-sa \\ --parameter ENV1=VAL1 --parameter ENV2=VAL2 \\ --customSecretNames testing,pass-openai \\ --readme &quot;&gt; this is a testing readme\\n\\n### this is a header\\n&quot; Watch logs of a microservice # Watch logs for a microservice every 2 seconds watch sctl microservices logs --id 12345  ","version":"Next","tagName":"h3"},{"title":"General Usage​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sessions/sctl#general-usage","content":"$ sctl [COMMAND]  Use the help command for detailed information about a specific topic or command. Examples: # Display help for the microservices topic $ sctl microservices --help # Display help for the immediate-jobs topic $ sctl immediate-jobs --help  ","version":"Next","tagName":"h2"},{"title":"Version​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sessions/sctl#version","content":"The current version of the Shakudo CLI is 0.0.1. shakudo-cli/0.0.1 darwin-arm64 node-v18.18.0  ","version":"Next","tagName":"h2"},{"title":"Additional Notes​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sessions/sctl#additional-notes","content":"Ensure that the appropriate flags and parameters are provided when creating or managing microservices.Invalid inputs, such as incorrect pipelineType, will throw an error. This documentation provides an overview of the Shakudo CLI’s primary functionality, command usage, and examples to help users manage Shakudo deployments effectively. ","version":"Next","tagName":"h2"},{"title":"Jupyter Shared Sessions","type":0,"sectionRef":"#","url":"/shakudo-platform-core/sessions/shared_sessions","content":"","keywords":"","version":"Next"},{"title":"Features of Jupyter Shared Sessions​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/sessions/shared_sessions#features-of-jupyter-shared-sessions","content":"Real-Time Collaboration: Multiple users can simultaneously edit and execute cells in notebooks or modify files in real time.Shared Cursors: Visualize where collaborators are working, with disappearing username indicators to maintain focus on the document.Automatic Saving: All changes are saved automatically after a short interval, ensuring no data is lost.Conflict-Free Editing: No warning dialogs or conflicts when editing the same file—synchronization is seamless.Environment Sharing: Collaborators gain access to the same runtime environment, ensuring consistency in data and code execution.  ","version":"Next","tagName":"h2"},{"title":"Step-by-Step Guide on How to Create and Share a Session​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/sessions/shared_sessions#step-by-step-guide-on-how-to-create-and-share-a-session","content":"","version":"Next","tagName":"h2"},{"title":"1. Create a Session with jhub-basic environment config​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/sessions/shared_sessions#1-create-a-session-with-jhub-basic-environment-config","content":"Navigate to the Sessions panel and click on the create session button. Select the Basic environment config and click on the create session button. Wait for the session to start and initialize the JupyterLab server. Once the JupyterLab icon appears, click on it to open the session. ","version":"Next","tagName":"h3"},{"title":"2. Within JupyterLab Create a RTC python Notebook​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/sessions/shared_sessions#2-within-jupyterlab-create-a-rtc-python-notebook","content":"Click on the Python 3 ipykernel icon. Start developing. ","version":"Next","tagName":"h3"},{"title":"3. Create a shared link to invite other users​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/sessions/shared_sessions#3-create-a-shared-link-to-invite-other-users","content":"Click on the link icon in the top right corner of the screen. Copy the link and share it with your additional users. Alternatively, include the session token in the shared link by selecting the checkbox. To share your Session with non-admins, you will need to publish your Session in the Shakudo dashboard. ","version":"Next","tagName":"h3"},{"title":"4. Access a session via a shared link​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/sessions/shared_sessions#4-access-a-session-via-a-shared-link","content":"Log into the session if the token was not provided in the URL (default password is hyperhub). ","version":"Next","tagName":"h3"},{"title":"5. Start developing together​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/sessions/shared_sessions#5-start-developing-together","content":"View the cursor and highlighted text of the other user ","version":"Next","tagName":"h3"},{"title":"Integrating git into Sessions Jupyterlab","type":0,"sectionRef":"#","url":"/shakudo-platform-core/sessions/versionControl","content":"","keywords":"","version":"Next"},{"title":"Features​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#features","content":"All of the following features work under the hood to ensure that git commits are clean and allow for better integration between Jupyter and git. ","version":"Next","tagName":"h2"},{"title":"Merging notebooks with git​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#merging-notebooks-with-git","content":"Jupyter has often had a problem with handling merge conflicts, often causing notebooks to break. nbdev iincludes a merge driver that leaves conflicts in a state that is appropriate for Jupyter. It works in all git commands that use merge under the hood, including merge, pull, rebase, and stash. Here’s what the conflict looks like in Jupyter with nbdev’s merge driver:  For more information on the underlying functionality, read the following docs. ","version":"Next","tagName":"h3"},{"title":"Making commits with clean diffs​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#making-commits-with-clean-diffs","content":"Jupyter notebooks store a variety of metadata that tend to pollute diffs in pull requests and git histories that often cause merge conflicts. nbdev includes a hook that cleans up unnessesary metadata that reduces the presence of insignificant changes.  { &quot;cell_type&quot;: &quot;code&quot;, - &quot;execution_count&quot;: 1, + &quot;execution_count&quot;: 2, &quot;metadata&quot;: { &quot;hide_input&quot;: false }  vs { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {} }  For more details on functionality, read the following blog post. Tutorial ","version":"Next","tagName":"h3"},{"title":"1. Start a Session​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#1-start-a-session","content":"Navigate to the Sessions tab on the dashboardClick the + Start a Session button. You will see a dialog window to start a session like the image below.  Image: Choose the image type to use in the session. In this example, we are going to use the Basic image in the dropdown. For more information on Session Types and other configurations check out the Guide on Sessions. ImageURL: You can paste any image URL in the Image Url field. This will overwrite the Image above field that we have chosen and use the ImageURL instead. As we will be using the Basic image, we will be skipping this. Timeout: Choose the idle timeout for the session. Idle timeout is defined as the number of seconds from which the session has been continuously idling. The default is 15 minutes. Drive: Drive is the persistent volume that this session will use. Persistent volumes is a Kubernetes term, imagine it as a hard drive in a laptop. You can have multiple drives and manage your drives by clicking on the icon to the right of the Drive field. Select the drive you'd like to stick with for this session. ","version":"Next","tagName":"h2"},{"title":"2. Access the Session​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#2-access-the-session","content":"Once the Session is ready, you'll see a Jupyterlab icon among other options. Select the Jupyterlab option to begin using Jupyterlab.  ","version":"Next","tagName":"h2"},{"title":"3. Set up git​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#3-set-up-git","content":"Within the Session Jupyterlab, users are able to use a terminal application to enter Linux commands. Select the terminal application  ","version":"Next","tagName":"h2"},{"title":"3.1 Set up ssh access to GitHub repositories​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#31-set-up-ssh-access-to-github-repositories","content":"The standard method of connecting to GitHub securely is to set up Secure Shell Protocol (SSH) connections with GitHub when making any changes to a repository. This will only have to be done once per drive, as the portion necessary to establish connections will reside within the drive. If you have done this previously, feel free to move on to creating the git repository. Run the following command, substituting in your GitHub email. ssh-keygen -t ed25519 -C &lt;your_email@example.com&gt;  When prompted with &quot;Enter a file in which to save the key&quot;, feel free to press &quot;Enter&quot; to accept the default file location. If you have previously created SSH keys it may cause you to rewrite that other key, so feel free to chance the location of the new key. When prompted to type in a secure passphrase, feel free to enter in a passphrase of your choice. To avoid entering the passphrase every time you connect, you can securely save your passphrase in the SSH agent. Here's more information on how to work with the passphrase. Start the ssh-agent. eval &quot;$(ssh-agent -s)&quot;  Add your SSH private key to the ssh-agent. If you changed the name of your key, feel free to substitute out id_ed25519 with the name. ssh-add ~/.ssh/id_ed25519  Copy the SSH public key to your clipboard.  cat ~/.ssh/id_ed25519.pub # Then select and copy the contents of the id_ed25519.pub file # displayed in the terminal to your clipboard  In GitHub, in the upper-right corner of any page, click on your profile photo, then click on Settings.  In the &quot;Access&quot; section of the sidebar, click &quot;SSH and GPG keys&quot;. Click New SSH key. In the &quot;Title&quot; field, add a descriptive label for the key. In the &quot;Key&quot; field, paste your public key. For more details, read the following docs. ","version":"Next","tagName":"h3"},{"title":"3.2 Set up git repository​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#32-set-up-git-repository","content":"Create a new repository ","version":"Next","tagName":"h3"},{"title":"New repository​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#new-repository","content":"mkdir &lt;new directory&gt; cd &lt;new directory&gt; echo &quot;# New repository&quot; &gt;&gt; README.md git init git add README.md git commit -m &quot;first commit&quot; git branch -M main git remote add origin &lt;remote repository url starting with git@github.com&gt; git push -u origin main  or clone an existing repository within the current directory. ","version":"Next","tagName":"h3"},{"title":"Existing repository​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#existing-repository","content":"git clone &lt;remote repository url starting with git@github.com&gt; cd &lt;repo name&gt;  ","version":"Next","tagName":"h3"},{"title":"4. Verify that nbdev2 is installed​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#4-verify-that-nbdev2-is-installed","content":"nbdev 2.0+ is the package required to use the current functionality. All default Shakudo images currently have nbdev2. Run this command to determine whether you have nbdev 2.0+ pip show nbdev  If not, install by running pip install nbdev  Then check by running pip show nbdev  ","version":"Next","tagName":"h2"},{"title":"5. Set up nbdev git hooks​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#5-set-up-nbdev-git-hooks","content":"As mentioned, nbdev uses git hooks to clean up git commits and allow for version control compatibility. Within the base directory of the repo, type in the following: nbdev_install_hooks  The response after running this command should be Hooks are installed. Now you should be able to use git alongside Jupyter properly. For further support in installing nbdev git hooks, follow this tutorial. ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#example","content":"Here is an example of creating a Jupyter notebook file and seeing merge conflicts occur within the notebook. ","version":"Next","tagName":"h2"},{"title":"1. Verify that you are within the git repository within the graphical interface​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#1-verify-that-you-are-within-the-git-repository-within-the-graphical-interface","content":"The sample git repository has the name nbdev_test, so that is the directory that will be used.   ","version":"Next","tagName":"h3"},{"title":"2. Create a notebook through Jupyterlab​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#2-create-a-notebook-through-jupyterlab","content":" ","version":"Next","tagName":"h3"},{"title":"3. Commit the new notebook to git​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#3-commit-the-new-notebook-to-git","content":"From the base directory of the repo. git add . git commit -m &quot;Add blank notebook&quot;  ","version":"Next","tagName":"h3"},{"title":"4. Create a new branch from the current branch​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#4-create-a-new-branch-from-the-current-branch","content":"git checkout -b merge_branch_1  Where merge_branch_1 is a sample branch name. ","version":"Next","tagName":"h3"},{"title":"5. Enter text within the first cell and press save (CTRL/CMD+S)​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#5-enter-text-within-the-first-cell-and-press-save-ctrlcmds","content":" ","version":"Next","tagName":"h3"},{"title":"6. Commit the file​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#6-commit-the-file","content":"git add . git commit -m &quot;Add foo print&quot;  ","version":"Next","tagName":"h3"},{"title":"7. Checkout the parent branch​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#7-checkout-the-parent-branch","content":"git checkout main  Feel free to replace main with whatever branch merge_branch_1 was based on. ","version":"Next","tagName":"h3"},{"title":"8. Create another branch from the current/base branch​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#8-create-another-branch-from-the-currentbase-branch","content":"git checkout -b merge_branch_2  Where merge_branch_2 is a sample branch name. ","version":"Next","tagName":"h3"},{"title":"10. Enter differing text within the first cell and press save (CTRL/CMD+S)​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#10-enter-differing-text-within-the-first-cell-and-press-save-ctrlcmds","content":" ","version":"Next","tagName":"h3"},{"title":"11. Commit the file​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#11-commit-the-file","content":"git add . git commit -m &quot;Add bar print&quot;  ","version":"Next","tagName":"h3"},{"title":"12. Run a merge from merge_branch_1 and check for merge conflict in Jupyter​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#12-run-a-merge-from-merge_branch_1-and-check-for-merge-conflict-in-jupyter","content":"git merge merge_branch_1  You will need to re-open the Jupyter notebook to see the new update. You will now be able to see a merge conflict displayed cleanly in your notebook.  ","version":"Next","tagName":"h3"},{"title":"13. Resolve the merge conflict​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#13-resolve-the-merge-conflict","content":"Remove the diff lines &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt; merge_branch_1 along with any lines you'd like to remove. Press Save to save changes.  ","version":"Next","tagName":"h3"},{"title":"14. Commit merge conflict changes​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#14-commit-merge-conflict-changes","content":"Then from the terminal, type: git add . git commit  and type CTRL+X to exit the nano editor. ","version":"Next","tagName":"h3"},{"title":"Airflow","type":0,"sectionRef":"#","url":"/Shakudo-stack/CI_CD/airflow","content":"Airflow Coming soon","keywords":"","version":"Next"},{"title":"Prefect","type":0,"sectionRef":"#","url":"/Shakudo-stack/CI_CD/prefect","content":"Prefect Coming soon","keywords":"","version":"Next"},{"title":"Airbyte","type":0,"sectionRef":"#","url":"/Shakudo-stack/dataIngestion/airbyte","content":"Airbyte Coming Soon","keywords":"","version":"Next"},{"title":"DBT","type":0,"sectionRef":"#","url":"/Shakudo-stack/dataTransformation/dbt","content":"DBT Coming soon","keywords":"","version":"Next"},{"title":"DuckDB","type":0,"sectionRef":"#","url":"/Shakudo-stack/dataTransformation/duckdb","content":"DuckDB Coming soon","keywords":"","version":"Next"},{"title":"Cube.js","type":0,"sectionRef":"#","url":"/Shakudo-stack/datavisualization/cube","content":"Cube.js Coming soon","keywords":"","version":"Next"},{"title":"Streamlit","type":0,"sectionRef":"#","url":"/Shakudo-stack/datavisualization/streamlit","content":"Streamlit Coming soon","keywords":"","version":"Next"},{"title":"Superset","type":0,"sectionRef":"#","url":"/Shakudo-stack/datavisualization/superset","content":"Superset Coming soon","keywords":"","version":"Next"},{"title":"Dask","type":0,"sectionRef":"#","url":"/Shakudo-stack/distributedComputing/dask","content":"","keywords":"","version":"Next"},{"title":"Notebook_Common​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#notebook_common","content":"notebook_common is part of the Shakudo Platform Hyperplane API that contains convenience functions for Dask and pipeline jobs. It contains functions to manage Dask clusters, pipeline jobs, and Slack messages, and GraphQL operations. ","version":"Next","tagName":"h2"},{"title":"quickstart_dask()​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#quickstart_dask","content":"Use quickstart_dask to quickly spin up a Dask cluster using t-shirt sizes. Returns a tuple [Client, KubeCluster]. from hyperplane.notebook_common import quickstart_dask client, cluster = quickstart_dask( num_workers = 4, size = 'hyperplane-med-high-mem' )  Parameters Name\tType\tDescriptionnum_workers Required integer\tNumber of workers size Required string\tPre-configured worker pools   Pre-configured Worker pools The preconfigured worker pools are the following Name\tWorker Pool\tAllocatable cores\tAllocatable ramhyperplane-xs-high-mem\tPOOL_4_32\t3.5\t7.0 hyperplane-small\tPOOL_8_8\t7.0\t5.0 hyperplane-small-mid-mem\tPOOL_8_16\t7.5\t12.0 hyperplane-small-high-mem\tPOOL_8_64\t7.5\t58.0 hyperplane-med\tPOOL_16_16\t15.0\t12.0 hyperplane-med-mid-mem\tPOOL_16_32\t15.0\t27.0 hyperplane-med-high-mem\tPOOL_16_128\t15.0\t110.0 hyperplane-large\tPOOL_32_32\t28.0\t27.0 hyperplane-xxl-high-mem\tPOOL_96_768\t94.0\t675.0  ","version":"Next","tagName":"h2"},{"title":"initialize_dask_cluster()​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#initialize_dask_cluster","content":"Initialize a distributed DASK cluster. Returns a tuple [Client, KubeCluster]. You may use the returned client and cluster like any other dask cluster. from hyperplane.notebook_common import initialize_dask_cluster client, cluster = initialize_dask_cluster( num_workers:int=2, local_mode:bool=False, worker_spec_yaml:str=WORKER_SPEC_TEMPLATE_1_1, timeout:int=1200, nthreads:int=1, nprocs:int=15, ram_gb_per_proc:float=0.7, cores_per_worker:int=15, scheduler_deploy_mode:str=&quot;remote&quot;, dashboard_port:str=&quot;random&quot;, logging:str=&quot;quiet&quot; )   Parameters  Name\tType\tDescriptionnum_workers\tinteger\t(Default value: 2) Number of Dask worker nodes. local_mode\tbool\tWhether to use local cluster or distributed KubeCluster worker_spec_yaml\tstring\tA string YAML for cluster configs timeout\tinteger\tTime limit (seconds) for a scheduler to wait to connect before returning a timeout error nthreads\tinteger\tNumber of threads per worker in your cluster nprocs\tinteger\tNumber of processes per worker in your cluster ram_gb_per_proc\tfloat\tGB of Ram per process, per worker cores_per_worker\tinteger\tNumber of cores per worker scheduler_deploy_mode\tstring\tWhere to deploy the scheduler (remote in its own worker, or locally in jhub). Choose remote when the Dask graph dashboard_port\tstring\tChoose a port number for your dashboard, or leave as &quot;random&quot; to have a random port, which will not conflict logging\tstring\tLogging level for printouts when initializing. Available options are verbose or quiet. note The number of dask workers in the cluster will be the num_workers x num_procs. Shakudo platform will automatically choose the closest pool from the pre-configured node pool based on the combination of parameters specified. Example from hyperplane import notebook_common as nc client, cluster = nc.initialize_dask_cluster(num_workers=2)  from hyperplane import notebook_common as nc client, cluster = nc.initialize__dask_cluster( num_workers=2, nthreads=1, nprocs=15, ram_gb_per_proc=0.7, cores_per_worker=15 )   ","version":"Next","tagName":"h2"},{"title":"daskpool_candidates​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#daskpool_candidates","content":"Use daskpool_candidates when you'd like to access the list of available dask pools to choose from to spin up a Dask cluster. candidates = nc.daskpool_candidates candidates   ","version":"Next","tagName":"h2"},{"title":"get_dask_cluster()​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#get_dask_cluster","content":"Retrieve a Dask cluster. Use this function if there's a Dask cluster that's already spun up that you would like to connect. from hyperplane import notebook_common as nc client = nc.get_dask_cluster(&quot;dask_cluster_name&quot;) client   Parameters  Name\tType\tDescriptiondask_cluster_name\tstring\tName of Dask cluster To retrieve the Dask cluster name, navigate to the Ray &amp; Dask tab on the platform and click the copy button in the table column Cluster Name.   ","version":"Next","tagName":"h2"},{"title":"cluster.close() & client.close()​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#clusterclose--clientclose","content":"Use cluster.close() and client.close() to destroy or shut down a dask cluster after it is no longer needed to free up resources. The platform comes with an automatic garbage collection functionality - if you forget to close the cluster the platform will automatically close it after a few minutes of idle time. Starting a cluster and shutting it down: from hyperplane import notebook_common as nc client, cluster = nc.initialize_dask_cluster(num_workers=2) cluster.close() client.close()  Retrieving a forgotten Dask cluster and closing it: from hyperplane import notebook_common as nc client = nc.get_dask_cluster(&quot;dask-cluster-with-some-random-hash&quot;) cluster.close() client.close()   ","version":"Next","tagName":"h2"},{"title":"client.restart()​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#clientrestart","content":"Use client.restart whenever you want to clean up dask memory.  client.restart()  note Dask remembers every line of code that was run since initializing the cluster. If you'd like to edit a line of code after it's already been run once, then restart the dask client to ensure that the script runs smoothly. ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/Shakudo-stack/distributedComputing/overview","content":"","keywords":"","version":"Next"},{"title":"Dask​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#dask","content":"Dask is a flexible open source distributed framework for parallel computing. It has similar APIs to NumPy and Pandas, is an ideal choice for parallelizing NumPy, Pandas and List based code. Shakudo Platform comes with a number of useful APIs to make using Dask easy. See the Hyperplane API page for a full list. ","version":"Next","tagName":"h2"},{"title":"Dask Collections​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#dask-collections","content":"Dask collections are useful for large datasets because they support delayed tasks. We will explore three types— Dask bags, Dask dataframes, and Dask arrays. Dask bags​ Dask bags (synonymous with multisets) are unordered collections of immutable objects. Below are some common operations: Select records where: b.filter(lambda record: record['num_clicks'] &gt; 2).take(2)  ({'id': '01mz489cnkd', 'area': 'Aerial Alaska', 'num_clicks': 3, 'info': {'a_field': 0}}, {'id': '25z48t9cfaf', 'area': 'Bustling Birktown', 'num_clicks': 5, 'info': {'a_field': 1}})  Select one field: b.map(lambda record: record['area']).take(2)  ('Aerial Alaska', 'Bustling Birktown')  Aggregate the number of records in your bag: b.count().compute()  100000  Note that the .take(n) function will return the first n records from the bag, only in the first partition. For more info, see https://examples.dask.org/bag.html Dask dataframes​ Dask dataframes are collections of pandas dataframes. It can be used in cases where one pandas dataframe is too large to fit in memory and to speed up expensive computations by using multiple cores. To read multiple csvs, use the * or a list of files. Each file will be read into a separate partition. import dask.dataframe as dd df = dd.read_csv('2014-*.csv')  A common workflow is the following: Load large datasets from filesFilter to a subset of dataShuffle data to get an intelligent indexPerform queries or aggregations using the indices For more information on Dask dataframes, see https://docs.dask.org/en/latest/dataframe.html. ","version":"Next","tagName":"h3"},{"title":"Lazy calculations​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#lazy-calculations","content":"Dask operates with lazy collections, meaning operations on a collection are simply scheduled by the scheduler, but the actual calculation will not be triggered until explicitly called. At first, you will notice that dataframe operations or functions seem to happen almost instantaneously, but nothing will be calculated until one of the following is used: .persist().compute().load() .compute() will trigger a computation on the Dask cluster without returning anything. You can use this if some of your functions include saving to a location. .persist() will trigger a computation on the Dask cluster and store the results in ram. Use this sparingly, only if you need to use an intermediate collection, or after a computationally expensive operation such as index, groupby, etc. .load() will trigger a computation on the Dask cluster when you are working with Dask xarrays. For example, you can trigger all computations on your dataframe like the following: df = dask.df.read_parquet('file*.csv') df = df[['col_a', 'col_b']] df = df.drop_duplicates() df = client.persist(df)  ","version":"Next","tagName":"h3"},{"title":"Repartitioning​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#repartitioning","content":"After a few computations, your Dask df may need to be repartitioned, due to the partition size-number tradeoff. Partitions that are too large will cause out of memory errors, while too many partitions will incure a larger overhead time for the schedule to process. See more on best practices at https://docs.dask.org/en/latest/dataframe-best-practices.html. ","version":"Next","tagName":"h3"},{"title":"Split out​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#split-out","content":"Dataframe aggregation operations can get slow. Try to use split_out in aggregation operationg like groupbys to spread the aggregation work. ","version":"Next","tagName":"h3"},{"title":"Cheap vs. expensive computations​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#cheap-vs-expensive-computations","content":"Examples of fast and cheap computations: Element-wise ops (addition, multiplication)Row-wise operations and filtering: df[df.x &gt; 0]Joining Dask dfs along indexed fields, or joining with a one-partition Dask dfMax, min, count, common aggregations (df.groupby(df.x).y.max())isin: df[df.x.isin([1, 2, 3])]drop_duplicatesgroupby-apply on an index: df.groupby(['idx', 'x']).apply(myfunc) Examples of slow and expensive computations (for this reason, it is often recommended to use persist your data after these steps for stability): setting an index: df.set_index(df.x)groupby-apply on non-index fields: df.groupby(df.x).apply(myfunc)joining two dataframes along non-index columns ","version":"Next","tagName":"h3"},{"title":"File types​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#file-types","content":"parquet files​ Parquet is a columnar storage format for Hadoop, which enables parallel reading and writing, and is most useful for efficiently filtering a subset of fields in a Dask df. avro files​ Avro is a row-based storage format for Hadoop, which is most efficient if you intend to retrieve and use all fields or columns in the dataset. ","version":"Next","tagName":"h3"},{"title":"Saving to cloud storage​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#saving-to-cloud-storage","content":"Remember that new workers are spun up when use .initialize_cluster(), and they are destroyed on cluster.close(). This means you should ensure your intermediate and output files are saved in a cloud storage location that can be accessed outside of each node. This can be achieved through the following code example: gcp_project = YOUR_GCP_PROJECT gcs_client = storage.Client(project=gcp_project) bucket = gcs_client.get_bucket(bucket) blob = bucket.blob(yourfile) blob.upload_from_string(filename, content_type='application/x-www-form-urlencoded;charset=UTF-8')  ","version":"Next","tagName":"h3"},{"title":"Choosing Dask workers and specs​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#choosing-dask-workers-and-specs","content":"If you are aiming for a specific pool, ensure that nprocs x nthreads ≤ cores_per_worker ≤ the number of allocatable cores and nprocs x ram_gb_per_proc ≤ allocatable ram. For example, if you would like to use a DASK_POOL_16_16 worker, you may want to choose the following cluster initialization nprocs = 5 nthreads = 3 ram_gb_proc = 2.4  Rules of Thumb For a job with aggregation (data transfer) choose a setup with a minimum of 10x the data size. Worker memory usage is about 10% initially. If at any point any worker’s memory usage exceeds 75%, the job is very likely to fail (see Worker freeze policies). For a job that consists of only parallel-friendly operations (no sorting, shuffling, or moving large chunks of data), use more CPU (for example 16_16, 32_32). Otherwise use more memory (for example 16_128). For a job that requires both huge data and a large number of tasks, split it into multiple jobs to avoid errors. For example, an optimized setting for 1TB group-by job is to split into 10 pieces (100GB each) and use 24 of 32_32 nodes. You can further convert the piece indicator to a parameter like chunk_id and convert the code into a pipeline job, then run 10 pipeline jobs concurrently to save more time. Use the steps below to estimate how many nodes and workers you will need: Check data size (uncompressed). For example, 100GBChoose operation type to find a multiplier of memory: light (x4), medium (x8), heavy(x48). Multiply your data size from step 1 by this multiplier. For example, a group-by will be medium, which requires 100G x 8 ~ 800GB total memoryUse the number of tasks (heavy vs. light) to determine the number of nodes. If the sequence of operations has many tasks, (computationally heavy), use 32_32. Otherwise use 16_128. Multiply your required memory from step 2 by 32 or 128 depending on computation load. For example, 800GB/32GB = 25 nodes, or 800GB/128GB = 8 of 16_128 nodes.  At this point, you should have an approximate Dask pool spec and number of workers. Add-on step: Setup automatic retry if in pipeline mode. Sometimes pipelines error out when spinning up nodes, or HTTP error, canceled error. These can be fixed by retrying. Worker freeze policies​ Below are the defaults for worker memory limits and actions to avoid memory blowup. distributed: worker: memory: target: 0.60 # target fraction to stay below spill: 0.70 # fraction at which we spill to disk pause: 0.80 # fraction at which we pause worker threads terminate: 0.95 # fraction at which we terminate the worker  ","version":"Next","tagName":"h3"},{"title":"Examples​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#examples","content":"Pandas to Dask.dataframe​ Below are some common examples of converting pandas operations to dask-friendly code. Column to_datetime​ # pandas to_datetime import pandas as pd df1['date'] = pd.to_datetime(df1['date'])  # dask to_datetime from dask import dataframe as dd df1['date'] = dd.to_datetime(df1['date'])  Dataframe groupby​ # pandas groupby df_metrics = df_metrics.groupby(pd.Grouper(timeframe='1day', closed='right',label='right') ).agg({'id':pd.Series.nunique, 'num_entries':'sum', 'total_runs':'sum')  # dask is exactly the same df_metrics = df_metrics.groupby(pd.Grouper(timeframe='1day', closed='right',label='right') ).agg({'id':pd.Series.nunique, 'num_entries':'sum', 'total_runs':'sum')  Get dummies​ The following example features a more complicated groupby; get_dummies will explode in dimension with large amount of data (i.e. possibly explode the data into many columns) # pandas version do dummies first then groupby time interval to get aggregation per time interval dfr= pd.get_dummies(df,['col_a','col_b','col_c']) dfr = dfr.merge(df['date'],right_index=True,left_index=True) dfr = dfr.sort_values(by='date') dfr = dfr.groupby(pd.Grouper(timeframe='1day',closed='right',label='right')).sum()  # dask version do value counts instead of getting dummies, and do pivot after groupby def agg_func(df: pd.DataFrame, timeframe: str, ts_col: str, sec_id: str, target_col:str) -&gt; pd.DataFrame: &quot;&quot;&quot; function that group data by required timeframe for one target column df: dataframe to be aggregated timeframe: aggregation timeframe ts_col: column name of the index sec_id: column name of a secondary id target_col: target column name for processing e.g. col_a &quot;&quot;&quot; df = df.groupby([sec_id, ts_col,target_col]).size().reset_index().set_index(ts_col) df.columns = [sec_id, target_col,'count'] df_agg = df.groupby(pd.Grouper(timeframe=timeframe, closed='right', label='right')).apply( lambda x: x.groupby(target_col).agg({'count': 'sum'})).reset_index() return df_agg meta_df = agg_func(df.head(10), timeframe, ts_col, sec_id, target_col).dtypes.to_dict() df = df.map_partitions(agg_func, timeframe, ts_col, sec_id, target_col, meta = meta_df) df.columns = [ts_col, target_col, 'count'] # further groupby session_ts and event as there will be duplicates among partition df = df.groupby([ts_col, target_col]).agg({'count': 'sum'}) # create pivot table for end results df = df.reset_index() df = df.pivot_table(values=&quot;count&quot;, index=ts_col, columns=target_col) df.columns = [target_col+'_'+i for i in list(df.columns)]  Dask map​ The following example optimizes a function that reads a list of files one by one. ## this snippet reads a list of files one by one import xarray as xr import gcsfs fs = gcsfs.GCSFileSystem(project='myproject', token=None) files_list = ['file1', 'file2', 'file3', 'file4'] gcsmap = gcsfs.mapping.GCSMap(f'gs://my-bucket/{files_list[0]}', gcs=fs) Glob = xr.open_zarr(store=gcsmap).load() ## add other datasets sequentially for filepath in files_list[1:]: gcsmap = gcsfs.mapping.GCSMap(f'gs://my-bucket/{filepath}', gcs=fs) ds = xr.open_zarr(store=gcsmap).load() Glob = xr.merge([Glob, ds], compat=&quot;no_conflicts&quot;, combine_attrs = &quot;no_conflicts&quot;)  ## in dask, create a function to read one file, then use client.map the function and list of files import xarray as xr import gcsfs fs = gcsfs.GCSFileSystem(project='myproject', token=None) files_list = ['file1', 'file2', 'file3', 'file4'] def read_files(gsfilepath): gcsmap = gcsfs.mapping.GCSMap(f&quot;gs://my-bucket/{gsfilepath}&quot;, gcs=fs) ds = xr.open_zarr(store=gcsmap).load().persist() return ds dss = client.map(read_files, files_list) ds_list = client.gather(dss) print(len(ds_list)) # output: 4 Glob = xr.merge(ds_list, compat=&quot;no_conflicts&quot;, combine_attrs = &quot;no_conflicts&quot;)  Parallel training and preprocessing on dask​ Sklearn training can be easily converted to distributed training with dask using joblib. import joblib with joblib.parallel_backend('dask'): grid_search.fit(X, y)  Many sklearn preprocessing modules (e.g. OneHotEncoder, Categorize, StandardScaler, etc.), models (NaiveBayes, xgboost, clustering, etc.), and model selection utilities (KFold, train_test_split, etc.) have dask equivalents. See https://ml.dask.org/index.html for full list of equivalents. ","version":"Next","tagName":"h3"},{"title":"Ray​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#ray","content":"Ray is an distributed frameworks open source project that has a more support for deep learning and reinforcement learning. It has a rich set of libraries and integrations built on a flexible distributed execution framework, is ideal choice for parallelizing model training and hyper-parameter tuning. ","version":"Next","tagName":"h2"},{"title":"Spark​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#spark","content":"Apache Spark is an open source platform for large-scale SQL, batch processing, stream processing, and machine learning. PySpark is the python API for Spark and in the recent releases PySpark adopted more Pandas like APIs. Spark is great for data processing especially for the computations that involves shuffling joining type of operations. Shakudo Platform provides simple APIs to use Spark on distributed Ray clusters using RayDP. RayDP combines your Spark and Ray clusters, making it easy to do large scale data processing using the PySpark API and seamlessly use that data to train your models using TensorFlow and PyTorch. ","version":"Next","tagName":"h2"},{"title":"Initializing a distributed Ray cluster for Spark​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#initializing-a-distributed-ray-cluster-for-spark","content":"Initialize a distributed Ray cluster as usual using the following: from hyperplane.ray_common import initialize_ray_cluster ray_cluster = initialize_ray_cluster( num_workers = 4, cpu_core_per_worker = 15, ram_gb_per_worker = 12 )  num_workers (int) is the number of Ray nodes to be initialized cpu_core_per_worker (int) is the number of CPU cores in each Ray node ram_gb_per_worker (float) is the memory size in GB for each Ray node Read more about Ray and Ray on Shakudo Platform. ","version":"Next","tagName":"h3"},{"title":"Start a Spark session​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#start-a-spark-session","content":"spark = raydp.init_spark( 'example', num_executors=2, executor_cores=4, executor_memory='4G' )  ","version":"Next","tagName":"h3"},{"title":"Use PySpark​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#use-pyspark","content":"Once the Spark session is initialized, you can use pyspark as ususal from here on. The latest RayDP supports PySpark 3.2.0+, which provides simple Pandas-like APIs. import pyspark.pandas as pd df = pd.read_csv(&quot;data.csv&quot;)  ","version":"Next","tagName":"h3"},{"title":"Shutdown a Ray cluster​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#shutdown-a-ray-cluster","content":"After computation is finished, it's a good idea to shutdown the distributed cluster and release the resources back to the node pool. If any Ray nodes are left hanging, Shakudo Platform's garbage collection function will also automatically shutdown the Ray workers when the Jupyter session or job is finished. You can add this line to the end of your code to shutdown the Ray nodes. stop_ray_cluster(ray_cluster)  ","version":"Next","tagName":"h3"},{"title":"RAPIDS​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#rapids","content":"Rapids is a suite of open source libraries and APIs for doing data science on GPUs. Rapids can speed up common computation by 50x and has similar APIs to Pandas, NumPy and Scikit-learn and support multi-GPU scale up. They are very useful in significantly speed up long-running preprocessing loads. ","version":"Next","tagName":"h2"},{"title":"Get started​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#get-started","content":"Start by spinning up a Session with the GPU session type. Initialize cluster with GPUs On the Sessions GPU image, you can scale up a Dask cluster with GPUs by adding ngpus=1 to the cluster initialization. client, cluster = nc.initialize_cluster( nprocs=1, nthreads=8, ram_gb_per_proc=7, cores_per_worker=2, num_workers = 2, ngpus = 1, scheduler_deploy_mode=&quot;local&quot; )  Once the Dask cluster is spun up use the RAPIDS library by import importing relevant packages. For example dask_cudf df = dask_cudf.read_csv(file_path, assume_missing=True)  ","version":"Next","tagName":"h3"},{"title":"Ray","type":0,"sectionRef":"#","url":"/Shakudo-stack/distributedComputing/ray","content":"","keywords":"","version":"Next"},{"title":"Ray_Common​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#ray_common","content":"ray_common is part of the Shakudo Platform Hyperplane API that contains convenience functions to manage Ray clusters. We support extensions to the basic Ray framework by supporting Ray Tune, Ray Spark, Ray with RAPIDS, and more. ","version":"Next","tagName":"h2"},{"title":"quickstart_ray()​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#quickstart_ray","content":"Use quickstart_ray to quickly spin up a Ray cluster using t-shirt sizes (Sizes are the same as quick start for Dask clusters). from hyperplane import ray_common as rc ray_cluster = rc.quickstart_ray( num_workers = 4, size = 'hyperplane-med-high-mem' )  Parameters Name\tType\tDescriptionnum_workers Required integer\tNumber of workers size Required object\tPre-configured worker pools   ","version":"Next","tagName":"h2"},{"title":"initialize_ray_cluster()​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#initialize_ray_cluster","content":"Initialize a distributed Ray cluster with ease and more customizability. You can also run this function to clean up the Ray nodes and re-initialize. from hyperplane import ray_common as rc ray_cluster = rc.initialize_ray_cluster( num_workers = 4, cpu_core_per_worker = 4, ram_gb_per_worker = 4, n_gpus = 0 )   Parameters  Name\tType\tDescriptionnum_workers\tinteger\t(Default value: 2) Number of Ray nodes to be initialized cpu_core_per_worker\tinteger\tNumber of CPU cores in each Ray node ram_gb_per_worker\tfloat\tMemory size in GB for each Ray node n_gpus\tinteger\tNumber of Nvidia GPUs in each Ray node (if n_gpus &gt; 0, cpu_core_per_worker and ram_gb_per_worker are ignored) use_existing\tboolean\t(Default: use_existing = False) Whether to connect to/ reinitialize existing Ray cluster or spin up a new one note If you are aiming for a specific pool, ensure your cpu_core_per_worker = the number of allocatable cores and ram_gb_per_worker = the allocatable ram. For example, if you would like to use a POOL_16_16 worker, you may want to use the following cluster initialization. from hyperplane import ray_common as rc ray_cluster = rc.initialize_ray_cluster( num_workers = 4, cpu_core_per_worker = 15, ram_gb_per_worker = 12 )   ","version":"Next","tagName":"h2"},{"title":"stop_ray_cluster()​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#stop_ray_cluster","content":"Use stop_ray_cluster to shutdown a Ray cluster. After computation is finished, it's a good idea to shutdown the distributed cluster and release the resources back to the node pool. If any Ray nodes are left hanging, Shakudo Platform's garbage collection function will also automatically shutdown the Ray workers when the Session or job is finished. from hyperplane import ray_common as rc rc.stop_ray_cluster(ray_cluster)   Parameters  Name\tType\tDescriptionray_cluster Required object\tRay cluster to shutdown  ","version":"Next","tagName":"h2"},{"title":"get_ray_cluster()​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#get_ray_cluster","content":"Reconnect to a Ray cluster by using the get_ray_cluster to retrieve the cluster. You can use this function if you've already spun up a Ray cluster and want to connect to the same cluster (for example: in another notebook in the same session). This function will connect to an existing cluster. There are two ways to reconnect to Ray clusters. from hyperplane import ray_common as rc rc.get_ray_cluster(extra_workers = 1)   Parameters  Name\tType\tDescriptionextra_workers\tinteger\tAdds nodes to your existing cluster (Default: extra_workers = 0) The nodes that are added to the cluster will be of the same specification as the original cluster. There are two ways to reconnect to Ray clusters. The method using the function get_ray_cluster() is the simpler and recommended way. You can also use the initialize_ray_cluster() to accomplish the same. Note, the arguments for cpu_core_per_worker and ram_gb_per_worker must be the same as when you initialized the cluster originally. from hyperplane import ray_common as rc ray_cluster = rc.initialize_ray_cluster( num_workers = 0, cpu_core_per_worker = 15, ram_gb_per_worker = 12, use_existing = True )   ","version":"Next","tagName":"h2"},{"title":"find_ray_workers​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#find_ray_workers","content":"Use find_ray_workers() function to see if there are any Ray workers already spun up. Returns a list of Ray workers running. from hyperplane import ray_common as rc rc.find_ray_workers()  ","version":"Next","tagName":"h2"},{"title":"Apache Spark","type":0,"sectionRef":"#","url":"/Shakudo-stack/distributedComputing/spark","content":"Apache Spark Coming soon","keywords":"","version":"Next"},{"title":"FastAPI","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelServing/fastapi","content":"FastAPI Coming soon","keywords":"","version":"Next"},{"title":"Flask","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelServing/flask","content":"Flask Coming soon","keywords":"","version":"Next"},{"title":"TensorFlow Serving","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelServing/tensorflowserving","content":"TensorFlow Serving Coming soon","keywords":"","version":"Next"},{"title":"TorchServe","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelServing/torchserve","content":"TorchServe Coming soon","keywords":"","version":"Next"},{"title":"NVIDIA Triton","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelServing/triton","content":"NVIDIA Triton The Shakudo Platform comes with a build-in NVIDIA Triton Inference Server that simplifies the deployment of AI models at scale in production. Triton is an open-source inference serving software that lets teams deploy trained AI models from any framework (TensorFlow, NVIDIA® TensorRT®, PyTorch, ONNX Runtime, or custom) from local storage or cloud platform on any GPU- or CPU-based infrastructure (cloud, data center, or edge). To serve your model with the Triton server, you need to upload your model to the triton server model repository and write a client file. The default path of the triton model repository is {your_cloud_bucket}/triton-server/model-repository/. The official Triton client examples will help you with different client files for popular machine learning tasks such as image recognition and NLP. Please find a simple App in the Shakudo example repository that serves an image recognition model.","keywords":"","version":"Next"},{"title":"MLFlow","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelTracking/mlflow","content":"MLFlow Coming soon","keywords":"","version":"Next"},{"title":"Weights and Biases","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelTracking/w&b","content":"Weights and Biases Coming soon","keywords":"","version":"Next"},{"title":"Grafana","type":0,"sectionRef":"#","url":"/Shakudo-stack/monitoring/grafana","content":"Grafana Coming soon","keywords":"","version":"Next"},{"title":"Slack Alerts","type":0,"sectionRef":"#","url":"/Shakudo-stack/monitoring/slack","content":"Slack Alerts Slack alerts are available for failed or timeout jobs. Slack messages are sent when all retries have failed. To get Slack alerts set up you must contact our customer success team. Many teams set up a dedicated Slack channel to send alerts to. Wherever you would like the alerts sent, once you have the channel setup follow the steps below (for more detailed guide by Slack on getting a webhook URL check this page) Create a new Slack app in the workspace you want to the Shakudo Platform messages to be postedFrom Features page, toggle on Activate Incoming WebhooksClick Add New Webhook to WorkspacePick a channel that you would like the notifications in, then click AuthorizeSend the Shakudo customer success team your Slack Incoming Webhook URL to start getting job failure notifications. The Shakudo Slack bot will post a message for any errored or timed out jobs. The message will include: Job nameJob IDTimestamp of failure or timeoutReason for failureLink to Grafana logs for the jobA snippet of the specific error message A preview sample message:","keywords":"","version":"Next"},{"title":"Milvus","type":0,"sectionRef":"#","url":"/Shakudo-stack/vectorStores/milvus","content":"","keywords":"","version":"Next"},{"title":"Connections​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#connections","content":"A milvus connection must be established before further operations can be performed. The alias names the connection for future reference. Functions that use the connection will typically have a using parameter with a default value of 'default', so opening a connection with alias='default' allows us to operate other pymilvus facilities while omitting the connection name. ","version":"Next","tagName":"h2"},{"title":"Creating a connection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#creating-a-connection","content":"Connections are created with the pymilvus connections module. from pymilvus import connections connections.connect( alias=&quot;default&quot;, host=os.environ['MILVUS_HOST'], port=os.environ['MILVUS_PORT'] )  For more details on the connection parameters, see the official pymilvus documentation ","version":"Next","tagName":"h3"},{"title":"Closing a connection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#closing-a-connection","content":"Since there is no management object for Milvus connections, they must be released explicitly through the connections manager: connections.disconnect(&quot;default&quot;)  ","version":"Next","tagName":"h3"},{"title":"Databases​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#databases","content":"It is optionally possible to create Databases, which allows setting user permissions ranging over a set of collections. Details on database management are available in the milvus documentation ","version":"Next","tagName":"h2"},{"title":"Collections​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#collections","content":"Milvus data is stored in collections, which have to be loaded in memory before they can be searched against. Loading is not necessary when filling the collection, however. ","version":"Next","tagName":"h2"},{"title":"Creating a Collection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#creating-a-collection","content":"Milvus can operate in schema or schemaless mode depending on how the collection is created. Simply set allow_dynamic_fields=True to enable schemaless. from pymilvus import Collection, FieldSchema, CollectionSchema, DataType MAX_TITLE = 512 MAX_TEXT = 1024 MAX_VEC = 384 NAME = &quot;WikiHow&quot; whschema = CollectionSchema( fields=[ FieldSchema(name=&quot;pk&quot;, dtype=DataType.INT64, is_primary=True, auto_id=True), FieldSchema(name=&quot;title&quot;, dtype=DataType.VARCHAR, max_length=65535, default_value=&quot;&quot;), FieldSchema(name=&quot;text&quot;, dtype=DataType.VARCHAR, max_length=65535, default_value=&quot;&quot;), FieldSchema(name=&quot;vector&quot;, dtype=DataType.FLOAT_VECTOR, dim=384, description=&quot;embedding vector&quot;) ], enable_dynamic_fields=False, description=&quot;WikiHow collection&quot; ) whcollection = Collection( name=NAME, schema=whschema, consistency_level=&quot;Session&quot; )  As for connections, the official pymilvus documentation provides more extensive details. Note that the field size limits are in bytes and depend on the encoding used in milvus, it is not based on character count for VARCHAR. The list of available datatypes is available here. Importantly, the primary key may be either INT64 or VARCHAR and vectors can be either FLOAT_VECTOR or BINARY_VECTOR. The consistency level of the collection is discussed further in the Consistency article at milvus.io. Briefly, consistency_level=&quot;Session&quot; is a good default which means that queries will always happen after reads in our current session, even though they could happen before writes from other sessions are actualized. By comparison, Strong consistency ensures queries will always happen after all writes are completed. Eventually is the weakest consistency level and will process reads immediately, against whatever values are available in the replica at the time. ","version":"Next","tagName":"h3"},{"title":"Inserting data in a collection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#inserting-data-in-a-collection","content":"Given a collection, its insert function can be used to insert a batch of data. If using an auto_id key, the key field should be elided from the input. The argument to insert is a list of lists of field values, positionally ordered as in the schema, such as the following example: def insert_data(data): vecs = embed_documents([d['title'] for d in data]) entries = [[], [], []] for i in range(len(data)): entries[0].append(data[i]['title']) entries[1].append(data[i]['text']) entries[2].append(vecs[i]) whcollection.insert(entries)  Note that if a Milvus worker crashes (e.g. OOM) during operations, although Milvus features redundancy and a second node will come online to keep smooth operations, the default timeout value (in the insert function) may be too low and may cause failure. Increasing it to a much larger value will allow the process to keep running across a worker crash. Milvus will not finalize an insertion (i.e. &quot;seal a segment&quot;) unless enough data has been inserted since the last sealed segment. To force Milvus to seal a segment, it is important to flush the collection: whcollection.flush()  ","version":"Next","tagName":"h3"},{"title":"Creating an index​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#creating-an-index","content":"Bruteforce searches against vectros in the database can be very slow. Setting up an index can drastically speed up the search. whcollection.create_index(field_name=&quot;vector&quot;, index_params={&quot;metric_type&quot;: &quot;L2&quot;, &quot;index_type&quot;: &quot;IVF_FLAT&quot;, &quot;nlist&quot;: &quot;1024&quot;})  In the above example, we have created an index on the field named vector with a flat index using an inverted file, a maximum of 1024 clusters, and an L2 metric. More details about how to parameterize index creation can be found at this link Milvus also supports creating indexes on (and searching against) scalar data (possibly in combination with the vector search). ","version":"Next","tagName":"h3"},{"title":"Referring to an existing collection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#referring-to-an-existing-collection","content":"An existing collection on the 'default' connection can be loaded with a simple NAME = &quot;WikiHow&quot; whcollection = Collection(NAME)  ","version":"Next","tagName":"h3"},{"title":"Loading a collection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#loading-a-collection","content":"Collections cannot be queried against unless they are loaded first. This is simply achieved as follows: whcollection.load()  ","version":"Next","tagName":"h3"},{"title":"Releasing a collection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#releasing-a-collection","content":"The collection will stay loaded until it is released, either programmatically or through Attu. whcollection.release()  ","version":"Next","tagName":"h3"},{"title":"Search​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#search","content":"Search is conducted on a loaded collection as follows: hits = whcollection.search( [embed_query(what)], # Vector for the query anns_field=&quot;vector&quot;, # Name of the field to search against param={'metric_type': 'L2', # Search params... 'offset': 0, 'params': {'nprobe': 1} }, limit=1, output_fields=['text', 'title']) # Fields to retrieve  The full documentation for the search parameters can be found here. In the above example, we obtain the top search result provided the embeddings for the data to search for. Milvus also supports filter expressions (discribed in the documentation). The param field relates to the index(es) defined on the collection. A consistency_level can also be specified for the query. The hits returned by a Milvus search contains a list of hits as specified by the search parameters for each input vector. Since we provided a single input vector in this case, we can obtain more details about the hits corresponding to this vector as follows: query_hits = hits[0] top_query_hit = query_hits[0] print(f&quot;Title: {top_query_hit.entity.get('title')}&quot;) print(f&quot;Text: {top_query_hit.entity.get('text')}&quot;) print(f&quot;Distance between query embedding and document embedding: {top_query_hit.distance}&quot;)  Since we specified that we only wanted the top hit, we only need to care about the first (i.e. only) hit returned for the first (once again, only) input vector in our search. In the above, we print out the fields retrieved from the search as specified in output_fields in our call, and the distance between the embedding we used to search the database and the document's embedding. ","version":"Next","tagName":"h2"},{"title":"Query​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#query","content":"Milvus can also do scalar searches, termed &quot;query&quot;. For details, see the Milvus documentation on Query ","version":"Next","tagName":"h2"},{"title":"Create a React App","type":0,"sectionRef":"#","url":"/tutorials/buildareactapp","content":"","keywords":"","version":"Next"},{"title":"1. Prepare your environment​","type":1,"pageTitle":"Create a React App","url":"/tutorials/buildareactapp#1-prepare-your-environment","content":"Start a Session with the NodeJs imageOpen your sessions and check your node version using: node —versionInstall npx using npm install npxCreate a React app using npx create-react-app my-react-app ","version":"Next","tagName":"h2"},{"title":"2. Create a pipeline YAML​","type":1,"pageTitle":"Create a React App","url":"/tutorials/buildareactapp#2-create-a-pipeline-yaml","content":"Create a .yaml file to list your steps. You can start with adding the following template to react_pipeline.yaml: pipeline: name: &quot;Example pipeline&quot; tasks: - ...  Add a Bash script step by adding the following block to your YAML. The Bash script should be runnable with bash [bash_script_path]. This will install the dependencies and start your Node server:  - name: &quot;[another_step_name]&quot; type: &quot;bash script&quot; bash_script_path: &quot;[sh/file/relative/to/top/level/of/repo.sh]&quot;  An example bash script to start your React application is: PROJECT_DIR=&quot;$(cd -P &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot; cd &quot;$PROJECT_DIR&quot; yes Y | curl -sL https://deb.nodesource.com/setup_14.x | bash - apt update apt install nodejs npm install npm start  ","version":"Next","tagName":"h2"},{"title":"3. Edit your package.json to host your React app​","type":1,"pageTitle":"Create a React App","url":"/tutorials/buildareactapp#3-edit-your-packagejson-to-host-your-react-app","content":"When you want to host a React application on the Shakudo platform you have to specify the homepage of your application. This will be the root domain plus the the prefix which will host your React application. You should also set your host as 0.0.0.0 and the port as 8787 on your start script, and turn of VS Rewrite. An example package.json file: { &quot;name&quot;: &quot;my-react-app&quot;, &quot;version&quot;: &quot;0.1.0&quot;, &quot;private&quot;: true, &quot;homepage&quot;: &quot;https://[CLUSTER_NAME].hyperplane.dev/[my-app]/&quot;, ... &quot;scripts&quot;: { &quot;start&quot;: &quot;HOST=0.0.0.0 PORT=8787 react-scripts start&quot;, &quot;build&quot;: &quot;react-scripts build&quot;, &quot;test&quot;: &quot;react-scripts test&quot;, &quot;eject&quot;: &quot;react-scripts eject&quot; }, ... }  note The [my-app] prefix should match the dashboard URL prefix when creating your service in step 4 ","version":"Next","tagName":"h2"},{"title":"4. Create your React App on the dashboard​","type":1,"pageTitle":"Create a React App","url":"/tutorials/buildareactapp#4-create-your-react-app-on-the-dashboard","content":"Use the Services tab on the dashboard to start your React application Set the pipeline YAML path and the endpoint for your React app. The endpoint should be the same as the one specified in previous step. Also turn off the Virtual Service Path Rewrite. Then click Create on the top right corner. You React App should appear in the services tab in the dashboard. Important notes when cloning other git repositories Exclude the node_modules folder when committing your code to your Shakudo repositoryRemove the package-lock.json to avoid mismatching packages.Ensure that your bash script can start your React app when your run it on your session.  You can use our GraphQL mutation query in the GraphQL playground to create your React App Service. Copy the GraphQL mutation created on the left handside of the service creation dialogue or simply copy the code block below. mutation { createShakudoService( jobName: &quot;my-react-app&quot;, maxRetries: 2, urlPrefix: &quot;my-app&quot;, jobType: &quot;basic&quot;, pipelineYamlPath: &quot;my-react-app/react_pipeline.yaml&quot;, defaultCommands: true, gitInit: true, vsRewrite: false, parameters: { create: [ ]} ) { id jobName dashboardPrefix parameters { key value } noGitInit noHyperplaneCommands } }  Open the GraphQL playground from the dashboard and paste the code above into the lefthand side and press the play button. Note the the urlPrefix should match the dashboard URL prefix set in your package.json file as the homepage of your application. ","version":"Next","tagName":"h2"},{"title":"GraphQL","type":0,"sectionRef":"#","url":"/shakudo-platform-core/adminSettings/graphql","content":"","keywords":"","version":"Next"},{"title":"Get Sessions​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#get-sessions","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description","content":"Retrieves a list of Sessions query hyperhubSessions($limit: Int!, $email: String, $status: String, $imageType: String) { hyperHubSessions(orderBy:{startTime: desc}, take: $limit, where: { hyperplaneUserEmail: {equals: $email}, imageType: {equals: $imageType}, status: {equals: $status}, }) { id hyperplaneUserEmail status imageType jLabUrl notebookURI estimatedCost resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime } countHyperHubSessions }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables","content":"{ &quot;limit&quot;: 10, &quot;email&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;in progress&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters","content":"Field\tType\tDescriptionlimit\tInt!\tThe maximum number of records to show in the result. (required) email\tString\tShakudo platform user email for the user who created the session imageType\tString\tName of the Shakudo platform EC. For example, &quot;basic&quot; status\tString\tUnderlying Kubernetes job status ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type","content":"Array of HyperHubSessions ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response","content":"{ &quot;data&quot;: { &quot;hyperHubSessions&quot;: [ { &quot;id&quot;: &quot;49475b67-3f8f-43c1-9f42-7b2175d1e679&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;in progress&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;jLabUrl&quot;: &quot;client.hyperplane.dev/jupyterlabUrl/&quot;, &quot;notebookURI&quot;: &quot;ssh demo-pvc-entry@demo.dev&quot;, &quot;estimatedCost&quot;: null, &quot;resourceCPUlimit&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;gpuRequest&quot;: null, &quot;startTime&quot;: &quot;2023-07-05T16:25:45.676Z&quot; } ], &quot;countHyperHubSessions&quot;: 22 } }  ","version":"Next","tagName":"h3"},{"title":"Create Session​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#create-session","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-1","content":"Creates a Session ","version":"Next","tagName":"h3"},{"title":"Creating using createHyperHubSession parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#creating-using-createhyperhubsession-parameters","content":" query GetHyperplaneUserId($hyperplaneUserEmail: String!){ hyperplaneUsers(where: {email: {equals: $hyperplaneUserEmail}}) { id email } } # billingProjectName optional query GetBillingProjectId($billingProjectName: String){ billingProjects(where: {name: {equals: $billingProjectName}}) { id name } } # userPvcName and displayName optional query GetUserPvcId($userPvcName: String, $displayName: String){ userPvcs(where: { pvcName: {equals: $userPvcName}, displayName: {equals: $displayName} }) { id pvcName displayName } } mutation createSession( $imageType: String! $hyperplaneUserId: String! $hyperplaneUserEmail: String! $timeout: Int! $collaborative: Boolean! $imageHash: String! $userPvcName: String = &quot;&quot; $userPvc: UserPvcCreateNestedOneWithoutHyperHubSessionInput $billingProjectId: String! ) { createHyperHubSession( data: { imageType: $imageType timeout: $timeout collaborative: $collaborative imageHash: $imageHash group: &quot;&quot; hyperplaneUser: { connect: { id: $hyperplaneUserId } } billingProject: { connect: { id: $billingProjectId } } userPvc: $userPvc userPvcName: $userPvcName hyperplaneUserEmail: $hyperplaneUserEmail } ) { id hyperplaneUserEmail status imageType jLabUrl estimatedCost resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime completionTime timeout group billingProjectId podSpec } }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-1","content":"Retrieve $hyperplaneUserId using GetHyperplaneUserId and $billingProjectId using GetBillingProjectId Default Drive { &quot;collaborative&quot;: false, &quot;imageType&quot;: &quot;basic&quot;, &quot;imageHash&quot;: &quot;&quot;, &quot;timeout&quot;: 900, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;hyperplaneUserId&quot;: &quot;93c6c00a-14b7-4cf7-845d-70d9e779b2cd&quot;, # From GetHyperplaneUserId &quot;billingProjectId&quot;: &quot;8359f1f9-2eca-465b-9ac5-7cdb0e97e73f&quot; # From GetBillingProjectId }  Custom Drive { &quot;collaborative&quot;: false, &quot;imageType&quot;: &quot;basic&quot;, &quot;imageHash&quot;: &quot;&quot;, &quot;timeout&quot;: 900, &quot;userPvcName&quot;: &quot;demo-user-pvc-name&quot;, &quot;displayName&quot;: &quot;demo drive&quot;, &quot;hyperplaneUserId&quot;: &quot;93c6c00a-14b7-4cf7-845d-70d9e779b2cd&quot;, # From GetHyperplaneUserId &quot;billingProjectId&quot;: &quot;8359f1f9-2eca-465b-9ac5-7cdb0e97e73f&quot; # From GetBillingProjectId &quot;userPvc&quot;: { &quot;connect&quot;: { &quot;id&quot;: &quot;bb2eeed2-6032-4036-9e8f-e757235533bb&quot; }}, # From GetUserPvcId &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters-1","content":"Field\tType\tDefinitionimageType\tString! (required)\tName of Shakudo platform EC hyperplaneUserId\tString! (required)\tShakudo platform user account ID hyperplaneUserEmail\tString! (required)\tShakudo platform user account email collaborative\tBoolean! (required)\tEnables collaborative mode. Collaborative mode allows multiple users to work together in the same session environment. timeout\tInt! (required)\tThe maximum time in seconds that the pipeline may run, starting from the moment of job submission. Default: -1, ie. never timeout; 86400 on dashboard imageHash\tString! (required)\tURL of custom image, &quot;&quot; if using a default image like basic userPvcName\tString (&quot;&quot; if not provided)\tPersistent volume name as found in Kubernetes. Typically includes the drive name found on the dashboard. Default: &quot;&quot; (empty string) which corresponds with default drive claim-{user-email} userPvc\tUserPvc\tShakudo session persistent volume (drive) details. Can either provide identifiers to connect to an existing drive or can provide values to create a new drive. Default: not present, which corresponds with default drive claim-{user-email}. Note: userPvc ID must correspond with same userPvc as userPvcName. displayName\tString\tDrive (PVC) display name as visible on UI dashboard billingProjectId\tString! (required)\tID for billing project that user would like Session costs to contribute. Can either provide identifiers to connect to an existing billing project or can provide values to create a new billing project. Can get from GetBillingProjectId. billingProjectName\tString\tName of billing project as shown on UI dashboard ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-1","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-1","content":"{ &quot;data&quot;: { &quot;createHyperHubSession&quot;: { &quot;id&quot;: &quot;0b8b90c7-b3d6-43d7-a34a-27a9b17521b4&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;jLabUrl&quot;: null, &quot;estimatedCost&quot;: null, &quot;resourceCPUlimit&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;gpuRequest&quot;: null, &quot;startTime&quot;: &quot;2023-07-06T21:14:29.245Z&quot;, &quot;completionTime&quot;: null, &quot;timeout&quot;: 900, &quot;group&quot;: &quot;&quot;, &quot;billingProjectId&quot;: &quot;bb2eeed2-6032-4036-9e8f-e757235533bb&quot;, &quot;podSpec&quot;: null } } }  ","version":"Next","tagName":"h3"},{"title":"Creating using PodSpec JSON (getHyperhubSessionDefaultPodSpec)​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#creating-using-podspec-json-gethyperhubsessiondefaultpodspec","content":"**Getting PodSpec JSON** query GetHyperhubSessionPodSpec($imageType: String, $userPvcName: String, $userEmail: String!, $imageUrl: String) { getHyperhubSessionPodSpec( imageType: $imageType, userPvcName: $userPvcName, userEmail: $userEmail, imageUrl: $imageUrl ) }  Sample Variables { &quot;imageType&quot;: &quot;basic&quot;, &quot;userEmail&quot;: &quot;demo@shakudo.io&quot; }  ****Parameters**** Field\tType\tDescriptionimageUrl\tString\tURL of custom image, same as imageHash userPvcName\tString\tPersistent volume name as found in Kubernetes. Typically includes the drive name found on the dashboard. Default: &quot;&quot; (empty string) which corresponds with default drive claim-{user-email} userEmail\tString! (required)\tShakudo platform user account email imageType\tString\tName of Shakudo platform Podspec/Image Creating Session with PodSpec JSON query GetHyperplaneUserId($userEmail: String!){ hyperplaneUsers(where: {email: {equals: $userEmail}}) { id email } } mutation CreateSessionWithPodSpecJSON( $userEmail: String! $hyperplaneUserId: String! $userPvcName: String = &quot;&quot; $podSpec: JSON ) { createHyperHubSession( data: { hyperplaneUserEmail: $userEmail, hyperplaneUser: { connect: { id: $hyperplaneUserId } }, userPvcName: $userPvcName podSpec: $podSpec } ) { id hyperplaneUserEmail status imageType jLabUrl estimatedCost resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime completionTime timeout group billingProjectId } }  Sample Variables Note: podSpec field contains result of GetHyperhubSessionPodSpec and the corresponding getHyperHubSessionDefaultPodSpec field in the query’s result object. ie. { &quot;imageType&quot;: &quot;basic&quot;, &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;hyperplaneUserId&quot;: &quot;bb2eeed2-6032-4036-9e8f-e757235533bb&quot;, &quot;podSpec&quot;: &lt;getHyperHubSessionDefaultPodSpec result&gt; }  Parameters Field\tType\tDescriptionuserEmail\tString! (required)\tShakudo platform user account email hyperplaneUserId\tString! (required)\tShakudo platform user account ID podSpec\tJSON\tShakudo platform PodSpec config object as a JSON object, originates from getHyperHubSessionDefaultPodSpec userPvcName\tString (”” if not provided)\tAdded as a parameter to align field in UI ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-2","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-2","content":"{ &quot;data&quot;: { &quot;createHyperHubSession&quot;: { &quot;id&quot;: &quot;bb2eeed2-6032-4036-9e8f-e757235533bb&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;jLabUrl&quot;: null, &quot;estimatedCost&quot;: null, &quot;resourceCPUlimit&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;gpuRequest&quot;: null, &quot;startTime&quot;: &quot;2023-07-05T16:26:06.346Z&quot;, &quot;completionTime&quot;: null, &quot;timeout&quot;: -1, &quot;group&quot;: null, &quot;billingProjectId&quot;: null } } }  ","version":"Next","tagName":"h3"},{"title":"Stop Session​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#stop-session","content":" mutation stopSession($id: String!) { updateHyperHubSession(where: {id: $id}, data: { status: {set: &quot;cancelled&quot;} }) { id status } }  ","version":"Next","tagName":"h2"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-2","content":"{ &quot;id&quot;: &quot;9276a796-229f-4ede-a2cf-a7cf329dab6a&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters-2","content":"Field\tType\tDescriptionid\tString! (required)\tSession ID ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-3","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-3","content":"{ &quot;data&quot;: { &quot;updateHyperHubSession&quot;: { &quot;id&quot;: &quot;9276a796-229f-4ede-a2cf-a7cf329dab6a&quot;, &quot;status&quot;: &quot;cancelled&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"Count Sessions​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#count-sessions","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-2","content":"Count the number of sessions based on the filters provided by the parameters. query CountHyperhubSessions($email: String, $imageType: String, $status: String) { countHyperHubSessions(whereOveride: { hyperplaneUserEmail: {equals: $email}, imageType: {equals: $imageType} status: {equals: $status} }) }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-3","content":"{ &quot;email&quot;: &quot;demo@shakudo.io&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;status&quot;: &quot;in progress&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters-3","content":"Field\tType\tDescriptionemail\tString\tShakudo platform user email for the user who created the session imageType\tString\tName of the Shakudo platform Podspec/Image, e.g., &quot;basic&quot; status\tString\tThe status of the Kubernetes job that runs the pipeline job ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-4","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-4","content":"{ &quot;data&quot;: { &quot;countHyperHubSessions&quot;: 1 } }  ","version":"Next","tagName":"h3"},{"title":"Create a Pipeline Job using createPipelineJob Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#create-a-pipeline-job-using-createpipelinejob-parameters","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-3","content":"Creates a Shakudo platform job, which allows users to run task scripts using custom configurations, either immediately as an “Immediate job”, at scheduled intervals as a “Scheduled Job”, or indefinitely as a “Service”. Immediate jobs: schedule = &quot;immediate&quot; Scheduled jobs: schedule != &quot;immediate&quot;, schedule is set to cron schedule expression, eg. * * * * * for a job running every minute Service: timeout and activeTimeout set to -1 , schedule=&quot;immediate&quot; and exposedPort != null, particularly set to a valid port mutation CreatePipelineJob( $type: String! $timeout: Int! $activeTimeout: Int $maxRetries: Int! $yamlPath: String! $exposedPort: String $schedule: String $parameters: ParameterCreateNestedManyWithoutPipelineJobInput $gitServer: HyperplaneVCServerCreateNestedOneWithoutPipelineJobsInput $hyperplaneUserEmail: String! $branchName: String ) { createPipelineJob( data: { jobType: $type timeout: $timeout activeTimeout: $activeTimeout maxRetries: $maxRetries pipelineYamlPath: $yamlPath exposedPort: $exposedPort parameters: $parameters schedule: $schedule hyperplaneVCServer: $gitServer hyperplaneUserEmail: $hyperplaneUserEmail branchName: $branchName } ) { id jobName pipelineYamlPath schedule status statusReason output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries exposedPort jobType parameters { key value } } }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters-4","content":"Field\tType\tDescriptiontype\tString! (required)\tName of Shakudo platform Podspec/Image, default or custom. Example: &quot;basic&quot; timeout\tInt! (required)\tThe maximum time in seconds that the pipeline may run, starting from the moment of job submission. Default: -1 (never timeout). Example: 86400 activeTimeout\tInt\tThe maximum time in seconds that the pipeline may run once it is picked up. Default: -1 (never timeout). Example: 86400 maxRetries\tInt! (required)\tThe maximum number of attempts to run your pipeline job before returning an error, even if timeouts are not reached. Default: 2 yamlPath\tString\tThe relative path to the .yaml file used to run this pipeline job. Example: &quot;example_notebooks/pipelines/python_hello_world_pipeline/pipeline.yaml&quot; exposedPort\tString\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. schedule\tString\tEither &quot;immediate&quot; for an immediate job or a cron schedule expression for a scheduled job at the specified interval. parameters\tParameterCreateNestedManyWithoutPipelineJobInput\tKey-value pairs that can be used within the container environment gitServer\tHyperplaneVCServerCreateNestedOneWithoutPipelineJobsInput\tGit server object, retrievable by searching git servers by name (hyperplaneVCServers) and using resulting id in the following manner: { connect: { id: &lt;gitServerId&gt; } } hyperplaneUserEmail\tString! (required)\tShakudo platform user email branchName\tString\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. If commitID is not specified, the latest commit is used. If not specified, default branch is used. Please note that the exclamation mark ! indicates that the field is required. ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-4","content":"{ &quot;type&quot;: &quot;basic&quot;, &quot;timeout&quot;: 86400, &quot;activeTimeout&quot;: 86400, &quot;maxRetries&quot;: 2, &quot;yamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;main&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-5","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-5","content":"{ &quot;data&quot;: { &quot;createPipelineJob&quot;: { &quot;id&quot;: &quot;7b728979-71b7-426c-9847-6fe3e29a6438&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;schedule&quot;: &quot;immediate&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;output&quot;: null, &quot;startTime&quot;: &quot;2023-06-30T16:03:42.668Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;timeout&quot;: 86400, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: 86400, &quot;maxRetries&quot;: 2, &quot;exposedPort&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;parameters&quot;: [] } } }  ","version":"Next","tagName":"h3"},{"title":"Create a Scheduled Job using createPipelineJob Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#create-a-scheduled-job-using-createpipelinejob-parameters","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-4","content":"Create a scheduled job by specifying a cron schedule. Use the following guide to create a suitable expression for a specific schedule. ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-5","content":"{ &quot;type&quot;: &quot;basic&quot;, &quot;timeout&quot;: 86400, &quot;maxRetries&quot;: 2, &quot;schedule&quot;: &quot;* * * * *&quot;, &quot;yamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;demo&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-6","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-6","content":"{ &quot;data&quot;: { &quot;createPipelineJob&quot;: { &quot;id&quot;: &quot;9276a796-229f-4ede-a2cf-a7cf329dab6a&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;schedule&quot;: &quot;* * * * *&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;output&quot;: null, &quot;startTime&quot;: &quot;2023-06-30T16:03:42.668Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;timeout&quot;: 86400, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: 86400, &quot;maxRetries&quot;: 2, &quot;exposedPort&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;parameters&quot;: [] } } }  ","version":"Next","tagName":"h3"},{"title":"Create a PipelineJob using PodSpec JSON (getPipelineJobPodSpec)​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#create-a-pipelinejob-using-podspec-json-getpipelinejobpodspec","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-5","content":"Create an immediate or scheduled job using a PodSpec JSON object that is customizable. Use the following guide to create a suitable expression for a specific schedule. query GetPipelineJobPodSpec( $parameters: ParametersInput $gitServerName: String = &quot;&quot; $noGitInit: Boolean = false $imageUrl: String = &quot;&quot; $userEmail: String! $noHyperplaneCommands: Boolean = false $commitId: String = &quot;&quot; $branchName: String $pipelineYamlPath: String = &quot;&quot; $debuggable: Boolean = false $jobType: String = &quot;&quot; ) { getPipelineJobPodSpec( parameters: $parameters gitServerName: $gitServerName noGitInit: $noGitInit imageUrl: $imageUrl userEmail: $userEmail noHyperplaneCommands: $noHyperplaneCommands pipelineYamlPath: $pipelineYamlPath commitId: $commitId branchName: $branchName debuggable: $debuggable jobType: $jobType ) }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-6","content":"{ &quot;jobType&quot;: &quot;basic&quot;, &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;branchName&quot;: &quot;demo&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters-5","content":"Field\tType\tDescriptionparameters\tParametersInput\tList of key-value parameters that are injected into the Job environment and can be used as environment variables gitServerName\tString (&quot;&quot; if not provided)\tGit Server name, corresponds with name field in HyperplaneVCServer, which is the display name assigned on the dashboard noGitInit\tBoolean (false if not provided)\tFalse if git server is to be set up using default Shakudo platform workflow. Default: false imageUrl\tString (&quot;&quot; if not provided)\tIf the image is custom, then the image URL can be provided userEmail\tString! (required)\tShakudo platform user account email noHyperplaneCommands\tBoolean\tFalse if using default Shakudo platform commands on job creation. Required to use Shakudo platform jobs through the pipeline YAML, but not required if the image has its own setup. Default: false commitId\tString (&quot;&quot; if not provided)\tThe commit ID with the versions of the pipeline YAML file and pipeline scripts wanted. Ensure that both are present if the commit ID is used. If left empty, assume that the latest commit on the branch is used branchName\tString\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. If commitID is not specified, the latest commit is used. If not specified, default branch is used. pipelineYamlPath\tString (&quot;&quot; if not provided)\tThe relative path to the .yaml file used to run this pipeline job debuggable\tBoolean (false if not provided)\tWhether to enable SSH-based debugging for the job, check the following tutorial for more details jobType\tString (&quot;&quot; if not provided)\tName of Shakudo platform Podspec/Image, default or custom mutation CreatePipelineJob( $jobName: String $pipelineYamlPath: String! $podSpec: JSON! $schedule: String $userEmail: String! ) { createPipelineJob (data: { jobName: $jobName pipelineYamlPath: $pipelineYamlPath podSpec: $podSpec schedule: $schedule hyperplaneUserEmail: $userEmail } ) { id jobName pipelineYamlPath schedule status statusReason output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries exposedPort jobType parameters { key value } } }  Field\tType\tDefinitionjobName\tString\tPlain display name of job viewable from the dashboard, not necessarily unique. pipelineYamlPath\tString!\tThe relative path to the .yaml file used to run this pipeline job podSpec\tJSON!\tShakudo platform PodSpec environment config object as JSON schedule\tString\tEither &quot;immediate&quot; for an immediate job or a cron schedule expression for a scheduled job. userEmail\tString!\tShakudo user account email ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-7","content":"podSpec will be the result of GetPipelineJobPodSpec from the field getPipelineJobPodSpec { &quot;jobType&quot;: &quot;basic&quot;, &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;demo&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;jobName&quot;: &quot;test-create-pipeline-job-with-podSpec&quot;, &quot;podSpec&quot;: &lt;GetPipelineJobPodSpec getPipelineJobPodSpec field result&gt; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-7","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-7","content":"{ &quot;data&quot;: { &quot;createPipelineJob&quot;: { &quot;id&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;schedule&quot;: &quot;* * * * *&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;output&quot;: null, &quot;startTime&quot;: &quot;2023-06-30T16:03:42.668Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;timeout&quot;: 86400, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: 86400, &quot;maxRetries&quot;: 2, &quot;exposedPort&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;parameters&quot;: [] } } }  ","version":"Next","tagName":"h3"},{"title":"Cancel a Pipeline Job​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#cancel-a-pipeline-job","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-6","content":"Cancel a job (Stop job from running). Find PipelineJob by jobName or another non-unique identifier, optional if user has ID query ($jobName: String) { pipelineJobs(where: {jobName: {equals: $jobName} }) { id pipelineYamlPath schedule status statusReason startTime completionTime timeout outputNotebooksPath activeTimeout jobType parameters { key value } } }  Sample Variables { &quot;jobName&quot;: &quot;foo&quot; }  Parameters Field\tType\tDescriptionjobName\tString\tPlain display name of job viewable from the dashboard, not necessarily unique. Use PipelineJob ID to cancel the job mutation ($id: String!) { updatePipelineJob(where: {id: $id}, data: { status: {set: &quot;cancelled&quot;} }) { id } }  Sample Variables { &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; }  Parameters Field\tType\tDescriptionid\tString! (required)\tPipeline Job ID ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-8","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-8","content":"{ &quot;data&quot;: { &quot;updatePipelineJob&quot;: { &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; } **** } }  ","version":"Next","tagName":"h3"},{"title":"Get Job Status​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#get-job-status","content":"query GetPipelineJobStatus($id: String!){ pipelineJob(where: {id: $id }) { status statusReason } }  ","version":"Next","tagName":"h2"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-8","content":"{ &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters-6","content":"Field\tType\tDescriptionid\tString! (required)\tPipeline Job ID ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-9","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-9","content":"{ &quot;data&quot;: { &quot;pipelineJob&quot;: { &quot;status&quot;: &quot;done&quot;, &quot;statusReason&quot;: null } } }  ","version":"Next","tagName":"h3"},{"title":"Get Job Status Statistics​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#get-job-status-statistics","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-7","content":"Count the number of jobs based on their statuses. For example, failed, pending, or cancelled jobs. The timeFrame parameter specifies the timeframe which will be considered. For instance: T_10M = past 10 minutesT_24H = past 24 hours query { COUNT_ALL_TOTAL: getJobStat(stat: COUNT_ALL, timeFrame: TOTAL) COUNT_CANCELLED_TOTAL: getJobStat(stat: COUNT_CANCELLED, timeFrame: TOTAL) COUNT_DONE_TOTAL: getJobStat(stat: COUNT_DONE, timeFrame: TOTAL) COUNT_FAILED_TOTAL: getJobStat(stat: COUNT_FAILED, timeFrame: TOTAL) COUNT_IN_PROGRESS_TOTAL: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: TOTAL ) COUNT_PENDING_TOTAL: getJobStat(stat: COUNT_PENDING, timeFrame: TOTAL) COUNT_SCHEDULED_TOTAL: getJobStat(stat: COUNT_SCHEDULED, timeFrame: TOTAL) COUNT_TIMED_OUT_TOTAL: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: TOTAL) COUNT_ALL_T_10M: getJobStat(stat: COUNT_ALL, timeFrame: T_10M) COUNT_CANCELLED_T_10M: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_10M) COUNT_DONE_T_10M: getJobStat(stat: COUNT_DONE, timeFrame: T_10M) COUNT_FAILED_T_10M: getJobStat(stat: COUNT_FAILED, timeFrame: T_10M) COUNT_IN_PROGRESS_T_10M: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_10M ) COUNT_PENDING_T_10M: getJobStat(stat: COUNT_PENDING, timeFrame: T_10M) COUNT_SCHEDULED_T_10M: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_10M) COUNT_TIMED_OUT_T_10M: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_10M) COUNT_ALL_T_1H: getJobStat(stat: COUNT_ALL, timeFrame: T_1H) COUNT_CANCELLED_T_1H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_1H) COUNT_DONE_T_1H: getJobStat(stat: COUNT_DONE, timeFrame: T_1H) COUNT_FAILED_T_1H: getJobStat(stat: COUNT_FAILED, timeFrame: T_1H) COUNT_IN_PROGRESS_T_1H: getJobStat(stat: COUNT_IN_PROGRESS, timeFrame: T_1H) COUNT_PENDING_T_1H: getJobStat(stat: COUNT_PENDING, timeFrame: T_1H) COUNT_SCHEDULED_T_1H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_1H) COUNT_TIMED_OUT_T_1H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_1H) COUNT_ALL_T_24H: getJobStat(stat: COUNT_ALL, timeFrame: T_24H) COUNT_CANCELLED_T_24H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_24H) COUNT_DONE_T_24H: getJobStat(stat: COUNT_DONE, timeFrame: T_24H) COUNT_FAILED_T_24H: getJobStat(stat: COUNT_FAILED, timeFrame: T_24H) COUNT_IN_PROGRESS_T_24H: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_24H ) COUNT_PENDING_T_24H: getJobStat(stat: COUNT_PENDING, timeFrame: T_24H) COUNT_SCHEDULED_T_24H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_24H) COUNT_TIMED_OUT_T_24H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_24H) }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-9","content":"getJobStat(stat: COUNT_ALL, timeFrame: TOTAL)  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-10","content":"Int ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-10","content":"{ &quot;data&quot;: { &quot;getJobStat&quot;: 105179 } }  ","version":"Next","tagName":"h3"},{"title":"Get Scheduled Jobs Status Statistics​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#get-scheduled-jobs-status-statistics","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-8","content":"Count the number of scheduled jobs based on their statuses, particularly failed, pending, or cancelled jobs. status: SCHEDULED to each getJobStat query to isolated *scheduled* jobs. query { COUNT_ALL_TOTAL: getJobStat(stat: COUNT_ALL, timeFrame: TOTAL, status: SCHEDULED) COUNT_CANCELLED_TOTAL: getJobStat(stat: COUNT_CANCELLED, timeFrame: TOTAL, status: SCHEDULED) COUNT_DONE_TOTAL: getJobStat(stat: COUNT_DONE, timeFrame: TOTAL, status: SCHEDULED) COUNT_FAILED_TOTAL: getJobStat(stat: COUNT_FAILED, timeFrame: TOTAL, status: SCHEDULED) COUNT_IN_PROGRESS_TOTAL: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: TOTAL ) COUNT_PENDING_TOTAL: getJobStat(stat: COUNT_PENDING, timeFrame: TOTAL, status: SCHEDULED) COUNT_SCHEDULED_TOTAL: getJobStat(stat: COUNT_SCHEDULED, timeFrame: TOTAL, status: SCHEDULED) COUNT_TIMED_OUT_TOTAL: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: TOTAL, status: SCHEDULED) COUNT_ALL_T_10M: getJobStat(stat: COUNT_ALL, timeFrame: T_10M) COUNT_CANCELLED_T_10M: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_10M, status: SCHEDULED) COUNT_DONE_T_10M: getJobStat(stat: COUNT_DONE, timeFrame: T_10M, status: SCHEDULED) COUNT_FAILED_T_10M: getJobStat(stat: COUNT_FAILED, timeFrame: T_10M, status: SCHEDULED) COUNT_IN_PROGRESS_T_10M: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_10M ) COUNT_PENDING_T_10M: getJobStat(stat: COUNT_PENDING, timeFrame: T_10M, status: SCHEDULED) COUNT_SCHEDULED_T_10M: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_10M, status: SCHEDULED) COUNT_TIMED_OUT_T_10M: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_10M, status: SCHEDULED) COUNT_ALL_T_1H: getJobStat(stat: COUNT_ALL, timeFrame: T_1H, status: SCHEDULED) COUNT_CANCELLED_T_1H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_1H, status: SCHEDULED) COUNT_DONE_T_1H: getJobStat(stat: COUNT_DONE, timeFrame: T_1H, status: SCHEDULED) COUNT_FAILED_T_1H: getJobStat(stat: COUNT_FAILED, timeFrame: T_1H, status: SCHEDULED) COUNT_IN_PROGRESS_T_1H: getJobStat(stat: COUNT_IN_PROGRESS, timeFrame: T_1H, status: SCHEDULED) COUNT_PENDING_T_1H: getJobStat(stat: COUNT_PENDING, timeFrame: T_1H, status: SCHEDULED) COUNT_SCHEDULED_T_1H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_1H, status: SCHEDULED) COUNT_TIMED_OUT_T_1H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_1H, status: SCHEDULED) COUNT_ALL_T_24H: getJobStat(stat: COUNT_ALL, timeFrame: T_24H) COUNT_CANCELLED_T_24H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_24H, status: SCHEDULED) COUNT_DONE_T_24H: getJobStat(stat: COUNT_DONE, timeFrame: T_24H, status: SCHEDULED) COUNT_FAILED_T_24H: getJobStat(stat: COUNT_FAILED, timeFrame: T_24H, status: SCHEDULED) COUNT_IN_PROGRESS_T_24H: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_24H , status: SCHEDULED ) COUNT_PENDING_T_24H: getJobStat(stat: COUNT_PENDING, timeFrame: T_24H, status: SCHEDULED) COUNT_SCHEDULED_T_24H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_24H, status: SCHEDULED) COUNT_TIMED_OUT_T_24H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_24H, status: SCHEDULED) }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-10","content":"getJobStat(stat: COUNT_ALL, timeFrame: TOTAL, status: SCHEDULED)  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-11","content":"Int ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-11","content":"{ &quot;data&quot;: { &quot;getJobStat&quot;: 179 } }  ","version":"Next","tagName":"h3"},{"title":"Create a Service​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#create-a-service","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-9","content":"Services are currently pipeline jobs which have an activeTimeout and timeout of -1, ie. never ending jobs, schedule = &quot;immediate&quot;, and with exposedPort != null mutation CreateService( $type: String! $maxRetries: Int! $yamlPath: String! $jobName: String! = &quot;&quot; $exposedPort: String = &quot;8787&quot; $parameters: ParameterCreateNestedManyWithoutPipelineJobInput $gitServer: HyperplaneVCServerCreateNestedOneWithoutPipelineJobsInput $hyperplaneUserEmail: String! $branchName: String ) { createPipelineJob( data: { jobType: $type, jobName: $jobName, maxRetries: $maxRetries, pipelineYamlPath: $yamlPath, parameters: $parameters, hyperplaneVCServer: $gitServer, hyperplaneUserEmail: $hyperplaneUserEmail, branchName: $branchName, exposedPort: $exposedPort, timeout: -1, activeTimeout: -1, schedule: &quot;immediate&quot; } ) { id pipelineYamlPath schedule status statusReason output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries exposedPort jobType parameters { key value } } }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-11","content":"{ &quot;type&quot;: &quot;basic&quot;, &quot;maxRetries&quot;: 2, &quot;jobName&quot;: &quot;test&quot;, &quot;yamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;demo&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters-7","content":"Field\tType\tDefinitiontype\tString!\tName of Shakudo platform Podspec/Image, default or custom. Example: &quot;basic&quot; timeout\tInt!\tThe maximum time in seconds that the pipeline may run, starting from the moment of job submission. Set to -1 for Services. activeTimeout\tInt\tThe maximum time in seconds that the pipeline may run once it is picked up. Set to -1 for Services. maxRetries\tInt!\tThe maximum number of attempts to run your pipeline job before returning an error, even if timeouts are not reached. Default: 2 yamlPath\tString\tThe relative path to the .yaml file used to run this pipeline job. Example: &quot;example_notebooks/pipelines/python_hello_world_pipeline/pipeline.yaml&quot; exposedPort\tString\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. schedule\tString\tSet to immediate for Services parameters\tParameterCreateNestedManyWithoutPipelineJobInput\tKey-value pairs that can be used within the container environment gitServer\tHyperplaneVCServerCreateNestedOneWithoutPipelineJobsInput\tGit server object, retrievable by searching git servers by name (hyperplaneVCServers) and using resulting id in the following manner: { connect: { id: $gitServerId } } hyperplaneUserEmail\tString!\tShakudo platform user email branchName\tString\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. If commitID is not specified, the latest commit is used. If not specified, default branch is used. ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-12","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-12","content":"{ &quot;data&quot;: { &quot;createPipelineJob&quot;: { &quot;id&quot;: &quot;9276a796-229f-4ede-a2cf-a7cf329dab6a&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;schedule&quot;: &quot;immediate&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;output&quot;: null, &quot;startTime&quot;: &quot;2023-07-06T14:51:35.506Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;timeout&quot;: -1, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: -1, &quot;maxRetries&quot;: 2, &quot;exposedPort&quot;: &quot;8787&quot;, &quot;jobType&quot;: &quot;basic&quot;, &quot;parameters&quot;: [] } } }  ","version":"Next","tagName":"h3"},{"title":"Create a Service using PodSpec JSON (getUserServicePodSpec)​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#create-a-service-using-podspec-json-getuserservicepodspec","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-10","content":"Create a Service using a PodSpec JSON object that is customizable. Retrieve UserServicePodSpec query GetUserServicePodSpec( $exposedPort: String $parameters: ParametersInput $gitServerName: String $noGitInit: Boolean $imageUrl: String $userEmail: String! $noHyperplaneCommands: Boolean $commitId: String $branchName: String! $pipelineYamlPath: String! $jobType: String! ) { getUserServicePodSpec( exposedPort: $exposedPort parameters: $parameters gitServerName: $gitServerName noGitInit: $noGitInit imageUrl: $imageUrl userEmail: $userEmail noHyperplaneCommands: $noHyperplaneCommands commitId: $commitId branchName: $branchName pipelineYamlPath: $pipelineYamlPath jobType: $jobType ) }  Sample Variables { &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;demo&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;jobType&quot;: &quot;basic&quot; }  Parameters Field\tType\tDescriptionexposedPort\tString\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. parameters\tParametersInput\tKey-value pairs that can be used within the container environment. gitServerName\tString\tThe name of the Git server used for the pipeline job. noGitInit\tBoolean\tSpecifies whether the Git server initialization is skipped for the pipeline job. imageUrl\tString\tThe URL of the image used for the pipeline job. userEmail\tString!\tShakudo platform user email for the user who created the session. noHyperplaneCommands\tBoolean\tSpecifies whether default Shakudo platform commands are used for the pipeline job creation. commitId\tString\tThe commit hash for the specific commit used to pull the latest files for the pipeline. branchName\tString!\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. pipelineYamlPath\tString!\tThe relative path to the .yaml file used to run this pipeline job. jobType\tString!\tName of the Shakudo platform Podspec/Image used for the pipeline job. eg. &quot;basic&quot; Create Service using UserServicePodSpec result mutation CreateService( $podSpec: JSON! $jobName: String! = &quot;&quot; $userEmail: String! $exposedPort: String! = &quot;8787&quot; $pipelineYamlPath: String! ) { createPipelineJob (data: { jobName: $jobName, podSpec: $podSpec, hyperplaneUserEmail: $userEmail, pipelineYamlPath: $pipelineYamlPath, exposedPort: $exposedPort, timeout: -1, activeTimeout: -1, schedule: &quot;immediate&quot; } ) { id pipelineYamlPath schedule status statusReason output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries exposedPort jobType parameters { key value } } }  Sample Variables { &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;exposedPort&quot;: &quot;8787&quot;, &quot;branchName&quot;: &quot;main&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;jobType&quot;: &quot;basic&quot;, &quot;jobName&quot;: &quot;test-service&quot;, &quot;podSpec&quot;: &lt;getUserServicePodSpec field result&gt; }  Parameters Field\tType\tDescriptionpodSpec\tJSON!\tThe JSON object representing the PodSpec configuration for the pipeline job. jobName\tString!\tThe name of the pipeline job. userEmail\tString!\tShakudo platform user email for the user who created the session. exposedPort\tString!\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. Default value: &quot;8787&quot;. pipelineYamlPath\tString!\tThe relative path to the .yaml file used to run this pipeline job. ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-13","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-13","content":"{ &quot;data&quot;: { &quot;createPipelineJob&quot;: { &quot;id&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;schedule&quot;: &quot;immediate&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;output&quot;: null, &quot;startTime&quot;: &quot;2023-06-30T16:03:42.668Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;timeout&quot;: -1, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: -1, &quot;maxRetries&quot;: 2, &quot;exposedPort&quot;: &quot;8787&quot;, &quot;jobType&quot;: &quot;basic&quot;, &quot;parameters&quot;: [] } } }  ","version":"Next","tagName":"h3"},{"title":"Get a List of Services​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#get-a-list-of-services","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-11","content":"Get a list of services — services are pipeline jobs which have an activeTimeout and timeout of -1, ie. never ending jobs, schedule = &quot;immediate&quot;, and with exposedPort != null query services($offset: Int, $limit: Int!, $status: String!) { pipelineJobs(orderBy: [{pinned: desc},{ startTime: desc}], take: $limit, skip: $offset, where: { AND: [ {activeTimeout: {equals: -1}}, {timeout: {equals: -1}}, {timeout: {equals: &quot;immediate&quot;}}, {status: {equals: $status}} ] }) { id exposedPort pinned pipelineYamlPath schedule status statusReason startTime completionTime daskDashboardUrl timeout output outputNotebooksPath activeTimeout duration jobType schedule estimatedCost owner maxRetries } }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-12","content":"{ &quot;limit&quot;: 10, &quot;status&quot;: &quot;in progress&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters-8","content":"Field\tType\tDescriptionoffset\tInt\tThe number of records to skip from the original result. limit\tInt! (required)\tThe number of records to retrieve. status\tString! (required)\tThe status of the Kubernetes job that runs the pipeline job. ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-14","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-14","content":"{ &quot;data&quot;: { &quot;pipelineJobs&quot;: [ { &quot;id&quot;: &quot;9276a796-229f-4ede-a2cf-a7cf329dab6a&quot;, &quot;exposedPort&quot;: &quot;8787&quot;, &quot;pinned&quot;: false, &quot;pipelineYamlPath&quot;: &quot;service.yaml&quot;, &quot;schedule&quot;: &quot;immediate&quot;, &quot;status&quot;: &quot;in progress&quot;, &quot;statusReason&quot;: null, &quot;startTime&quot;: &quot;2023-03-23T02:34:51.850Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: &quot;client.hyperplane.dev/dashboard/&quot;, &quot;timeout&quot;: -1, &quot;output&quot;: null, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: -1, &quot;duration&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;estimatedCost&quot;: null, &quot;owner&quot;: &quot;demo&quot;, &quot;maxRetries&quot;: 0 }, { &quot;id&quot;: &quot;abeee208-c717-42d9-81f9-9448cdf1473e&quot;, &quot;exposedPort&quot;: &quot;8787&quot;, &quot;pinned&quot;: false, &quot;pipelineYamlPath&quot;: &quot;service2.yaml&quot;, &quot;schedule&quot;: &quot;immediate&quot;, &quot;status&quot;: &quot;in progress&quot;, &quot;statusReason&quot;: null, &quot;startTime&quot;: &quot;2022-11-18T19:21:23.504Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: &quot;client.hyperplane.dev/dashboard2/&quot;, &quot;timeout&quot;: -1, &quot;output&quot;: null, &quot;outputNotebooksPath&quot;: &quot;gs://outputNotebookPath&quot;, &quot;activeTimeout&quot;: -1, &quot;duration&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;estimatedCost&quot;: null, &quot;owner&quot;: &quot;demo&quot;, &quot;maxRetries&quot;: 0 } ] } }  ","version":"Next","tagName":"h3"},{"title":"Cancel all Scheduled Jobs​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#cancel-all-scheduled-jobs","content":"Cancel all Scheduled Jobs mutation cancelScheduledJobs { updateManyPipelineJob( where: { status: { equals: &quot;scheduled&quot; } } data: { status: { set: &quot;cancelled&quot; } } ) { count } }  Cancel all Scheduled Jobs for a Specific User Users can also add hyperplaneUserEmail: { equals: $userEmail } to cancel all scheduled jobs created by a particular user. mutation cancelScheduledJobsForUser($userEmail: String!) { updateManyPipelineJob( where: { status: { equals: &quot;scheduled&quot; }, hyperplaneUserEmail: { equals: $userEmail } } data: { status: { set: &quot;cancelled&quot; } } ) { count } }  Sample Variables { &quot;userEmail&quot;: &quot;demo@shakudo.io&quot; }  Parameters Field\tType\tDescriptionuserEmail\tString! (required)\tThe email corresponding to the user who created all the scheduled jobs to be cancelled. ","version":"Next","tagName":"h2"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-15","content":"AffectedRowsOutput ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-15","content":"{ &quot;data&quot;: { &quot;updateManyPipelineJob&quot;: { &quot;count&quot;: 2 } } }  ","version":"Next","tagName":"h3"},{"title":"Get Job Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#get-job-parameters","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-12","content":"Get the list of parameters for a pipeline job query jobParameters($id: String!) { pipelineJobs(where: {id: {equals: $id}}) { parameters { key value id pipelineJobId } } }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters-9","content":"Field\tType\tDescriptionid\tString! (required)\tThe ID of the job for which the parameters are being listed. ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-13","content":"{ &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields","content":"Array of Parameters ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-16","content":"{ &quot;data&quot;: { &quot;pipelineJobs&quot;: [ { &quot;parameters&quot;: [ { &quot;key&quot;: &quot;key&quot;, &quot;value&quot;: &quot;value&quot;, &quot;id&quot;: &quot;abeee208-c717-42d9-81f9-9448cdf1473e&quot;, &quot;pipelineJobId&quot;: &quot;d1e5cd20-05d3-4517-b009-ec2e8e4f171d&quot; } ] } ] } }  ","version":"Next","tagName":"h3"},{"title":"Delete a Job Parameter​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#delete-a-job-parameter","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-13","content":"Delete a parameter for a pipeline job # Retrieve parameterId query GetPipelineJobParameters($jobId: String!){ pipelineJob(where:{id: $jobId}){ jobName parameters{ id key value } } } mutation DeletePipelineJobParameter($parameterId: String!) { deleteParameter(where: { id: $parameterId }) { id key value } }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters-10","content":"Field\tType\tDescriptionjobId\tString! (required)\tThe ID of the job from which the parameter is being deleted. parameterId\tString! (required)\tThe ID of the parameter being deleted. Retrieved from GetPipelineJobParameters ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-14","content":"{ &quot;jobId&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;parameterId&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-16","content":"Parameter ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-17","content":"{ &quot;data&quot;: { &quot;deleteParameter&quot;: { &quot;id&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot;, &quot;key&quot;: &quot;foo&quot;, &quot;value&quot;: &quot;bar&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"Update a Job Parameter​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#update-a-job-parameter","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#description-14","content":"Updates the key and/or value of a parameter. mutation ($parameterId: String!, $keyValue: String, $valueValue: String) { updateParameter(where: {id: $parameterId}, data: { key: {set: $keyValue} value: {set: $valueValue} }) { id key value } }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameters-11","content":"Field\tType\tDescriptionparameterId\tString! (required)\tID of the parameter being updated. keyValue\tString\tNew value for the &quot;key&quot; field of the parameter. valueValue\tString\tNew value for the &quot;value&quot; field of the parameter. ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-15","content":"{ &quot;parameterId&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;keyValue&quot;: &quot;newKey&quot;, &quot;valueValue&quot;: &quot;newValue&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-17","content":"Parameter ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-18","content":"{ &quot;data&quot;: { &quot;updateParameter&quot;: { &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; &quot;keyValue&quot;: &quot;newKey&quot;, &quot;valueValue&quot;: &quot;newValue&quot; } } }  Types ","version":"Next","tagName":"h3"},{"title":"HyperHubSession​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#hyperhubsession","content":"Metadata for Sessions billingProject: BillingProject billingProjectId: String collaborative: Boolean completionTime: DateTime currentPodEvents: String department: String duration: Int estimatedCost: Float gpuRequest: String group: String hyperplaneUser: HyperplaneUser! hyperplaneUserEmail: String! hyperplanepodspecName: String id: String imageHash: String imageType: String jLabUrl: String notebookURI: String owner: String podEventsLog: String podSpec: String podSpecTemplate: HyperplanePodSpec premptableNode: Boolean resourceCPUlimit: String resourceCPUrequest: String resourceRAMlimit: String resourceRAMrequest: String runId: String sshCommand: String startTime: DateTime status: String statusReason: String timeout: Int useHyperplanepodspec: Boolean userPvc: UserPvc userPvcName: String workerPodName: String  Field\tType\tDefinitionbillingProject\tBillingProject\tBilling project that user would like Session costs to contribute. Can either provide identifiers to connect to an existing billing project or can provide values to create a new billing project. billingProjectId\tString\tBilling Project ID. collaborative\tBoolean\tToggle collaborative mode completionTime\tDateTime\tCompletion time of the pipeline job currentPodEvents\tString\tDisplays log of states of pod (current events in pod) department\tString\tDisabled, not used duration\tInt\tDuration of the pipeline job estimatedCost\tFloat\tDisabled, plan on using it for tracking estimated cost of the job gpuRequest\tString\tNumber of gpus requested group\tString\tNot used, leave as an empty string hyperplaneUser\tHyperplaneUser!\tShakudo platform user account details. Can either provide identifiers to connect to an existing account or can provide values to create a new user account. hyperplaneUserEmail\tString!\tShakudo platform user account email hyperplanepodspecName\tString\tDisabled id\tString\tHyperHubSession object identifier imageHash\tString\tURL of custom image, same as imageUrl imageType\tString\tName of Shakudo platform Podspec/Image jLabUrl\tString\tURL for JupyterLab version of the Session environment notebookURI\tString\tBase url to access jupyter notebook and vscode notebooks owner\tString\tUsername of the user account that owns the session, currently the user that created the session podEventsLog\tString\tSession pod event status log details podSpec\tJson?\tShakudo platform PodSpec environment config object as JSON podSpecTemplate\tHyperplanePodSpec\tNot used, similar use to jobType premptableNode\tBoolean\tDisabled resourceCPUlimit\tString\tLimit to the number of CPUs to be allocated resourceCPUrequest\tString\tNumber of CPUs requested to be allocated resourceRAMlimit\tString\tMemory allocation limit resourceRAMrequest\tString\tMemory allocation amount request runId\tString sshCommand\tString startTime\tDateTime\tSession environment creation time status\tString\tThe status of the Kubernetes job that runs the pipeline job statusReason\tString\tKubernetes job status details timeout\tInt\tThe maximum time in seconds that the pipeline may run, starting from the moment of job submission. Default: -1, i.e., never timeout; 86400 on the dashboard useHyperplanepodspec\tBoolean userPvc\tUserPvc\tShakudo session persistent volume (drive) details. Can either provide identifiers to connect to an existing drive or can provide values to create a new drive. Default: not present, which corresponds with default drive claim-{user-email}. userPvcName\tString\tPersistent volume name as found in Kubernetes. Typically includes the drive name found on the dashboard. Default: an empty string, which corresponds with the default drive claim-{user-email}. workerPodName\tString\t Example: { &quot;billingProject&quot;: null, &quot;collaborative&quot;: false, &quot;completionTime&quot;: &quot;2023-05-17T21:26:02.310Z&quot;, &quot;currentPodEvents&quot;: null, &quot;department&quot;: null, &quot;duration&quot;: 901, &quot;estimatedCost&quot;: null, &quot;gpuRequest&quot;: null, &quot;group&quot;: &quot;&quot;, &quot;hyperplaneUser&quot;: { &quot;id&quot;: &quot;3661bd41-6ca9-4b20-a74b-c46ce6ff6951&quot;, &quot;email&quot;: &quot;demo@shakudo.io&quot; }, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;hyperplanepodspecName&quot;: null, &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;imageHash&quot;: &quot;gcr.io/imageHash&quot;, &quot;imageType&quot;: &quot;test-custom-image&quot;, &quot;jLabUrl&quot;: &quot;&quot;, &quot;notebookURI&quot;: &quot;&quot;, &quot;owner&quot;: &quot;demo&quot;, &quot;podEventsLog&quot;: &quot;Stopping container hyperhub-user&quot;, &quot;podSpec&quot;: null, &quot;podSpecTemplate&quot;: null, &quot;premptableNode&quot;: false, &quot;resourceCPUlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;runId&quot;: null, &quot;sshCommand&quot;: null, &quot;startTime&quot;: &quot;2023-05-17T21:11:00.948Z&quot;, &quot;status&quot;: &quot;cancelled&quot;, &quot;statusReason&quot;: &quot;Ready--true&quot;, &quot;timeout&quot;: 900, &quot;useHyperplanepodspec&quot;: false, &quot;userPvc&quot;: null, &quot;userPvcName&quot;: &quot;&quot;, &quot;workerPodName&quot;: null }  ","version":"Next","tagName":"h2"},{"title":"PipelineJob​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#pipelinejob","content":"Shakudo platform job config docs type PipelineJob { TritonClient: TritonClient activeTimeout: Int! billingProject: BillingProject billingProjectId: String branchName: String branchNameOrCommit: BranchSelection childJobs(cursor: PipelineJobWhereUniqueInput, distinct: [PipelineJobScalarFieldEnum!], orderBy: [PipelineJobOrderByInput!], skip: Int, take: Int, where: PipelineJobWhereInput): [PipelineJob!]! cloudRunner: String commitId: String completionTime: DateTime customCommand: String customTrigger: String dashboardPrefix: String daskDashboardUrl: String debuggable: Boolean! department: String displayedOwner: String! duration: Int estimatedCost: Float exposedPort: String grafanaLink: String! group: String hyperplaneUser: HyperplaneUser hyperplaneUserEmail: String hyperplaneUserId: String hyperplaneVCServer: HyperplaneVCServer hyperplaneVCServerId: String hyperplanepodspecName: String icon: String id: String! imageHash: String jobCommand: String jobName: String jobType: String! mappedUrl: String maxHpaRange: Int! maxRetries: Int! maxRetriesPerStep: Int! minReplicas: Int! noGitInit: Boolean noHyperplaneCommands: Boolean noVSRewrite: Boolean output: String outputNotebooksPath: String owner: String parameters(cursor: ParameterWhereUniqueInput, distinct: [ParameterScalarFieldEnum!], orderBy: [ParameterOrderByInput!], skip: Int, take: Int, where: ParameterWhereInput): [Parameter!]! parentJob: PipelineJob parentJobId: String pinned: Boolean! pipelineYamlPath: String podSpecTemplate: HyperplanePodSpec podSpecTemplateId: String preemptible: Boolean! premptableNode: Boolean! priorityClass: String! runId: String schedule: String! sendNotification: Boolean slackChannelName: String sshCommand: String startTime: DateTime! status: String! statusReason: String steps(cursor: PipelineStepWhereUniqueInput, distinct: [PipelineStepScalarFieldEnum!], orderBy: [PipelineStepOrderByInput!], skip: Int, take: Int, where: PipelineStepWhereInput): [PipelineStep!]! timeout: Int! timeoutPerStep: Int timezone: String! useHyperplanepodspec: Boolean workerPodName: String }  Field\tType\tDefinitionTritonClient\tTritonClient\tIf TritonClient is non-null, then this PipelineJob is a Triton Job. TritonClient stores Triton client instance object metadata. activeTimeout\tInt!\tThe maximum time in seconds that the pipeline may run once it is picked up. Default: 86400, use -1 to never timeout. billingProject\tBillingProject\tBilling project that user would like Job costs to contribute. Can either provide identifiers to connect to an existing billing project or can provide values to create a new billing project. billingProjectId\tString\tBilling Project ID. branchName\tString\tName of git branch. branchNameOrCommit\tBranchSelection\tEnum that states whether the image is based on branch or commit. childJobs\t[PipelineJob!]!\tJobs that spawned based on this job. cloudRunner\tString\tCurrently disabled, but will be used to determine which cloud the job will run on. Will be added as part of the multicloud feature. commitId\tString\tCommit hash for the commit used to pull the latest files for the pipeline. completionTime\tDateTime\tCompletion time of the pipeline job. customCommand\tString\tNot used customTrigger\tString\tCurrently disabled on cluster, but will be used on KEDA jobs. dashboardPrefix\tString\tWhich URL subpath you want a Service to run in with respect to your Shakudo service domain. e.g., http://shakudoservice.io/modelV1. daskDashboardUrl\tString\tURL for dask dashboard debuggable\tBoolean!\tWhether the debuggable service is enabled. department\tString\tNot used displayedOwner\tString!\tUsername of the user account that owns the job, currently the user that created the job. Username is based on email. duration\tInt\tDuration of the pipeline job. estimatedCost\tFloat\tNot used exposedPort\tString\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. grafanaLink\tString!\tLink to Grafana logs. group\tString\tDisabled, not used. Leave as empty string hyperplaneUser\tHyperplaneUser\tShakudo platform user account details. Can either provide identifiers to connect to an existing account or can provide values to create a new user account. hyperplaneUserEmail\tString\tShakudo platform user email. hyperplaneUserId\tString\tShakudo platform user ID. hyperplaneVCServer\tHyperplaneVCServer\tShakudo Platform Git Server object. hyperplaneVCServerId\tString\tGit server object ID. hyperplanepodspecName\tString icon\tString id\tString! imageHash\tString\timageUrl jobCommand\tString jobName\tString\tPlain name of job viewable from the dashboard, is not necessarily unique. jobType\tString!\tName of Shakudo platform Podspec/Image. mappedUrl\tString maxHpaRange\tInt!\tMaximum number of replicas for HPA (horizontal pod autoscaling). maxRetries\tInt!\tMaximum number of job retries. maxRetriesPerStep\tInt!\tMaximum number of retries per job step. minReplicas\tInt!\tMinimum number of K8s ReplicaSets. noGitInit\tBoolean\tFalse if git server is to be set up using Shakudo platform workflow. Default: false. noHyperplaneCommands\tBoolean\tFalse if using default Shakudo platform commands on job creation. Required to use Shakudo platform jobs through the pipeline yaml, but not required if the image has its own setup. Default: false. noVSRewrite\tBoolean\tOnly supported for Services. If enabled, the external prefix/subpath on the Shakudo domain directly corresponds to the same subpath within the Service. output\tString outputNotebooksPath\tString owner\tString\tTypically mirrors displayedOwner, refer primarily to displayedOwner. parameters\t[Parameter!]!\tList of key-value parameters that are injected into the Job environment and can be used as environment variables. parentJob\tPipelineJob\tThe info of that parent job if the current job spawned from another job. parentJobId\tString\tParent job ID. pinned\tBoolean!\tWhether the job is pinned on the dashboard. pipelineYamlPath\tString\tRelative path to .yaml file for running pipeline podSpecTemplate\tHyperplanePodSpec\tNot used, similar use to jobType. podSpecTemplateId\tString\tID for the corresponding podSpecTemplate. preemptible\tBoolean!\tDetermines whether the job can be preempted, i.e., timed out. This means that a Preemptible VM will be used. premptableNode\tBoolean!\tNot used priorityClass\tString!\tK8s pod priority classification. runId\tString schedule\tString!\tEither &quot;immediate&quot; for an immediate job or a cron schedule expression for a scheduled job at the specified interval. sendNotification\tBoolean slackChannelName\tString sshCommand\tString startTime\tDateTime!\tJob start time. status\tString!\tThe status of the Kubernetes job that runs | statusReason | String | Kubernetes job status detail | | steps | (cursor: PipelineStepWhereUniqueInput, distinct: [PipelineStepScalarFieldEnum!], orderBy: [PipelineStepOrderByInput!], skip: Int, take: Int, where: PipelineStepWhereInput): [PipelineStep!]! | pipeline step objects that correspond with individual script steps | | timeoutPerStep | Int | | | timezone | String | eg. UTC | | useHyperplanepodspec | Boolean | Not used | | workerPodName | String | Not used | Example: { &quot;TritonClient&quot;: null, &quot;activeTimeout&quot;: 82400, &quot;billingProject&quot;: null, &quot;billingProjectId&quot;: null, &quot;branchName&quot;: null, &quot;branchNameOrCommit&quot;: null, &quot;childJobs&quot;: [], &quot;cloudRunner&quot;: &quot;&quot;, &quot;commitId&quot;: null, &quot;completionTime&quot;: null, &quot;customCommand&quot;: null, &quot;customTrigger&quot;: null, &quot;dashboardPrefix&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;debuggable&quot;: false, &quot;department&quot;: null, &quot;displayedOwner&quot;: &quot;&quot;, &quot;duration&quot;: null, &quot;estimatedCost&quot;: null, &quot;exposedPort&quot;: null, &quot;grafanaLink&quot;: &quot;https://grafana.sample.hyperplane.dev/explore&quot;, &quot;group&quot;: null, &quot;hyperplaneUser&quot;: null, &quot;hyperplaneUserEmail&quot;: null, &quot;hyperplaneUserId&quot;: null, &quot;hyperplaneVCServer&quot;: null, &quot;hyperplaneVCServerId&quot;: null, &quot;hyperplanepodspecName&quot;: null, &quot;icon&quot;: null, &quot;id&quot;: &quot;b50e8ea9-1627-4a5d-b7c7-ebad6c801d0a&quot;, &quot;imageHash&quot;: &quot;&quot;, &quot;jobCommand&quot;: null, &quot;jobName&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;mappedUrl&quot;: null, &quot;maxHpaRange&quot;: 1, &quot;maxRetries&quot;: 2, &quot;maxRetriesPerStep&quot;: 0, &quot;minReplicas&quot;: 1, &quot;noGitInit&quot;: false, &quot;noHyperplaneCommands&quot;: false, &quot;noVSRewrite&quot;: false, &quot;output&quot;: null, &quot;outputNotebooksPath&quot;: null, &quot;owner&quot;: null, &quot;parameters&quot;: [], &quot;parentJob&quot;: null, &quot;parentJobId&quot;: null, &quot;pinned&quot;: false, &quot;pipelineYamlPath&quot;: &quot;example_pipeline.yaml&quot;, &quot;podSpecTemplate&quot;: null, &quot;podSpecTemplateId&quot;: null, &quot;preemptible&quot;: true, &quot;premptableNode&quot;: true, &quot;priorityClass&quot;: &quot;shakudo-priority-class&quot;, &quot;runId&quot;: null, &quot;schedule&quot;: &quot;immediate&quot;, &quot;sendNotification&quot;: null, &quot;slackChannelName&quot;: null, &quot;sshCommand&quot;: null, &quot;startTime&quot;: &quot;2023-06-29T16:38:10.829Z&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;steps&quot;: [], &quot;timeout&quot;: 82400, &quot;timeoutPerStep&quot;: null, &quot;timezone&quot;: &quot;UTC&quot;, &quot;useHyperplanepodspec&quot;: false, &quot;workerPodName&quot;: null }  ","version":"Next","tagName":"h2"},{"title":"Parameter​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#parameter","content":"Key-value pairs that are injected into Jobs and Session environments type Parameter { PipelineJob: PipelineJob id: String! key: String! pipelineJobId: String value: String }  Field\tType\tDefinitionPipelineJob\tPipelineJob\tPipeline job that has this parameter id\tString!\tThe ID of the parameter key\tString!\tThe key of the parameter pipelineJobId\tString\tThe ID of the pipeline job that has this parameter value\tString\tThe value of the parameter { &quot;key&quot;: &quot;key&quot;, &quot;value&quot;: &quot;value&quot;, &quot;id&quot;: &quot;b50e8ea9-1627-4a5d-b7c7-ebad6c801d0a&quot;, &quot;pipelineJobId&quot;: &quot;833bd3d1-bb63-4289-99d8-25c2856f2fba&quot; }  ","version":"Next","tagName":"h2"},{"title":"HyperplaneVCServer​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#hyperplanevcserver","content":"Git servers tied to remote git repositories type HyperplaneVCServer { defaultBranch: String! id: String! name: String! pipelineJobs(cursor: PipelineJobWhereUniqueInput, distinct: [PipelineJobScalarFieldEnum!], orderBy: [PipelineJobOrderByInput!], skip: Int, take: Int, where: PipelineJobWhereInput): [PipelineJob!]! serviceUrl: String status: HyperplaneVCServerStatus! url: String! }  Field\tType\tDefinitiondefaultBranch\tString!\tThe default git branch of the git server id\tString!\tThe ID of the git server (HyperplaneVCServer) object name\tString!\tThe name of the git server pipelineJobs\t[PipelineJob!]!\tThe pipeline jobs that are connected to this git server. Mirrors pipelineJobs query serviceUrl\tString\tThe service URL (DNS record) for in-cluster connection access status\tHyperplaneVCServerStatus!\tThe status of the git server resource url\tString!\tThe remote repository SSH URL ","version":"Next","tagName":"h2"},{"title":"HyperplanePodSpec​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#hyperplanepodspec","content":"Environment configs for defining Shakudo resources; surrounds image, hardware, storage, kubernetes settings, etc. type HyperplanePodSpec { description: String! displayName: String! extraEnvars: String extraTolerations: String extraVolumeMounts: String extraVolumes: String gpuResourceType: String hyperhubSessions(cursor: HyperHubSessionWhereUniqueInput, distinct: [HyperHubSessionScalarFieldEnum!], orderBy: [HyperHubSessionOrderByInput!], skip: Int, take: Int, where: HyperHubSessionWhereInput): [HyperHubSession!]! hyperplaneImage: HyperplaneImage hyperplaneImageId: String hyperplaneUser: HyperplaneUser hyperplaneUserEmail: String hyperplaneUserId: String icon: String! id: String! imagePullPolicy: String imageUrl: String nodeSelector: String nodeSelectorKey: String nodeSelectorValue: String pipelineJobs(cursor: PipelineJobWhereUniqueInput, distinct: [PipelineJobScalarFieldEnum!], orderBy: [PipelineJobOrderByInput!], skip: Int, take: Int, where: PipelineJobWhereInput): [PipelineJob!]! podSpec: String podspecName: String! pv: String pvc: String resourceCPUlimit: String resourceCPUrequest: String resourceGPUrequest: String resourceRAMlimit: String resourceRAMrequest: String show: Boolean! status: String! statusReason: String workingDir: String }  Field\tType\tDefinitiondescription\tString!\tPodSpec description displayName\tString!\tGeneral purpose display name for PodSpec that appears as a title on dashboard extraEnvars\tString\tList of key-value parameters that are injected into any Shakudo resource environment extraTolerations\tString\tAdditional pod toleration rules extraVolumeMounts\tString\tAdditional storage mounting rules. These are relative to the provided Volumes extraVolumes\tString\tAdditional persistent storage spaces gpuResourceType\tString\tType of GPU resource hyperhubSessions\t[HyperHubSession!]!\tSessions that currently use this PodSpec. Mirrors hyperhubSessions query hyperplaneImage\tHyperplaneImage\tHyperplane image associated with the PodSpec hyperplaneUser\tHyperplaneUser\tShakudo platform user account details icon\tString!\tNot used id\tString!\tID of the PodSpec imagePullPolicy\tString imageUrl\tString\tURL of image nodeSelector\tString nodeSelectorKey\tString nodeSelectorValue\tString pipelineJobs\t[PipelineJob!]!\tThe pipeline jobs that are connected to this git server. Mirrors pipelineJobs query podSpec\tString podspecName\tString! pv\tString\tPersistent volume associated with the PodSpec pvc\tString\tPersistent volume claim associated with the PodSpec resourceCPUlimit\tString\tCPU limit for resource allocation resourceCPUrequest\tString\tCPU request for resource allocation resourceGPUrequest\tString\tGPU request for resource allocation resourceRAMlimit\tString\tRAM limit for resource allocation resourceRAMrequest\tString\tRAM request for resource allocation show\tBoolean! status\tString!\tStatus of the PodSpec statusReason\tString\tReason for the PodSpec status workingDir\tString\tWorking directory for the PodSpec Operations ","version":"Next","tagName":"h2"},{"title":"hyperHubSessions​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#hyperhubsessions","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature","content":"hyperHubSessions( cursor: HyperHubSessionWhereUniqueInput, distinct: [HyperHubSessionScalarFieldEnum!], orderBy: [HyperHubSessionOrderByInput!], skip: Int, take: Int, where: HyperHubSessionWhereInput ): [HyperHubSession!]!  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description","content":"Retrieves a list of Shakudo platform session metadata, allowing for pagination (cursor and offset-based) and filtering. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields","content":"Field\tType\tDefinitioncursor\tHyperHubSessionWhereUniqueInput\tStarting session value to paginate from using cursor-based pagination, i.e., the current result starts from this session record. distinct\t[HyperHubSessionScalarFieldEnum!]\tList of fields where their values will remain distinct per record. orderBy\t[HyperHubSessionOrderByInput!]\tList of fields that will be used to order the results, ordering precedence determined by the location in the list. skip\tInt\tThe number of records to skip from the original result. take\tInt\tThe maximum number of records to show in the result. where\tHyperHubSessionWhereInput\tConditional values to filter for a specific HyperHubSession object. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example","content":"query HyperhubSessions($limit: Int!, $email: String, $status: String) { hyperHubSessions(orderBy:{startTime: desc}, take: $limit, where: { hyperplaneUserEmail: {equals: $email}, status: {equals: $status}, }) { id hyperplaneUserEmail status imageType jLabUrl notebookURI estimatedCost department resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime completionTime } countHyperHubSessions }  variables { &quot;limit&quot;: 10, &quot;email&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;in progress&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields-1","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-example","content":"{ &quot;data&quot;: { &quot;hyperHubSessions&quot;: [ { &quot;id&quot;: &quot;78ba5679-1fd0-475a-88b0-d1877413747f&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;in progress&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;jLabUrl&quot;: &quot;client/hyperplane.dev/jlabUrl/&quot;, &quot;notebookURI&quot;: &quot;ssh demo-pvc-entry@demo.dev&quot;, &quot;estimatedCost&quot;: null, &quot;department&quot;: null, &quot;resourceCPUlimit&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;gpuRequest&quot;: null, &quot;startTime&quot;: &quot;2023-06-28T15:32:40.090Z&quot;, &quot;completionTime&quot;: null } ], &quot;countHyperHubSessions&quot;: 3006 } }  ","version":"Next","tagName":"h3"},{"title":"createHyperHubSession​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#createhyperhubsession","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature-1","content":"createHyperHubSession( data: HyperHubSessionCreateInput! ): HyperHubSession!  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description-1","content":"Creates a Shakudo Session environment, a data development environment comes with pre-configured environments, typically accessible in the form of a jupyter notebook. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields-1","content":"Field\tType\tDescriptiondata\tHyperHubSessionCreateInput\tHyperHubSession object that contains field values used to create a Session. Check HyperHubSessionCreateInput for specific fields. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example-1","content":"mutation CreateHyperHubSession($input: HyperHubSessionCreateInput!) { createHyperHubSession(data: $input) { id hyperplaneUserEmail status imageType jLabUrl estimatedCost department resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime completionTime timeout group billingProjectId } }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-16","content":"{ &quot;input&quot;: { &quot;collaborative&quot;: false, &quot;imageType&quot;: &quot;basic&quot;, &quot;imageUrl&quot;: &quot;&quot;, &quot;timeout&quot;: 900, &quot;userPvcName&quot;: &quot;&quot;, &quot;group&quot;: &quot;&quot;, &quot;hyperplaneUserId&quot;: &quot;2a9980d9-f43c-4369-b71e-70d12d369e47&quot;, &quot;billingProjectId&quot;: { &quot;connect&quot;: { &quot;id&quot;: &quot;284f0a8e-52d9-4a57-be42-f461fc4315c7&quot; } }, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot; } }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields-2","content":"GraphQL Docs ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-example-1","content":"{ &quot;data&quot;: { &quot;createHyperHubSession&quot;: { &quot;id&quot;: &quot;f48e3b18-bced-4a8c-85b9-b3c2fdd1a06a&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;jLabUrl&quot;: null, &quot;estimatedCost&quot;: null, &quot;department&quot;: null, &quot;resourceCPUlimit&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;gpuRequest&quot;: null, &quot;startTime&quot;: &quot;2023-06-27T19:27:43.987Z&quot;, &quot;completionTime&quot;: null, &quot;timeout&quot;: 900, &quot;group&quot;: &quot;&quot;, &quot;billingProjectId&quot;: &quot;f6a3911d-e048-49f0-96d8-1abd930b66db&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"updateHyperHubSession​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#updatehyperhubsession","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature-2","content":"updateHyperHubSession( data: HyperHubSessionUpdateInput! where: HyperHubSessionWhereUniqueInput! ): HyperHubSession  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description-2","content":"Updates the fields for a specific Session based on the provided data and conditions provided. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields-2","content":"Field\tType\tDefinitiondata\tHyperHubSessionUpdateInput!\tHyperHubSession partial object that contains field values used to update, specified in the format [field]: {[action]: [value]} where\tHyperHubSessionWhereUniqueInput!\tConditional values to filter for a specific HyperHubSession object ","version":"Next","tagName":"h3"},{"title":"Request Example: Cancelling a Session​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example-cancelling-a-session","content":"mutation UpdateHyperHubSession($id: String!) { updateHyperHubSession(where: {id: $id}, data: { status: {set: &quot;cancelled&quot;} }) { id status } }  variables { &quot;id&quot;: &quot;7b728979-71b7-426c-9847-6fe3e29a6438&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields-3","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-example-2","content":"{ &quot;data&quot;: { &quot;updateHyperHubSession&quot;: { &quot;id&quot;: &quot;7b728979-71b7-426c-9847-6fe3e29a6438&quot;, &quot;status&quot;: &quot;cancelled&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"getJobStat​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#getjobstat","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature-3","content":"getJobStat( stat: StatType!, status: StatusType, timeFrame: TimeFrame! ): Int  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description-3","content":"Retrieves job count statistics based on the conditions provided. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields-3","content":"Field\tType\tDescriptionstat\tStatType!\tStatistic type options. Possible values: COUNT_ALL, COUNT_CANCELLED, COUNT_DONE, COUNT_FAILED, COUNT_IN_PROGRESS, COUNT_PENDING, COUNT_SCHEDULED, COUNT_TIMED_OUT, COUNT_TRIGGERED. status\tStatusType\tStatus type options. Possible values: ALL, SCHEDULED, TRIGGERED. timeFrame\tTimeFrame!\tTimeframe options. Possible values: TOTAL, T_1H, T_10M, T_24H. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example-2","content":"query { getJobStat(stat: COUNT_ALL, timeFrame: TOTAL) }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields-4","content":"Int ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-example-3","content":"{ &quot;data&quot;: { &quot;getJobStat&quot;: 105179 } }  ","version":"Next","tagName":"h3"},{"title":"createPipelineJob​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#createpipelinejob","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature-4","content":"createPipelineJob( data: PipelineJobCreateInput! ): PipelineJob!  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description-4","content":"Creates a Shakudo platform job, which allows users to run task scripts using custom configurations, either immediately as an “Immediate job”, at scheduled intervals as a “Scheduled Job”, or indefinitely as a “Service”. Immediate jobs: schedule = “immediate” Scheduled jobs: schedule ≠ “immediate”, schedule is set to cron schedule expression, eg. 0 0 * * * for a job running every minute Service: timeout and activeTimeout set to -1 and exposedPort != null, particularly set to a valid port ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields-4","content":"data: PipelineJob object that contains field values used to create a PipelineJob. Check PipelineJobCreateInput for specific fields. Example fields: Field\tType\tDefinitiontype\tString!\tName of Shakudo platform Podspec/Image, default or custom. Example: &quot;basic&quot; timeout\tInt!\tThe maximum time in seconds that the pipeline may run, starting from the moment of job submission. Default: -1 (never timeout). Example: 86400 activeTimeout\tInt\tThe maximum time in seconds that the pipeline may run once it is picked up. Default: -1 (never timeout). Example: 86400 maxRetries\tInt!\tThe maximum number of attempts to run your pipeline job before returning an error, even if timeouts are not reached. Default: 2 yamlPath\tString\tThe relative path to the .yaml file used to run this pipeline job. Example: &quot;example_notebooks/pipelines/python_hello_world_pipeline/pipeline.yaml&quot; exposedPort\tString\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. schedule\tString\tEither &quot;immediate&quot; for an immediate job or a cron schedule expression for a scheduled job at the specified interval. parameters\tParameterCreateNestedManyWithoutPipelineJobInput\tKey-value pairs that can be used within the container environment gitServer\tHyperplaneVCServerCreateNestedOneWithoutPipelineJobsInput\tGit server object, retrievable by searching git servers by name (hyperplaneVCServers) and using resulting id in the following manner: { connect: { id: $gitServerId } } hyperplaneUserEmail\tString!\tShakudo platform user email branchName\tString\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. If commitID is not specified, the latest commit is used. If not specified, default branch is used. podSpec\tJSON\tShakudo platform PodSpec config object as a JSON object ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example-3","content":"Variables ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields-5","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-example-4","content":"","version":"Next","tagName":"h3"},{"title":"updatePipelineJob​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#updatepipelinejob","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature-5","content":"updatePipelineJob( data: PipelineJobUpdateInput! where: PipelineJobWhereUniqueInput! ): PipelineJob  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description-5","content":"Updates the database fields of a specific PipelineJob. Check PipelineJobUpdateInput to see how to do so. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields-5","content":"Field\tType\tDescriptiondata\tPipelineJobUpdateInput!\tPipelineJob partial object that contains field values used to update, specified in the following format [field]: {[action]: [value]}. Check the PipelineJobUpdateInput type for specific fields and their descriptions. where\tPipelineJobWhereUniqueInput!\tConditional values to filter for a specific PipelineJob object. Check the PipelineJobWhereUniqueInput type for specific fields and their descriptions. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example-4","content":"mutation UpdatePipelineJob($jobId: String!, $parameterId: String!) { updatePipelineJob(where: {id: $jobId}, data: { parameters: {disconnect: {id: $parameterId}}, }) { id } }  Variables { &quot;jobId&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;parameterId&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields-6","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-example-5","content":"{ &quot;data&quot;: { &quot;updatePipelineJob&quot;: { &quot;id&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"updateParameter​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#updateparameter","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature-6","content":"updateParameter( data: ParameterUpdateInput! where: ParameterWhereUniqueInput! ): Parameter  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description-6","content":"Updates the database fields of a specific Parameter. Parameters are objects that represent environment variables within Shakudo resources like Jobs and Sessions. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields-6","content":"Field\tType\tDescriptiondata\tPipelineJobUpdateInput!\tParameter partial object that contains field values used to update, specified in the following format [field]: {[action]: [value]} , where [field] is the name of the field to update, [action] is the update action (e.g., set, increment, decrement, etc.), and [value] is the new value for the field. Check the ParameterUpdateInput type for specific fields and their descriptions. where\tPipelineJobWhereUniqueInput!\tConditional values to filter for a specific PipelineJob object. Check the PipelineJobWhereUniqueInput type for specific fields and their descriptions. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example-5","content":"mutation ($parameterId: String!, $keyValue: String, $valueValue: String) { updateParameter(where: {id: $parameterId}, data: { key: {set: $keyValue} value: {set: $valueValue} }) { id key value } }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-variables-17","content":"{ &quot;parameterId&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;keyValue&quot;: &quot;newKey&quot;, &quot;valueValue&quot;: &quot;newValue&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-type-18","content":"Parameter ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#sample-response-19","content":"{ &quot;data&quot;: { &quot;updateParameter&quot;: { &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; &quot;keyValue&quot;: &quot;newKey&quot;, &quot;valueValue&quot;: &quot;newValue&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"createHyperplaneVCServer​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#createhyperplanevcserver","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature-7","content":"createHyperplaneVCServer(data: HyperplaneVCServerCreateInput!): HyperplaneVCServer!  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description-7","content":"Creates a git server connected to a specific git repository to make it accessible on the Shakudo platform. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields-7","content":"Field\tType\tDescriptiondata\tHyperplaneVCServerCreateInput!\tHyperplaneVCServer object that contains field values used to create a git server. Check the HyperplaneVCServerCreateInput type for specific fields and their descriptions. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example-6","content":"mutation($data: HyperplaneVCServerCreateInput!) { createHyperplaneVCServer(data: $data) { id defaultBranch name url } }  Variables { &quot;data&quot;: { &quot;defaultBranch&quot;: &quot;main&quot;, &quot;name&quot;: &quot;examples-graphql-test&quot;, &quot;url&quot;: &quot;git@github.com:org/sample.git&quot; } }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields-7","content":"HyperplaneVCServer ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-example-6","content":"{ &quot;data&quot;: { &quot;createHyperplaneVCServer&quot;: { &quot;id&quot;: &quot;3aff9f7c-c208-44e2-b389-495a11708349&quot;, &quot;defaultBranch&quot;: &quot;main&quot;, &quot;name&quot;: &quot;examples-graphql-test&quot;, &quot;pipelineJobs&quot;: [], &quot;status&quot;: &quot;CREATING&quot;, &quot;url&quot;: &quot;git@github.com:org/sample.git&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"hyperplaneVCServers​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#hyperplanevcservers","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature-8","content":"hyperplaneVCServers( cursor: HyperplaneVCServerWhereUniqueInput, distinct: [HyperplaneVCServerScalarFieldEnum!], orderBy: [HyperplaneVCServerOrderByInput!], skip: Int, take: Int, where: HyperplaneVCServerWhereInput ): [HyperplaneVCServer!]!  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description-8","content":"Retrieves a list of git server instances based on conditions provided, allowing for pagination (cursor and offset-based) and filtering. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields-8","content":"Field\tType\tDescriptioncursor\tHyperplaneVCServerWhereUniqueInput\tStarting git server value to paginate from using cursor-based pagination. The current result starts from this git server record. distinct\t[HyperplaneVCServerScalarFieldEnum!]\tList of fields where their values will remain distinct per record. orderBy\t[HyperplaneVCServerOrderByInput!]\tList of fields that will be used to order the results. The ordering precedence is determined by the location in the list. skip\tInt\tThe number of records to skip from the original result. take\tInt\tThe maximum number of records to show in the result. where\tHyperplaneVCServerWhereInput\tConditional values to filter for a specific HyperplaneVCServer object. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example-7","content":" query ($name: String!) { hyperplaneVCServers(where: { name: {equals: $name } }){ id defaultBranch name pipelineJobs { id } status url serviceUrl } }  Variables { &quot;name&quot;: &quot;examples-graphql-test&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields-8","content":"Array of HyperplaneVCServer ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-example-7","content":"{ &quot;data&quot;: { &quot;hyperplaneVCServers&quot;: [ { &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;defaultBranch&quot;: &quot;main&quot;, &quot;name&quot;: &quot;examples-graphql-test&quot;, &quot;pipelineJobs&quot;: [], &quot;status&quot;: &quot;CREATED&quot;, &quot;url&quot;: &quot;git@github.com:org/sample.git&quot;, &quot;serviceUrl&quot;: &quot;sample-service-url.namespace.svc.cluster.local&quot; } ] } }  ","version":"Next","tagName":"h3"},{"title":"getHyperhubSessionDefaultPodSpec​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#gethyperhubsessiondefaultpodspec","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature-9","content":"getHyperhubSessionPodSpec( imageUrl: String = &quot;&quot; userPvcName: String = &quot;&quot; userEmail: String = &quot;&quot; imageType: String = &quot;&quot; ): JSON  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description-9","content":"Retrieves full PodSpec as JSON string for Sessions, which can be used and customized with granularity in createHyperHubSession by itself, instead of relying on creating a PodSpec object using createPodSpec, which has more limited options. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields-9","content":"Field\tType\tDescriptionimageUrl\tString\tURL of custom image, same as imageHash userPvcName\tString\tPersistent volume name as found in Kubernetes. Typically includes the drive name found on the dashboard. Default: empty string, which corresponds with default drive claim-{user-email}. userEmail\tString\tShakudo platform user account email imageType\tString\tName of Shakudo platform Podspec/Image Note: userEmail is required in Request example to identify the respective user, but is not actually required to use query. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example-8","content":"query GetHyperhubSessionPodSpec($imageType: String, $userPvcName: String = &quot;&quot;, $userEmail: String!, $imageUrl: String) { getHyperhubSessionPodSpec( imageType: $imageType, userPvcName: $userPvcName, userEmail: $userEmail, imageUrl: $imageUrl ) }  Variables { &quot;imageType&quot;: &quot;basic&quot;, &quot;userEmail&quot;: &quot;demo@shakudo.io&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields-9","content":"JSON ","version":"Next","tagName":"h3"},{"title":"getPipelineJobPodSpec​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#getpipelinejobpodspec","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature-10","content":"getPipelineJobPodSpec( parameters: ParametersInput gitServerName: String = &quot;&quot; noGitInit: Boolean = false imageUrl: String = &quot;&quot; userEmail: String = &quot;&quot; noHyperplaneCommands: Boolean = false commitId: String = &quot;&quot; branchName: String = &quot;&quot; pipelineYamlPath: String = &quot;&quot; debuggable: Boolean = false jobType: String = &quot;&quot; ): JSON  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description-10","content":"Retrieves full PodSpec as JSON string for PipelineJobs, which can be used and customized with granularity in createPipelineJob by itself, instead of relying on creating a PodSpec object using createPodSpec, which has more limited options. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields-10","content":"Field\tType\tDescriptionparameters\tParametersInput\tList of key-value parameters that are injected into the Job environment and can be used as environment variables gitServerName\tString (&quot;&quot; if not provided)\tGit Server name, corresponds with HyperplaneVCServer.name, which is the display name assigned on the dashboard noGitInit\tBoolean (false if not provided)\tFalse if git server is to be set up using default Shakudo platform workflow. Default: false imageUrl\tString (&quot;&quot; if not provided)\tIf the image is custom, then the image URL can be provided userEmail\tString! (required)\tShakudo platform user account email noHyperplaneCommands\tBoolean\tFalse if using default Shakudo platform commands on job creation. Required to use Shakudo platform jobs through the pipeline YAML, but not required if the image has its own setup. Default: false commitId\tString (&quot;&quot; if not provided)\tThe commit ID with the versions of the pipeline YAML file and pipeline scripts wanted. Ensure that both are present if the commit ID is used. If left empty, assume that the latest commit on the branch is used branchName\tString\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. If commitID is not specified, the latest commit is used. If not specified, default branch is used. pipelineYamlPath\tString (&quot;&quot; if not provided)\tThe relative path to the .yaml file used to run this pipeline job debuggable\tBoolean (false if not provided)\tWhether to enable SSH-based debugging for the job, check the following tutorial for more details jobType\tString (&quot;&quot; if not provided)\tName of Shakudo platform Podspec/Image, default or custom ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example-9","content":"query GetPipelineJobPodSpec( $parameters: ParametersInput $gitServerName: String = &quot;&quot; $noGitInit: Boolean = false $imageUrl: String = &quot;&quot; $userEmail: String! $noHyperplaneCommands: Boolean = false $commitId: String = &quot;&quot; $branchName: String! $pipelineYamlPath: String! $debuggable: Boolean = false $jobType: String! ) { getPipelineJobPodSpec( parameters: $parameters gitServerName: $gitServerName noGitInit: $noGitInit imageUrl: $imageUrl userEmail: $userEmail noHyperplaneCommands: $noHyperplaneCommands commitId: $commitId branchName: $branchName pipelineYamlPath: $pipelineYamlPath debuggable: $debuggable jobType: $jobType ) }  Variables { &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;main&quot;, &quot;pipelineYamlPath&quot;: &quot;example/pipeline.yaml&quot;, &quot;jobType&quot;: &quot;basic&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields-10","content":"JSON ","version":"Next","tagName":"h3"},{"title":"getUserServicePodSpec​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#getuserservicepodspec","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#signature-11","content":"getUserServicePodSpec( exposedPort: String = &quot;8787&quot; parameters: ParametersInput gitServerName: String = &quot;&quot; noGitInit: Boolean = false imageUrl: String = &quot;&quot; userEmail: String = &quot;&quot; noHyperplaneCommands: Boolean = false commitId: String = &quot;&quot; branchName: String = &quot;&quot; pipelineYamlPath: String = &quot;&quot; jobType: String = &quot;&quot; ): JSON  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#function-description-11","content":"Retrieves full PodSpec as JSON string for Services, which can be used and customized with granularity in createPipelineJob by itself, instead of relying on the parameters found in createPodSpec. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#input-object-fields-11","content":"Field\tType\tDescriptionexposedPort\tString\tThe exposed port for the job. Default value is 8787. parameters\tParametersInput\tList of key-value parameters that are injected into the job environment and can be used as environment variables. gitServerName\tString\tThe name of the Git server. It corresponds with the display name assigned on the HyperplaneVCServer dashboard. Default value is an empty string. noGitInit\tBoolean\tSpecifies whether to set up the Git server using the default Shakudo platform workflow. Default value is false. imageUrl\tString\tThe URL of a custom image. If the image is custom, this field can be provided. Default: &quot;&quot; (empty string) userEmail\tString\tThe email of the Shakudo platform user account. This field is required. noHyperplaneCommands\tBoolean\tSpecifies whether to use default Shakudo platform commands on job creation. It is required to use Shakudo platform jobs through the pipeline YAML, but not required if the image has its own setup. Default value is false. commitId\tString\tThe commit ID with the versions of the pipeline YAML file and pipeline scripts wanted. Ensure that both are present if commit ID is used. If left empty, assume that the latest commit on the branch is used. Default value is an empty string. branchName\tString\tThe name of the specific Git branch that contains the pipeline YAML file and pipeline scripts. If commitId is not specified, the latest commit is used. This field is required. pipelineYamlPath\tString\tThe relative path to the .yaml file used to run this pipeline job. This field is required. jobType\tString\tThe name of the Shakudo platform Podspec/Image. If the empty string is provided, the Podspec used will be basic. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#request-example-10","content":"query GetUserServicePodSpec( $exposedPort: String! = &quot;8787&quot; $parameters: ParametersInput $gitServerName: String $noGitInit: Boolean $imageUrl: String $userEmail: String! $noHyperplaneCommands: Boolean $commitId: String $branchName: String! $pipelineYamlPath: String! $jobType: String! ) { getUserServicePodSpec( exposedPort: $exposedPort parameters: $parameters gitServerName: $gitServerName noGitInit: $noGitInit imageUrl: $imageUrl userEmail: $userEmail noHyperplaneCommands: $noHyperplaneCommands commitId: $commitId branchName: $branchName pipelineYamlPath: $pipelineYamlPath jobType: $jobType ) }  Variables { &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;main&quot;, &quot;pipelineYamlPath&quot;: &quot;example/pipeline.yaml&quot;, &quot;jobType&quot;: &quot;basic&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/adminSettings/graphql#response-object-fields-11","content":"JSON ","version":"Next","tagName":"h3"},{"title":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","type":0,"sectionRef":"#","url":"/tutorials/confluenceapp","content":"","keywords":"","version":"Next"},{"title":"The problem:​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#the-problem","content":"ChatGPT's human-like capability to extract information from vast data has truly transformed the field of language models. But with a 4096-token context limit, extracting details from extensive text documents is still a challenge. There are multiple ways to get around this problem. Option one involves generating text snippets and sequentially prompting the large language model (LLM), refining the answer step by step. Although this method covers the text effectively, it falls short when it comes to time and cost efficiency due to its resource-intensive nature. Option two involves utilizing LLMs with larger context windows, such as the Claude model by Anthropic, offering a 100k-token window. However, it partially solves the problem as we need to ensure that the model can accurately and comprehensively extract from our extensive knowledge base. Option three capitalizes on the power of embeddings and similarity search and is the one we chose for the tutorial. It maintains an embedding vector store for each text snippet, calculates question embeddings, and retrieves the nearest text snippets via a similarity search on embedding. The retrieved text snippets are used to query the LLM with by constructing a prompt to obtain an answer ","version":"Next","tagName":"h2"},{"title":"Solution overview:​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#solution-overview","content":" Our proposed architecture operates as a pipeline that efficiently retrieves information from a knowledge base (in this case, Confluence) in response to user queries. It includes four main steps: Step 1: Knowledge base processing This step involves transforming information from a knowledge base into a more manageable format for subsequent stages. Information is segmented into smaller text snippets and vector representations (embeddings) of these snippets are generated for quick and easy comparison and retrieval. Here, we use Langchain's ConfluenceLoader with TextSplitter and TokenSplitter to efficiently split the documents into text snippets. Then, we create embeddings using OpenAI's ada-v2 model. Step 2: User query processing: When a user submits a question, it is transformed into an embedding using the same process applied to the text snippets. Langchain's RetrievalQA, in conjunction with ChromaDB, then identifies the most relevant text snippets based on their embeddings. Step 3: Answer generation Relevant text snippets, together with the user's question, are used to generate a prompt. This prompt is processed by our chosen LLM to generate an appropriate response to the user's query. Step 4: Streamlit service and Shakudo deployment Finally, we package everything into a Streamlit application, expose the endpoint, and deploy it on a cluster using Shakudo. This step ensures a seamless transition from development to production quickly and reliably, as Shakudo automates DevOps tasks and lets developers use Langchain, Hugging Face pipelines, and LLM models effortlessly with pre-built images. ","version":"Next","tagName":"h2"},{"title":"Setting up the environment​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#setting-up-the-environment","content":"We use OpenAI's adav2 for text embeddings and OpenAI's gpt-3.5-turbo as our LLM. OpenAI offers a range of embedding models and LLMs. The ones that we have chosen balance efficiency and cost-effectiveness, but depending on your needs, other models might be more suitable. ","version":"Next","tagName":"h2"},{"title":"ConfluenceQA initialize​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#confluenceqa-initialize","content":"This stage involves preparing the embedding model for text snippet processing and the LLM model for the final query response. Our go-to models are ada-v2 for embeddings and gpt-3.5-turbo for text generation, respectively. Learn more about embeddings from OpenAI Documentation. embedding = OpenAIEmbeddings()  After that, let’s initialize the LLM model to be used for the final LLM call to query with prompt: llm = ChatOpenAI(model_name=LLM_OPENAI_GPT35, temperature=0.)  The 'temperature' parameter in the LLM initialization impacts the randomness of the model's responses, with higher values producing more random responses and lower values producing more deterministic ones. Here, we've set it to 0, which makes the output entirely deterministic. ","version":"Next","tagName":"h3"},{"title":"OpenAI key setup​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#openai-key-setup","content":"To wrap up the environment setup, we specify the OpenAI API key, a prerequisite for LangChain's functionality. Make sure the API environment key is named OPENAI_API_KEY – it's a requirement for LangChain. import os os.environ[&quot;OPENAI_API_KEY&quot;] =&quot;sk-**&quot;  For security, we store the key in a .env file, and ensure the key is correctly recognized by our application. Never print or share your keys as this can expose them to potential security threats. With our environment set up, we are now ready to start building our Confluence Q&amp;A application.  ","version":"Next","tagName":"h3"},{"title":"Step 1: Creating an Embedding Store from the knowledge base:​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#step-1-creating-an-embedding-store-from-the-knowledge-base","content":"In this step, we will extract the documents from the Confluence knowledge base, transform these documents into text snippets, generate embeddings for these snippets, and store these embeddings in a Chroma store. ","version":"Next","tagName":"h2"},{"title":"Extract the documents with ConfluenceLoader​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#extract-the-documents-with-confluenceloader","content":"ConfluenceLoader is a powerful tool that allows us to extract documents from a Confluence site using login credentials. It currently supports username/api_key and OAuth2 authentication methods. Be careful when handling these credentials, as they are sensitive information. config = {&quot;persist_directory&quot;:&quot;./chroma_db/&quot;, &quot;confluence_url&quot;:&quot;https://templates.atlassian.net/wiki/&quot;, &quot;username&quot;:None, &quot;api_key&quot;:None, &quot;space_key&quot;:&quot;RD&quot; } persist_directory = config.get(&quot;persist_directory&quot;,None) confluence_url = config.get(&quot;confluence_url&quot;,None) username = config.get(&quot;username&quot;,None) api_key = config.get(&quot;api_key&quot;,None) space_key = config.get(&quot;space_key&quot;,None) ## 1. Extract the documents loader = ConfluenceLoader( url=confluence_url, username = username, api_key= api_key ) documents = loader.load( space_key=space_key, limit=100 )  ","version":"Next","tagName":"h3"},{"title":"Splitting Documents into Text Snippets:​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#splitting-documents-into-text-snippets","content":"Next, we split these documents into smaller, manageable text snippets. We employ CharacterTextSplitter and TokenTextSplitter from LangChain for this task. text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0) texts = text_splitter.split_documents(documents) text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name=&quot;cl100k_base&quot;) # This the encoding for text-embedding-ada-002 texts = text_splitter.split_documents(texts)  ","version":"Next","tagName":"h3"},{"title":"Generating Embeddings and Adding to Chroma Store:​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#generating-embeddings-and-adding-to-chroma-store","content":"Lastly, we generate embeddings for these text snippets and store them in a Chroma database. The Chroma class handles this with the help of an embedding function. if persist_directory and os.path.exists(persist_directory): vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding) else: vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)  We have now successfully transformed our knowledge base into a store of embedded text snippets, ready for efficient querying in the subsequent stages of our pipeline. For a dynamic confluence pages, the vector store creation process can be scheduled with the help of Shakudo jobs pipeline (Link Shakudo Jobs pipeline documentation here)  ","version":"Next","tagName":"h3"},{"title":"Step 2: Computing questions embeddings and finding relevant snippets​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#step-2-computing-questions-embeddings-and-finding-relevant-snippets","content":"ChromaDB is an advanced indexing system that accelerates retrieval by finding and matching things that have the same meaning. This makes our process quicker and more accurate. Next, we create &quot;questions embeddings&quot; to understand the meaning behind the questions. This is like converting the questions into a language that ChromaDB speaks fluently. Now, ChromaDB can pinpoint the most useful snippets of information. These snippets form the foundation of smart and detailed responses in our app. These steps enhance the effectiveness of our RetrievalQA chain. This ensures that our app delivers fast, accurate, and useful answers to the questions received. In the next step, we will show you how it works along with prompt engineering.  ","version":"Next","tagName":"h2"},{"title":"Step 3: Prompt engineering and LLM query​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#step-3-prompt-engineering-and-llm-query","content":"In this step, we construct a prompt for our LLM. A prompt is a message that sets the context and asks the question that we want the LLM to answer. To pass a custom prompt with context and question, you can define your own template as follows: custom_prompt_template = &quot;&quot;&quot;You are a Confluence chatbot answering questions. Use the following pieces of context to answer the question at the end. If you don't know the answer, say that you don't know, don't try to make up an answer. {context} Question: {question} Helpful Answer:&quot;&quot;&quot; CUSTOMPROMPT = PromptTemplate( template=custom_prompt_template, input_variables=[&quot;context&quot;, &quot;question&quot;] ) ## Inject custom prompt qa.combine_documents_chain.llm_chain.prompt = CUSTOMPROMPT retriever = vectordb.as_retriever(search_kwargs={&quot;k&quot;:4}) #Top4-Snippets qa = RetrievalQA.from_chain_type(llm=llm, chain_type=&quot;stuff&quot;,retriever=retriever) question = &quot;How to organize content in a space?&quot; answer = qa.run(question) print(answer) # Answer: To organize content in a space, you can create pages or blogs for different types of content. Pages can have child pages, which allows you to organize content into categories and subcategories. You can also use labels to categorize and identify content, and create a table of contents for your space using the Content Report Table Macro. Additionally, you can customize the sidebar to make it easier to navigate through your space and add a search box to find content within your space.  In the following class, ConfluenceQA, we package all the necessary steps that include initializing the models, embedding, and combining the retriever and answer generator into one organized module. This encapsulation improves code readability and reusability.  class ConfluenceQA: def __init__(self,config:dict = {}): self.config = config self.embedding = None self.vectordb = None self.llm = None self.qa = None self.retriever = None ... # You can see the full script on Github  Once the ConfluenceQA class is set up, you can initialize and run it as follows: # Configuration for ConfluenceQA config = {&quot;persist_directory&quot;:&quot;./chroma_db/&quot;, &quot;confluence_url&quot;:&quot;https://templates.atlassian.net/wiki/&quot;, &quot;username&quot;:None, &quot;api_key&quot;:None, &quot;space_key&quot;:&quot;RD&quot;} # Initialize ConfluenceQA confluenceQA = ConfluenceQA(config=config) confluenceQA.init_embeddings() confluenceQA.init_models() # Create Vector DB confluenceQA.vector_db_confluence_docs() # Set up Retrieval QA Chain confluenceQA.retreival_qa_chain() # Query the model question = &quot;How to organize content in a space?&quot; confluenceQA.answer_confluence(question)  Remember that the above approach is a structured way to access the Confluence knowledge base and get your desired information using a combination of embeddings, retrieval, and prompt engineering. However, the success of the approach would largely depend on the quality of the knowledge base and the prompt that is used to question the LLM. ","version":"Next","tagName":"h2"},{"title":"Step 4: Streamlit app​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#step-4-streamlit-app","content":"Let’s wrap our solution in a Streamlit app and deploy it as a service. This will make it accessible either locally or on a cloud-based cluster. ","version":"Next","tagName":"h2"},{"title":"Building a Streamlit App​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#building-a-streamlit-app","content":"To create an interactive web application around our ConfluenceQA class, we use Streamlit, a Python library that simplifies app creation. Below is the breakdown of the code: We start by importing necessary modules and initializing our ConfluenceQA instance: import streamlit as st from confluence_qa import ConfluenceQA st.set_page_config( page_title='Q&amp;A Bot for Confluence Page', page_icon='⚡', layout='wide', initial_sidebar_state='auto', ) st.session_state[&quot;config&quot;] = {} confluence_qa = None  We then define a sidebar form for user inputs: with st.sidebar.form(key ='Form1'): # Form fields and submit button go here  And finally, we provide a user interface for asking questions and getting answers: st.title(&quot;Confluence Q&amp;A Demo&quot;) question = st.text_input('Ask a question', &quot;How do I make a space public?&quot;) if st.button('Get Answer'): # Code to generate and display the answer  This Streamlit app can be launched locally or on a cluster and allows us to interact with our Confluence Q&amp;A system in a user-friendly manner. ","version":"Next","tagName":"h3"},{"title":"Deploying with Shakudo​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#deploying-with-shakudo","content":"Finally our app is ready, and we can deploy it as a service on Shakudo. The platform makes the process of deployment easier, allowing you to quickly put your app online. To deploy our app on Shakudo, we need two key files: pipeline.yaml, which describes our deployment pipeline, and run.sh, a bash script to set up and run our application. Here's what these files look like: ‘pipeline.yaml’: pipeline: name: &quot;QA demo&quot; tasks: - name: &quot;QA app&quot; type: &quot;bash script&quot; port: 8787 bash_script_path: &quot;LLM/confluence_app/run.sh&quot;  ‘run.sh’: PROJECT_DIR=&quot;$(cd -P &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot; cd &quot;$PROJECT_DIR&quot; pip install -r requirements.txt export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python export STREAMLIT_RUNONSSAVE=True streamlit run app.py --server.port 8787 --browser.serverAddress localhost  In this script: Set the project directory and navigate into it.Install the necessary Python libraries from the requirements.txt file.Set two environment variables to: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION and STREAMLIT_RUNONSSAVE.Runthe Streamlit app on port 8787. After you have these files ready, navigate to the service tab and click the '+' button to create a new service. Fill in the details in the service creation panel and click 'CREATE'. On the services dashboard, click on the ‘Endpoint URL’ button to access the application. Check out the Shakudo docs for a more detailed explanation.  Now, your application is live! You can browse through the user interface to see how it works.  ","version":"Next","tagName":"h3"},{"title":"Adopting Shakudo into your workflow​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#adopting-shakudo-into-your-workflow","content":"Shakudo is designed to facilitate the entire lifecycle of data and AI applications. Automating all stages of the development process, including the stack integrations, deployment, and the ongoing management of data-driven applications.  By using Shakudo, you’ll take advantage of: Integrated Environment: Shakudo is designed to provide compatibility across a diverse range of best-of-breed data tools. This unique feature facilitates experimentation and the creation of a data stack that's not only more reliable and performant but also cost-effective.Continuous Integration and Deployment (CI/CD): Shakudo automates the stages of application deployment with managed Kubernetes. This ensures rapid deployment and eliminates the complex task of self-managing a cluster, resulting in more reliable releases and data flows.Services Deployment and Pipelines Orchestration: With Shakudo you can deploy your app as a service that shows an endpoint, be it a dashboard, a website, or an API endpoint. It also facilitates the creation and orchestration of data pipelines, reducing setup time to only a few minutes. Conclusion This blog provides a comprehensive guide to developing a Confluence Q&amp;A application utilizing the power of Shakudo, Langchain, and ChatGPT, aiming to resolve the challenge posed by ChatGPT's token limit when extracting information from extensive text documents. We used a new method leveraging embeddings and similarity search, ensuring a more efficient process of retrieving accurate information from a knowledge base. Are you ready to start building more efficient data-driven applications? Check out our blog post on “How to Easily and Securely Integrate LLMs for Enterprise Data Initiatives”. Feel free to reach out to our team to check out how you can start using Shakudo to help your business. Thank you for reading and happy coding! References: This code is adapted based on the work in LLM-WikipediaQA, where the author compares FastChat-T5, Flan-T5 with ChatGPT running a Q&amp;A on Wikipedia Articles.Buster: Overview figure inspired from Buster’s demo. Buster is a QA bot that can be used to answer from any source of documentation.Claude model: 100K Context Window model from Anthropic AI ","version":"Next","tagName":"h2"},{"title":"Custom YAML Examples","type":0,"sectionRef":"#","url":"/tutorials/custom-yaml-examples","content":"","keywords":"","version":"Next"},{"title":"Add a Liveness Probe​","type":1,"pageTitle":"Custom YAML Examples","url":"/tutorials/custom-yaml-examples#add-a-liveness-probe","content":"Use a liveness probe to automatically restart your container if it becomes unhealthy. You can add probes directly under your container spec in the Custom YAML panel. See additional background in the Service docs - Health Probes. Example (HTTP GET to /health): spec: template: spec: containers: - name: your-container ports: - containerPort: 8787 livenessProbe: httpGet: path: /health port: 8787 initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3  Example (simple TCP socket on the exposed port): spec: template: spec: containers: - name: your-container ports: - containerPort: 8787 livenessProbe: tcpSocket: port: 8787 initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3  ","version":"Next","tagName":"h2"},{"title":"Add a Shakudo Secret to a Job or Service​","type":1,"pageTitle":"Custom YAML Examples","url":"/tutorials/custom-yaml-examples#add-a-shakudo-secret-to-a-job-or-service","content":"Shakudo Secrets let you pass sensitive data securely to workloads. Start by creating a Secret in the dashboard (see the Secrets docs for details and naming guidance). When attached via the Shakudo UI, secrets are injected automatically as: Environment variables: HYPERPLANE_CUSTOM_SECRET_KEY_{KEY}Files: /etc/hyperplane/secrets/{secret_name}/{secret_key} If you prefer to wire a Kubernetes Secret explicitly in Custom YAML, start by filling in as much of the Jobs or Microservices creation form as you need, then click the &quot;Customize Pod YAML&quot; button and use one of the patterns below (the Secret must already exist in the same namespace as the workload): Option A — reference a single key as an environment variable: spec: template: spec: containers: - name: your-container env: - name: API_KEY valueFrom: secretKeyRef: name: my-secret # Shakudo Secret name key: api-key # key inside the secret  Option B — mount the whole secret as files and read from the mount path: spec: template: spec: containers: - name: your-container volumeMounts: - name: my-secret-vol mountPath: /etc/hyperplane/secrets/my-secret readOnly: true volumes: - name: my-secret-vol secret: secretName: my-secret  Tip: With Shakudo-attached Secrets, your code can also read files directly from /etc/hyperplane/secrets/{secret_name}/{secret_key} or use the auto-injected env var HYPERPLANE_CUSTOM_SECRET_KEY_{UPPERCASE_KEY}.  More examples coming soon! ","version":"Next","tagName":"h2"},{"title":"Create a Flask App","type":0,"sectionRef":"#","url":"/tutorials/flaskapp","content":"","keywords":"","version":"Next"},{"title":"1. Prepare your Flask App​","type":1,"pageTitle":"Create a Flask App","url":"/tutorials/flaskapp#1-prepare-your-flask-app","content":"First and foremost, you need to have a working Flask API program. You can either develop it using Shakudo's Session or on your local machine. We have a simple working version that you could find here Flask Program Then push this code to a repository that are in-sync on Shakudo platform. ","version":"Next","tagName":"h2"},{"title":"2. Set up micro-service on Shakudo dashboard​","type":1,"pageTitle":"Create a Flask App","url":"/tutorials/flaskapp#2-set-up-micro-service-on-shakudo-dashboard","content":"From the sidebar, go to Microservices panel and click to Create Microservices button on the right hand side.  On the General tab, you have to choose the value for the following field: Name: The name of your service Endpoint: How you like the public URL would be. For example if you choose.flask_tut as endpoint, the public URL would be https://staging-aks.canopyhub.io/flask_tut, in which https://staging-aks.canopyhub.io is the url of the platform. Environment Config: This includes hardware requirement and base environment to run the micro-service. In the Pipeline section, you will need to point to the program's entrypoint.  On the Advance tab, choose the git repository in which the code is stored. Apart from that, we need to set up the value for port. In this example, I choose 8000 since this is the port being used by the micro-service in the example.  On the Parameter tab, you can set up some environment variables being used by the app.  Once the configuration is done, click to Create Microservice to set up the service. ","version":"Next","tagName":"h2"},{"title":"3. View micro-service status​","type":1,"pageTitle":"Create a Flask App","url":"/tutorials/flaskapp#3-view-micro-service-status","content":"Once the service is up, you can view its corresponding status or its endpoint on the dashboard. Moreover, we provide options for the user to interact with the microservice if necessary: Turn off: Scale down the service if it's not being userRestart: Redeploy the service with the latest code, in-cluster URL will be the samePublish: Allow other users to see the status of this serviceClone: Create a new service based on the current settings.  ","version":"Next","tagName":"h2"},{"title":"Open WebUI","type":0,"sectionRef":"#","url":"/tutorials/openwebui","content":"Open WebUI Open WebUI is an extensible, feature-rich, and user-friendly self-hosted AI interface designed to operate entirely offline. It supports LLM models from OpenAI, Anthropic, self-hosted models, and more. Users can also define and expose their own model backends through the chat interface. See our user guide here! 📖 Open WebUI User Guide →","keywords":"","version":"Next"},{"title":"Create a Shakudo Service Account with Custom Image Pull Secret","type":0,"sectionRef":"#","url":"/tutorials/k8s-serviceaccount-ecr-secret","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Create a Shakudo Service Account with Custom Image Pull Secret","url":"/tutorials/k8s-serviceaccount-ecr-secret#prerequisites","content":"Permissions to create secrets, and service accounts.An existing private repository with images to pull. ","version":"Next","tagName":"h2"},{"title":"1 Create a service account​","type":1,"pageTitle":"Create a Shakudo Service Account with Custom Image Pull Secret","url":"/tutorials/k8s-serviceaccount-ecr-secret#1-create-a-service-account","content":" The name can be hyperplane-pipelines-sa as an example. Following steps will be finished in Shakudo cloud terminal.  ","version":"Next","tagName":"h2"},{"title":"2 Set variables​","type":1,"pageTitle":"Create a Shakudo Service Account with Custom Image Pull Secret","url":"/tutorials/k8s-serviceaccount-ecr-secret#2-set-variables","content":"NS=hyperplane-pipelines SA_NAME=hyperplane-pipelines-sa PULL_SECRET_NAME=ecr-pull-secret  ","version":"Next","tagName":"h2"},{"title":"3 Create the image pull secret​","type":1,"pageTitle":"Create a Shakudo Service Account with Custom Image Pull Secret","url":"/tutorials/k8s-serviceaccount-ecr-secret#3-create-the-image-pull-secret","content":"Create a docker-registry secret using your existing username password. kubectl create secret docker-registry &quot;$PULL_SECRET_NAME&quot; \\ --docker-server=&quot;$PRIVATE_REGISTRY&quot; \\ --docker-username=username \\ # This should be your username --docker-password-stdin \\ -n &quot;$NS&quot;  If the secret exists and you need to refresh the token, delete and recreate it: kubectl delete secret &quot;$PULL_SECRET_NAME&quot; -n &quot;$NS&quot; --ignore-not-found kubectl create secret docker-registry &quot;$PULL_SECRET_NAME&quot; \\ --docker-server=&quot;$PRIVATE_REGISTRY&quot; \\ --docker-username=username \\ # This should be your username --docker-password-stdin \\ -n &quot;$NS&quot;  ","version":"Next","tagName":"h2"},{"title":"4) Patch the Service Account and attach the imagePullSecret​","type":1,"pageTitle":"Create a Shakudo Service Account with Custom Image Pull Secret","url":"/tutorials/k8s-serviceaccount-ecr-secret#4-patch-the-service-account-and-attach-the-imagepullsecret","content":"kubectl patch serviceaccount &quot;$SA_NAME&quot; -n &quot;$NS&quot; \\ -p &quot;{\\&quot;imagePullSecrets\\&quot;:[{\\&quot;name\\&quot;:\\&quot;$PULL_SECRET_NAME\\&quot;}]}&quot;  ","version":"Next","tagName":"h2"},{"title":"5) Verify​","type":1,"pageTitle":"Create a Shakudo Service Account with Custom Image Pull Secret","url":"/tutorials/k8s-serviceaccount-ecr-secret#5-verify","content":"kubectl get sa &quot;$SA_NAME&quot; -n &quot;$NS&quot; -o yaml | grep -A2 imagePullSecrets  You should see the secret name under imagePullSecrets. ","version":"Next","tagName":"h2"},{"title":"6) Use the Service Account in your workloads​","type":1,"pageTitle":"Create a Shakudo Service Account with Custom Image Pull Secret","url":"/tutorials/k8s-serviceaccount-ecr-secret#6-use-the-service-account-in-your-workloads","content":"Reference the Service Account so Pods inherit the pull secret. ","version":"Next","tagName":"h2"},{"title":"(Optional) Attach the pull secret to the default Service Account, to allow every pod within the namespace be able to pull the images by default.​","type":1,"pageTitle":"Create a Shakudo Service Account with Custom Image Pull Secret","url":"/tutorials/k8s-serviceaccount-ecr-secret#optional-attach-the-pull-secret-to-the-default-service-account-to-allow-every-pod-within-the-namespace-be-able-to-pull-the-images-by-default","content":"kubectl patch serviceaccount default -n &quot;$NS&quot; \\ -p &quot;{\\&quot;imagePullSecrets\\&quot;:[{\\&quot;name\\&quot;:\\&quot;$PULL_SECRET_NAME\\&quot;}]}&quot;  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"Create a Shakudo Service Account with Custom Image Pull Secret","url":"/tutorials/k8s-serviceaccount-ecr-secret#troubleshooting","content":"ImagePullBackOff / ErrImagePull: Secret must exist in the same namespace as the Pod.Ensure --docker-server matches your custom registry.Check image URI correctness and repository permissions. ","version":"Next","tagName":"h2"},{"title":"Build and Deploy with Streamlit on Shakudo","type":0,"sectionRef":"#","url":"/tutorials/streamlitapp","content":"","keywords":"","version":"Next"},{"title":"Building the App​","type":1,"pageTitle":"Build and Deploy with Streamlit on Shakudo","url":"/tutorials/streamlitapp#building-the-app","content":"Let's create a small dashboard for demonstration purposes. We will call it dashtest.py. Before we get started, let's create a script called run.sh so we can just execute the script to easily run our streamlit app. Its contents should be as follows: streamlit run streamlit_example/dashtest.py --server.port 8787 --browser.serverAddress localhost  Remember to adjust your paths! When developing on Shakudo, the localhost port will automatically be forwarded when using vscode or codeserver. In vscode, the following notification will pop up to access the app from the local browser:  We will also maintain a requirements.txt file to keep track of our dependencies (in this case, that will be streamlit alone). It is recommended to keep track of the exact version of your dependencies to ease reproducibility, but that is not necessary for this demo. streamlit  Now let's get started with the app proper. import streamlit as st  Let's set the page title and icon in our dashtest.py script: st.set_page_config( page_title=&quot;Shakudo Streamlit Example&quot;, page_icon=&quot;:shark:&quot; )  And run the startup script to make sure everything is working fine. If all goes well, you should be able to access the page on localhost:8787. While the page is blank, the title should look like in this picture:   Next, let's organize a basic layout and play with some simple widgets: st.title(&quot;This is an example Title&quot;) st.subheader(&quot;multiple columns&quot;) col1, col2, col3 = st.columns([2, 4, 5]) with col1: st.button(&quot;Click me&quot;, on_click=lambda: st.balloons()) with col2: st.text(&quot;this is a text place holder&quot;) with col3: st.table([[1, 2, 3], [4, 5, 6]])  Here we created 3 columns spanning 2, 4, and 5 units of space. We then filled the first column with a button, the second with a simple text, and the third with a table. If all went well, we can now run our start script and Streamlit will load the page in the default browser. The result should look like this:   col4, col5 = st.columns([5, 5]) with col4: st.subheader(&quot;json content&quot;) st.json(&quot;&quot;&quot;{ &quot;key&quot; : &quot;value&quot;, &quot;key2&quot; : 123, &quot;somelist&quot; : [1, 2, &quot;3&quot;] }&quot;&quot;&quot;) with col5: st.subheader('charts') st.bar_chart([1, 4, 5, 3, 2, 6])  Streamlit supports many kinds of elements, including json displays and various charts, which are displayed beautifully without any user styling or tweaking.   st.subheader(&quot;Progress bar&quot;) import time progressbar = st.empty() n = 0 with st.expander(&quot;expandable&quot;): st.text(&quot;&quot;&quot;This is some text. &quot;&quot;&quot;* 30) with st.sidebar: st.slider(&quot;slider&quot;, 0, 100, 50) st.select_slider(&quot;select slider&quot;, list(range(10))) st.selectbox('select box', ['apple', 'banana', 'pear']) st.checkbox('check box 1') st.checkbox('check box 2') st.checkbox('check box 3') st.checkbox('check box 4') st.time_input(&quot;time&quot;) st.date_input(&quot;Date&quot;) st.text_input(&quot;text&quot;) while 1: n += 1 with progressbar: st.progress(n) time.sleep(1) if n == 100: n = 0  Here we show how streamlit allows us to easily interact with controls such as progress bars: it's as simple as a st.progress after &quot;entering&quot; the progressbar in a with block. Streamlit also allows quickly creating common UI elements like sidebars and supports various user inputs, like dates, times, dropdown selectors, and more. The final result is shown below.   ","version":"Next","tagName":"h2"},{"title":"Deploying on Shakudo​","type":1,"pageTitle":"Build and Deploy with Streamlit on Shakudo","url":"/tutorials/streamlitapp#deploying-on-shakudo","content":"For long-running tasks, Shakudo provides Services, which are defined with a simple yaml file that described the set of steps to run. For our deployment, all we need is the following pipeline definition: pipeline: name: &quot;streamlit&quot; requirements: &quot;streamlit_example/requirements.txt&quot; tasks: - name: &quot;dashbaord example&quot; type: &quot;bash script&quot; port: 8787 bash_script_path: &quot;streamlit_example/run.sh&quot;  That's it! Now we can create a service, specify the repository where we pushed the code and yaml, the path from the repository's root, and run the service. Our app is now available on port 8787 at the endpoint we selected during service configuration on Shakudo. ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}