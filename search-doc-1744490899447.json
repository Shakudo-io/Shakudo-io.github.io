{"searchDocs":[{"title":"First Blog Post","type":0,"sectionRef":"#","url":"/blog/first-blog-post","content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","keywords":"","version":null},{"title":"Long Blog Post","type":0,"sectionRef":"#","url":"/blog/long-blog-post","content":"This is the summary of a very long blog post, Use a &lt;!-- truncate --&gt; comment to limit blog post size in the list view. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","keywords":"","version":null},{"title":"MDX Blog Post","type":0,"sectionRef":"#","url":"/blog/mdx-blog-post","content":"Blog posts support Docusaurus Markdown features, such as MDX. tip Use the power of React to create interactive blog posts. &lt;button onClick={() =&gt; alert('button clicked!')}&gt;Click me!&lt;/button&gt; Click me!","keywords":"","version":null},{"title":"Welcome","type":0,"sectionRef":"#","url":"/blog/welcome","content":"Docusaurus blogging features are powered by the blog plugin. Simply add Markdown files (or folders) to the blog directory. Regular blog authors can be added to authors.yml. The blog post date can be extracted from filenames, such as: 2019-05-30-welcome.md2019-05-30-welcome/index.md A blog post folder can be convenient to co-locate blog post images: The blog supports tags as well! And if you don't want a blog: just delete this directory, and use blog: false in your Docusaurus config.","keywords":"","version":null},{"title":"Installation","type":0,"sectionRef":"#","url":"/Getting started/installation","content":"Installation Shakudo is a Kubernetes-based system that can be installed on any cloud or on-premises servers. Shakudo can be installed using Terraform and Helm. To install Shakudo, follow the steps below: Clone the repository (the repository link can be obtained from the Shakudo team)Install the dependenciesRun the Terraform script to obtain resourcesRun the Helm install command to install the Shakudo platform","keywords":"","version":"Next"},{"title":"Log in","type":0,"sectionRef":"#","url":"/Getting started/login","content":"Log in You can login to the Shakudo UI (usually at https://yourcompanyname.hyperplane.dev) with the following Single SignOn options Google workspaceMicrosoft Azure Active DirectoryOkta Identity Cloud You can also access the Shakudo platform through kubectl or with tools like k9s to access the Kubernetes cluster on the terminal. You will need a kubeconfig file for the Kubernetes cluster, which can be obtained on the kubernetes cluster page on your cloud provider. Your Kubernetes cluster admin can assist with the kubeconfig file, or contact Shakudo Support.","keywords":"","version":"Next"},{"title":"What is Shakudo","type":0,"sectionRef":"#","url":"/home","content":"What is Shakudo Shakudo is the data and AI operating system on your VPC. Shakudo creates compatibility across the best-of-breed data tools for a more reliable, performant, and cost effective data and AI operating system. On Shakudo, data teams can choose and mix and match best-of-breed software and try out new emerging tools without DevOps overhead. On Shakudo, the workflow is simplified with the Shakudo components: Sessions are development environments on Shakudo. They come with pre-configured resources, environment variables, mounted credentials, network connections, and connections to databases, so users can start development without any additional setup. Jobs run code -- from a single step bash script to a multi-step pipeline -- from beginning to end. Users can connect a git repository and run pushed code, or deploy custom Docker images. Jobs can be triggered on demand, on a schedule, on with Kafka. Microservices run frontend or backend applications with frameworks like Flask, fastAPI, Django, NextJS, React, and more. Similar to jobs, you can use code in a linked git repositories, or simply use a pre-built Docker image. A Microservice exposes an endpoint, which can be a dashboard, a website or an API endpoint. Shakudo Stack Components are best-of-breed, production-ready data tools and frameworks preconfigured to work seamlessly on the Shakudo Platform. Shakudo manages the networking, credentials, SSO, security, and interconnectivity between components so you can install and use Stack Components with one click. Shakudo adds new integrations every day. Visit our integration page to see the latest list. If you can't find the tool you are looking for, please send us an integration request.","keywords":"","version":"Next"},{"title":"Quick start","type":0,"sectionRef":"#","url":"/Getting started/getstarted","content":"","keywords":"","version":"Next"},{"title":"1. Start a Session​","type":1,"pageTitle":"Quick start","url":"/Getting started/getstarted#1-start-a-session","content":"Sessions are pre-configured virtual machines (VMs) with connections to all the tools and resources necessary for development. To begin development, navigate to the Sessions tab on the dashboard and click the +Start a Session button. You will see a dialog window to start a session like the image below.  Image: Choose the image type to use in the session. To add new image to the dropdown permanently, you can add a new Environment Config under the admin settings. Please checkout EC for more details. In this example, we are going to use the Basic image in the dropdown. For more information on Session Types and other configurations check out the Guide on Sessions ImageURL: You can paste any image URL in the Image Url field. This will overwrite the Image above field that we have chosen and use the ImageURL instead. This is useful for quick testing, we need to make sure the image registry credentials are added to the shakudo environment. This is usually setup at installation time. If you'd like to add more image registry access, please contact your Kubernets cluster admin or Shakudo support. Timeout: Choose the idle timeout for the session. Idle timeout is defined as the number of seconds from which the session has been continuously idling. It's default to 15 minutes. Drive: Drive is the persistent volume that this session will use. Persistent volumes is a Kubernetes term, imaging it as a hard drive in a laptop. You can have multiple drives and manage your drives by clicking on the icon to the right of the Drive field. ","version":"Next","tagName":"h2"},{"title":"2. Access the Session​","type":1,"pageTitle":"Quick start","url":"/Getting started/getstarted#2-access-the-session","content":"Once the Session is ready, you'll see a Jupyterlab icon and a SSH icon. If your image has CodeServer, you will also see a VsCode icon. To see the different ways of accessing the session, checkout the Guide on Sessions. In this example, we'll use the Jupyterlab option.  ","version":"Next","tagName":"h2"},{"title":"3. Process with Pandas​","type":1,"pageTitle":"Quick start","url":"/Getting started/getstarted#3-process-with-pandas","content":"The dataset we are using is the flight dataset in a public AWS S3 bucket. The dataset has 22 CSV files and has a total size of 11 GB. The objective is to identify the airport with the most frequently delayed arriving flights. This can be achieved through a few simple Pandas DataFrame transformations. Your session should already be set up to connect to your internal data source if this is a local storage bucket. Let's first use the groupby and nlargest function in Pandas to benchmark. ## loop through 22 files and groupby one by one, on the 16vCPU 16 GB RAM node that the session is on from tqdm.notebook import tqdm results = [] for file in tqdm(files): df = pd.read_csv( f&quot;s3://{file}&quot;, encoding = &quot;ISO-8859-1&quot;, usecols = ['DepTime','FlightNum','DepDelay','Origin', 'Dest','Distance'] ) df['isDelayed'] = df.DepDelay.fillna(16) &gt; 15 df_delayed = df.groupby(['Origin','Dest']).isDelayed.mean() results.append(df_delayed) df_results = pd.concat(results).reset_index() df_results = df_results.groupby(['Origin','Dest']).mean() df_results = df_results[df_results.isDelayed==1].reset_index() print(f'found {len(df_results)} most delayed flights Origin and destination pairs')  The processing time of the above operation is 6 minutes and 26 seconds on a 16 vCPU 16GB RAM machine. There are many ways to speed things up, here we use distributed Dask. ","version":"Next","tagName":"h2"},{"title":"4. Spin up a Dask cluster​","type":1,"pageTitle":"Quick start","url":"/Getting started/getstarted#4-spin-up-a-dask-cluster","content":"Dask is a powerful distributed computing framework and can scale from multi-core local machine to large distributed clusters. Dask provides a familiar user interface by mirroring the APIs of other libraries in the PyData ecosystem including: Pandas, Scikit-learn and NumPy, leading to a shallow learning curve. We can use the Shakudo package notebook_common to spin up a fully configured Dask cluster with preemptible nodes. You can specify the number of workers with argument num_workers or specify more specs to better fit the computation. Shakudo will automatically choose a cluster configuration for you and provides a Dask dashboard link to monitor progress. from hyperplane import notebook_common as nc num_workers = 2 ## number of worker nodes total_memory = 12 ## total memory size for the worker nodes in GB cors_per_worker = 15 ## total number of cores for the worker nodes nprocs = 3 ## number of processes for each worker node ram_gb_per_proc = total_memory/nprocs ## calculated memory size per processes in GB nthreads = int(cors_per_worker/nprocs) ## calculated number of threads per processes client, cluster = nc.initialize_cluster( num_workers = num_workers, nprocs = nprocs, nthreads = nthreads, ram_gb_per_proc = ram_gb_per_proc, cores_per_worker = cors_per_worker, node_selector = {} )  You will be able to see the spinning up logs of the Dask cluster and the link to the Dask dashboard. 👉 Shakudo Platform: selecting worker node pool 👉 Shakudo Platform: selecting scheduler node pool Creating scheduler pod on cluster. This may take some time. 👉 Shakudo Platform: spinning up a dask cluster with a scheduler as a standalone container. 👉 Shakudo Platform: In a few minutes you'll be able to access the dashboard at https://ds.hyperplane.dev/dask-cluster-e002f3d0-b18d-4027-81c5-bed613eb63a4/status 👉 Shakudo Platform: to get logs from all workers, do `cluster.get_logs()`  By clicking on the link above you'll see the unique distributed Dask dashboard. ","version":"Next","tagName":"h2"},{"title":"5. Process data in Dask​","type":1,"pageTitle":"Quick start","url":"/Getting started/getstarted#5-process-data-in-dask","content":"To run the code from step 3 on a Dask cluster, we just need to swap the Pandas API to the Dask API. Dask does lazy computation, the last line .compute() function triggers the actual computation. You can find information on the Dask concepts and Dask best practices page. Check out the Dask official documentation for more. import dask.dataframe as dd df = dd.read_csv([f&quot;s3://{file}&quot; for file in files], encoding = &quot;ISO-8859-1&quot;, usecols = ['DepTime','FlightNum','DepDelay','Origin', 'Dest','Distance'], dtype={'Distance': 'float64'} ) df['isDelayed'] = df.DepDelay.fillna(16) &gt; 15 df_delayed = df.groupby(['Origin','Dest']).isDelayed.mean() df_results = df_delayed[df_delayed==1].compute().reset_index() print(f'found {len(df_results)} most delayed flights Origin and destination pairs')  The Dask operation took 20 seconds using 2 remote 16 vCPU 16GB RAM Dask nodes. Comparing to Pandas, that's a ~20x speed up with only 2 extra nodes! After using Dask it's good practice to close the cluster after the computation. Add the line below at the end of your notebook: client.shutdown()  ","version":"Next","tagName":"h2"},{"title":"6. Creating the YAML file for deployment​","type":1,"pageTitle":"Quick start","url":"/Getting started/getstarted#6-creating-the-yaml-file-for-deployment","content":"Now the data processing notebook is developed and tested, to automatically run this notebook on a schedule as in most production setups, we can simply add a pipeline.yaml file to build a pipeline. To read more on pipeline YAML files please visit the create a pipeline job page. Open a text file on your browser Session by clicking on the blue + button on the top left of the side bar. Copy and paste the content below: pipeline: name: &quot;data prep pipeline&quot; tasks: - name: &quot;dask processing data&quot; type: &quot;jupyter notebook&quot; notebook_path: &quot;example_notebooks/doc_demo/quick_start/dask.ipynb&quot; # path to the script to run notebook_output_path: &quot;dask_output.ipynb&quot;  In this YAML file, we'll need to change the notebook_path to the actual path in your repository. This YAML is all we need to deploy the pipeline. Now let's commit the notebook and the YAML file to a GIT repository and run the job. Shakudo maintains live syncs of the repositories that are connect and make the code available for deployment immediately after code is synced. You can check the status of the git sync at Admin Settings.  ","version":"Next","tagName":"h2"},{"title":"7. Deploy to a pipeline job​","type":1,"pageTitle":"Quick start","url":"/Getting started/getstarted#7-deploy-to-a-pipeline-job","content":"Now we are one step away to put the job in production! To launch a pipeline job, we can go to the Shakudo Platform dashboard's Jobs tab and click Create.   In the job dialogue, we need to fill the following: Name: Job name or use the randomized name Image: Choose the image that we developed the code on to maintain environment consistency YAML path: Paste the path of the YAML file that we created above, from the root of the repository  Click the Create Immediate Job button on the top right corner to create the job. ","version":"Next","tagName":"h2"},{"title":"8. Check job status​","type":1,"pageTitle":"Quick start","url":"/Getting started/getstarted#8-check-job-status","content":"Once the job is submitted, you are redirected to the immediate jobs dashboard where our job is at the top of the job list. To see the live log of the job, click on the file button. You can pin the job to the top with the pin button, or checkout more functions in side the three dots.  Congratulations on developed and deployed your first Shakudo job pipeline with distributed computing! ","version":"Next","tagName":"h2"},{"title":"9. Additional Steps​","type":1,"pageTitle":"Quick start","url":"/Getting started/getstarted#9-additional-steps","content":"Shakudo Platform offers a variety of other functionalities for more advanced workflows. Some additional uses include the following: Run pipelines on a scheduleTriggering pipeline jobs programmatically ","version":"Next","tagName":"h2"},{"title":"Managing Git Repositories","type":0,"sectionRef":"#","url":"/shakudo-platform-core/addingGit","content":"","keywords":"","version":"Next"},{"title":"Adding a Git Repository​","type":1,"pageTitle":"Managing Git Repositories","url":"/shakudo-platform-core/addingGit#adding-a-git-repository","content":"If the default git repository does not suit your need, first head to the Git Repositories tab (as depicted below)  Then click the &quot;link git repository&quot; button and fill in the linking form.  The name is an arbitrary string to display the entry in the list of git repositories listed on the page. The git repository ssh url is the url of the git repository to link to. HTTPS is not supported, only ssh. The link format looks like user@server.com:repo.git. For example, Shakudo's public examples repository's ssh url is git@github.com:devsentient/examples.git. Finally, the default branch is the branch to use when none is specified in a job or service. This can be the repository's actual default branch (for example, main), or any other branch to be considered the default on Shakudo. The git repository that was added will be displayed in the table on the Git Repositories page.  note The name provided in the form to add a new repository appears next to the branch associated with the repository that was just added, not with the repository itself. To see the branches associated with a repository, simply click the repository of interest to have it expand and show all branches. You may notice that the status for the branch we added says &quot;remote not found&quot;. As suggested in the tooltip when hovering over the status icon, the problem is that we haven't yet added shakudo's key to our repository. To do this, copy the ssh key by selecting the key icon from the actions menu  And add it as a deploy key on your platform. Exact procedure depends on your git infrastructure. ","version":"Next","tagName":"h2"},{"title":"Unlinking a Repository​","type":1,"pageTitle":"Managing Git Repositories","url":"/shakudo-platform-core/addingGit#unlinking-a-repository","content":"To unlink a git repository, simply click the &quot;x&quot; button in the actions menu next to the branch from a repository that you wish to unlink. ","version":"Next","tagName":"h2"},{"title":"Adding a Branch to an Existing Repository​","type":1,"pageTitle":"Managing Git Repositories","url":"/shakudo-platform-core/addingGit#adding-a-branch-to-an-existing-repository","content":"To add a branch to a repository that was previously added, click the &quot;add branch&quot; button (next to the key icon) in the actions area  Next, input the name of the branch you would like to add and click &quot;connect branch&quot;. ","version":"Next","tagName":"h2"},{"title":"Traffic Control","type":0,"sectionRef":"#","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess","content":"","keywords":"","version":"Next"},{"title":"Table of Contents​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#table-of-contents","content":"IntroductionAccess Control Options Full AccessNo AccessPartial AccessAir Gap Mode Managing Access via UI Granting Full AccessBlocking Access CompletelyGranting Access to Specific Hosts Air Gap ModeBackend AutomationExample ScenariosConclusion ","version":"Next","tagName":"h2"},{"title":"1. Introduction​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#1-introduction","content":"Istio provides a robust solution for managing egress traffic control in a Kubernetes environment. This guide explains how to use our UI to manage namespace access control, including cluster air-gap mode, full access, no access, and host-specific access ","version":"Next","tagName":"h2"},{"title":"2. Access Control Options​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#2-access-control-options","content":"","version":"Next","tagName":"h2"},{"title":"Air Gap Mode​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#air-gap-mode","content":"Blocks accessing all external services on all namespaces ","version":"Next","tagName":"h3"},{"title":"Full Access​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#full-access","content":"Granting full access allows a namespace to communicate freely with external services without any restrictions. ","version":"Next","tagName":"h3"},{"title":"No Access​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#no-access","content":"Blocking access completely prevents a namespace from communicating with any external services. ","version":"Next","tagName":"h3"},{"title":"Partial Access​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#partial-access","content":"Granting access to specific hosts allows a namespace to communicate only with specified external services. ","version":"Next","tagName":"h3"},{"title":"3. Managing Access via UI​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#3-managing-access-via-ui","content":"","version":"Next","tagName":"h2"},{"title":"Granting Full Access​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#granting-full-access","content":"Open the UI and navigate to Admin &gt; Outbound Traffic AccessSelect the settings of the namespace you want to configure.Choose the &quot;Full Access&quot; option.Click &quot;Save&quot; to save the changes. ","version":"Next","tagName":"h3"},{"title":"Blocking Access Completely​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#blocking-access-completely","content":"Open the UI and navigate to Admin &gt; Outbound Traffic AccessSelect the settings of the namespace you want to configure.Choose the &quot;No Access&quot; option.Click &quot;Save&quot; to save the changes. ","version":"Next","tagName":"h3"},{"title":"Granting Access to Specific Hosts​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#granting-access-to-specific-hosts","content":"Open the UI and navigate to Admin &gt; Outbound Traffic AccessSelect the settings of the namespace you want to configure.Choose the &quot;Partial Access&quot; option.Enter a comma-separated list of valid hosts (.ca, .com, example.com, my.example.com, *.my.example.com - Wildcard only is not allowed).Click &quot;Save&quot; to save the changes. ","version":"Next","tagName":"h3"},{"title":"5. Air Gap Mode​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#5-air-gap-mode","content":"Air Gap mode blocks access to all egress traffic on all namespaces. This mode is useful for environments that require complete isolation from external networks. To enable Air Gap mode: Open the UI and navigate to the Outbound Traffic Access mode section.Enable Air Gap mode.Confirm your change and wait for it to be applied ","version":"Next","tagName":"h2"},{"title":"6. Backend Automation​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#6-backend-automation","content":"Our backend handles the creation of service entries and sidecars based on the UI selections. When a user selects an access control option, the backend will automatically create the necessary Istio configurations to enforce the desired access control. ","version":"Next","tagName":"h2"},{"title":"7. Example Scenarios​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#7-example-scenarios","content":"","version":"Next","tagName":"h2"},{"title":"Scenario 1: Granting Full Access​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#scenario-1-granting-full-access","content":"Namespace: namespace1Action: Grant full accessResult: namespace1 can communicate with all external services. ","version":"Next","tagName":"h3"},{"title":"Scenario 2: Blocking Access Completely​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#scenario-2-blocking-access-completely","content":"Namespace: namespace2Action: Block access completelyResult: namespace2 cannot communicate with any external services. ","version":"Next","tagName":"h3"},{"title":"Scenario 3: Granting Access to Specific Hosts​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#scenario-3-granting-access-to-specific-hosts","content":"Namespace: namespace3Action: Grant access to specific hosts (e.g., example.com, api.example.com)Result: namespace3 can only communicate with example.com and api.example.com. ","version":"Next","tagName":"h3"},{"title":"8. Conclusion​","type":1,"pageTitle":"Traffic Control","url":"/shakudo-platform-core/adminSettings/outboundtrafficaccess#8-conclusion","content":"This guide provides detailed instructions for managing namespace access control to egress traffic on Istio using our UI. By following these steps, you can effectively control egress traffic in your cluster. ","version":"Next","tagName":"h2"},{"title":"Environment Configs","type":0,"sectionRef":"#","url":"/shakudo-platform-core/adminSettings/podspecs","content":"","keywords":"","version":"Next"},{"title":"Environment Config details​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#environment-config-details","content":"To check the configurations for each EC, navigate to the Environment Config tab and click on the card you'd like to see the details for.  ","version":"Next","tagName":"h2"},{"title":"Check installed packages​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#check-installed-packages","content":"To check what packages are installed in an EC, you can do so from Sessions. Create a new session and choose the session type for which you'd like to check the packages for. Open up the browser Session and open a Jupyter notebook. Type in pip freeze to see pip packages installed and conda list for conda packages installed.  ","version":"Next","tagName":"h3"},{"title":"Create an EC​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#create-an-ec","content":"","version":"Next","tagName":"h2"},{"title":"Prerequisities​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#prerequisities","content":"These are a few requirements when creating an EC: An image that is accessible to ShakudoVolumes and volume mounts to existAppropriate node pools or resources ","version":"Next","tagName":"h3"},{"title":"Steps to create an EC​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#steps-to-create-an-ec","content":"You can create a new EC to use in your Sessions, Jobs, or Services. Follow the steps below to do so: Navigate to the Environment Config tab and click on the + button to create a new EC Customize the details. For details on each field, see the section below for podspec configurations Click CREATE To view and use the new EC you must press ⌘ + R to hard refresh the page.  ","version":"Next","tagName":"h3"},{"title":"EC configurations​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#ec-configurations","content":"","version":"Next","tagName":"h2"},{"title":"Name​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#name","content":"This is the name that will show up when choosing which EC to use in Sessions, Jobs, and Services. ","version":"Next","tagName":"h3"},{"title":"EC name​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#ec-name","content":"This name acts as a unique ID for the EC. ","version":"Next","tagName":"h3"},{"title":"Description​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#description","content":"Give the EC a one liner description for when someone should pick it. This description appears in the EC page as well as in the Session Type selection.  ","version":"Next","tagName":"h3"},{"title":"Image URL​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#image-url","content":"You can choose to bring your own docker image to build your custom EC with. Paste in the link to the image in this field. ","version":"Next","tagName":"h3"},{"title":"Resources​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#resources","content":"You can specify the CPU request &amp; limit as well as the memory request &amp; limit. If you want GPUs make sure to specify GPUs that are available to your cluster. If you're unsure please contact us and we can provide you the details. ","version":"Next","tagName":"h3"},{"title":"Volumes & volume mounts​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#volumes--volume-mounts","content":"Specify your Volumes and Volume Mounts to attach by clicking on the + button. For more information see the Kubernetes guide on Volumes. ","version":"Next","tagName":"h3"},{"title":"Environment variables​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#environment-variables","content":"Use this field to specify the name and value for environment variables.  ","version":"Next","tagName":"h3"},{"title":"Edit an EC​","type":1,"pageTitle":"Environment Configs","url":"/shakudo-platform-core/adminSettings/podspecs#edit-an-ec","content":"You can edit details of an EC by clicking on the pencil icon from the EC tab. By default you can only edit the custom created ECs (not the default ECs created by Shakudo) When an EC is edited and saved, newly created jobs and sessions will use the updated version. Any jobs or sessions currently in progress will use the old version. Any services in progress using the edited EC will be restarted to update. ","version":"Next","tagName":"h2"},{"title":"Shakudo Cloud Shell","type":0,"sectionRef":"#","url":"/shakudo-platform-core/cloudshell","content":"","keywords":"","version":"Next"},{"title":"Installing Stack Components using their Shakudo Helm Charts​","type":1,"pageTitle":"Shakudo Cloud Shell","url":"/shakudo-platform-core/cloudshell#installing-stack-components-using-their-shakudo-helm-charts","content":"","version":"Next","tagName":"h2"},{"title":"What are Helm Charts?​","type":1,"pageTitle":"Shakudo Cloud Shell","url":"/shakudo-platform-core/cloudshell#what-are-helm-charts","content":"Helm is a package manager for Kubernetes that simplifies the deployment and management of applications. A Helm chart is a collection of YAML templates and configurations that describe a set of Kubernetes resources required to deploy an application. ","version":"Next","tagName":"h3"},{"title":"Tools and Environment​","type":1,"pageTitle":"Shakudo Cloud Shell","url":"/shakudo-platform-core/cloudshell#tools-and-environment","content":"You will be using the Shakudo Cloud Shell, which comes pre-installed with Helm, Kubernetes CLI (kubectl), and other essential tools. Since permissions are already configured, you can directly install charts from the Shakudo registry. ","version":"Next","tagName":"h3"},{"title":"Step-by-Step Guide of installing a Stack Component​","type":1,"pageTitle":"Shakudo Cloud Shell","url":"/shakudo-platform-core/cloudshell#step-by-step-guide-of-installing-a-stack-component","content":"1. Dry Run the Installation (Helm --dry-run=server)​ Before performing the actual installation, it’s a good practice to execute a dry run to ensure that the command will succeed without errors: helm upgrade --install n8n oci://us-docker.pkg.dev/shakudo-417723/charts/n8n --namespace hyperplane-n8n-test --version 1.2.4 --create-namespace --dry-run=server --debug  This will simulate the installation and output what changes will be made, helping you verify that the installation parameters are correct.  2. Installing the n8n Stack Component (Example)​ Let’s assume you want to install the n8n stack component, version 1.2.4, in the hyperplane-n8n-test namespace. Run the following command to install or upgrade the n8n chart: helm upgrade --install n8n oci://us-docker.pkg.dev/shakudo-417723/charts/n8n --namespace hyperplane-n8n-test --version 1.2.4 --create-namespace   Explanation: upgrade --install: Ensures that if the release does not exist, it will be installed; otherwise, it will be upgraded.n8n: This is the release name you assign. You can choose any meaningful name.oci://us-docker.pkg.dev/shakudo-417723/charts/n8n: Refers to the n8n Helm chart stored in the Shakudo registry.--namespace hyperplane-n8n-test: Specifies the Kubernetes namespace where the component will be installed.--version 1.2.4: Ensures that version 1.2.4 of the n8n chart is installed.--create-namespace: Creates the namespace if it does not already exist. 3. Verifying Installation​ After the installation completes, verify the deployed resources: List Helm releases: helm list --namespace hyperplane-n8n-test   Check Kubernetes resources: kubectl get all --namespace hyperplane-n8n-test  This will display all the resources associated with the n8n stack component, such as pods, services, and deployments.  4. Customizing Installation with valuesOverride.yaml​ If you need to customize the deployment, you can use a valuesOverride.yaml file to override default chart values. Here’s an example valuesOverride.yaml file for n8n: # valuesOverride.yaml - you may want to name this different as there may be multiple values file for different stack components resources: limits: cpu: &quot;5000m&quot; memory: &quot;12Gi&quot;  To install the chart with custom values, use: helm upgrade --install n8n oci://us-docker.pkg.dev/shakudo-417723/charts/n8n --namespace hyperplane-n8n-test -f valuesOverride.yaml  ","version":"Next","tagName":"h3"},{"title":"Important Notes​","type":1,"pageTitle":"Shakudo Cloud Shell","url":"/shakudo-platform-core/cloudshell#important-notes","content":"Permissions: Shakudo Cloud Shell is pre-configured with the necessary permissions, so you do not need to set up additional roles or service accounts.Namespace Management: Always specify the correct namespace during installation to avoid conflicts.Version Control: It’s important to specify the exact version of the Helm chart you want to deploy to ensure consistency across environments.Dry Run Before Install: Using --dry-run before actual installation helps catch potential issues in advance and ensures a smooth deployment. By following these steps, even users new to Kubernetes and Helm can successfully deploy and manage Stack Components using the Shakudo Cloud Shell. ","version":"Next","tagName":"h3"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/shakudo-platform-core/faq","content":"","keywords":"","version":"Next"},{"title":"Out of memory​","type":1,"pageTitle":"FAQ","url":"/shakudo-platform-core/faq#out-of-memory","content":"Out of memory errors may occur if one of your chunks/partitions or persisted data is too large to fit into RAM. OOM errors can also occur if one of your operations requires more RAM than what is available. Increasing the available memory on your workers can usually solve this issue. OOMs show up as any of the following: killed worker, key error, canceled, http error. ","version":"Next","tagName":"h2"},{"title":"Module not found​","type":1,"pageTitle":"FAQ","url":"/shakudo-platform-core/faq#module-not-found","content":"When add code to your pipeline, some references to .py modules may not be found. Ensure you are importing from the correct directory (relative to the top level of your repository). ","version":"Next","tagName":"h2"},{"title":"Timeouts when spinning up Dask workers​","type":1,"pageTitle":"FAQ","url":"/shakudo-platform-core/faq#timeouts-when-spinning-up-dask-workers","content":"You may receive an error if the Dask nodes are taking too long to scale up (due to resource availability, resource limits for your project, etc.). You can retry by re-running your cell or script when you see this error. ","version":"Next","tagName":"h2"},{"title":"Timeout when using Sessions​","type":1,"pageTitle":"FAQ","url":"/shakudo-platform-core/faq#timeout-when-using-sessions","content":"In your browser Session, if you get a popup that says Directory not found it means your Session has timed out. ","version":"Next","tagName":"h2"},{"title":"Preempted nodes​","type":1,"pageTitle":"FAQ","url":"/shakudo-platform-core/faq#preempted-nodes","content":"If a node is preempted, you will see a canceled error. For this reason, we do not recommend long jobs, but rather split your long-running functions into multiple jobs to avoid http error or canceled error. ","version":"Next","tagName":"h2"},{"title":"Image Builder","type":0,"sectionRef":"#","url":"/shakudo-platform-core/image_builder","content":"","keywords":"","version":"Next"},{"title":"Steps:​","type":1,"pageTitle":"Image Builder","url":"/shakudo-platform-core/image_builder#steps","content":"Add a git server (shakudo-examples) where we want to fetch the Dockerfile. Go to Container Images tab. Select Create Container Image on the top right corner and fill in the details appropriately. See the example below:  The Dockerfile Path is the file path from the root of the repo. After the job is completed, you can Convert to EC and use it in Sessions, Jobs, or Microservices. You can also view the container image details in Harbor by selecting Open in Harbor to see the images built from Image Builder. ","version":"Next","tagName":"h3"},{"title":"Jobs - Deploy","type":0,"sectionRef":"#","url":"/shakudo-platform-core/jobs","content":"","keywords":"","version":"Next"},{"title":"Creating a Job​","type":1,"pageTitle":"Jobs - Deploy","url":"/shakudo-platform-core/jobs#creating-a-job","content":"Jobs can be created from the corresponding job menu (immediate or scheduled) depending on the desired job type. As the names imply, immediate jobs will run immediately while scheduled jobs will run on a configured schedule. Shakudo supports various job types, including ipython notebooks and python scripts. ","version":"Next","tagName":"h2"},{"title":"Job Specification (multi-step)​","type":1,"pageTitle":"Jobs - Deploy","url":"/shakudo-platform-core/jobs#job-specification-multi-step","content":"Jobs are specified in a pipeline yaml format. A basic pipeline yaml will resemble the below: pipeline: name: [name of your pipeline job] working_dir: (optional) directory where pipeline is ran from (default is the root of the repository) tasks: - name: “step1” type: type of file notebook_path: path to notebook notebook output path: output notebook location depends_on : run steps in parallel or specify dependencies working_dir: (optional) directory where task is ran from (default is the root of the repository)  Additional steps can be specified as per job requirements simply by adding more step entries within the tasks list. For the type of file, the following are accepted: VSCode notebook: &quot;vscode notebook&quot;Python scripts: &quot;python script&quot;, &quot;python&quot;, or &quot;py&quot;Javascript: &quot;js script&quot;, &quot;javascript&quot;, or &quot;js&quot;Jupyter notebooks: &quot;jupyter notebook&quot;, &quot;jupyter&quot;, &quot;notebook&quot;, or &quot;ipynb&quot; Jobs can be specified for sequential, parallel, or mixed execution. Steps will run sequentially by default. Specifying dependencies in the depends_on field of a step entry allows steps to dispatch in parallel, so long as their dependencies (i.e. jobs, identified by name, which have to have completed execution before this step executes) are fulfilled. An empty list ([]) as a value for depends_on will dispatch the step immediately (since it has no dependencies), in parallel with other jobs that are ready to run. The following example specifies a step to run in parallel:  - name: “parallelStep” type: type of file notebook_path: path to notebook notebook output path: output notebook location depends_on : []  For more examples of pipeline yaml files, see the Shakudo examples repository Below are example step specifications for job types supported by Shakudo. To add a jupyter notebook step:  - name: &quot;[your_step_name]&quot; type: &quot;jupyter notebook&quot; notebook_path: &quot;[notebook/path/relative/to/top/level/of/repo.ipynb]&quot; notebook_output_path: &quot;[some/notebook_output_name.ipynb]&quot;  To add a vscode notebook step:  - name: &quot;[another_step_name]&quot; type: &quot;vscode notebook&quot; py_path: &quot;[py/file/relative/to/top/level/of/repo.py]&quot;  To add a Node.js javascript step:  - name: &quot;[another_step_name]&quot; type: &quot;js script&quot; js_path: &quot;[js/file/relative/to/top/level/of/repo.js]&quot;  To add a bash script step:  - name: &quot;[another_step_name]&quot; type: &quot;bash script&quot; bash_script_path: &quot;[sh/file/relative/to/top/level/of/repo.sh]&quot;  ","version":"Next","tagName":"h3"},{"title":"Bash Script (single step job)​","type":1,"pageTitle":"Jobs - Deploy","url":"/shakudo-platform-core/jobs#bash-script-single-step-job","content":"If you only need to run a single step consisting of a bash script, select &quot;Shell&quot; instead of &quot;Multi-step&quot; in the Job creation dialogue.  As you would for multi-step pipeline YAMLs, specify the path to the bash script relative to the root of your git repo. An example bash script might look like the following: #!/bin/bash set -e PROJECT_DIR=&quot;$(cd -P &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot; cd &quot;$PROJECT_DIR&quot; pip install foo-package==bar-version python ./a-script.py  Note the shebang for bash. The -e flag detects errors in the currently running script, and stops the script when one of the commands within returns a non-zero status. PROJECT_DIR finds the current directory of the current bash script. It is useful when you want to reference things relative to the current bash script. ","version":"Next","tagName":"h3"},{"title":"Immediate Jobs​","type":1,"pageTitle":"Jobs - Deploy","url":"/shakudo-platform-core/jobs#immediate-jobs","content":"Immediate jobs can be created by clicking &quot;start a job&quot; on the Shakudo landing page.  Creating a job involves, at minimum, filling in the job name (automatically generated by default), environment configuration (which works the same way as for Shakudo sessions or services), and specifying the job yaml specification as described above. When using a non-default environment configuration, advanced settings like source git repository, job timeout, and job retry count can be modified to suit job requirements. Immediate Jobs can be rerun as scheduled jobs from the immediate jobs page by using the actions menu (see Scheduled Jobs below), which will clone the job specification and change it into a Scheduled Job.  ","version":"Next","tagName":"h3"},{"title":"Scheduled Jobs​","type":1,"pageTitle":"Jobs - Deploy","url":"/shakudo-platform-core/jobs#scheduled-jobs","content":"Scheduled jobs can be created from the Scheduled Jobs page by first selecting the appropriate menu item  And clicking on the &quot;create scheduled job&quot; button on the Scheduled Jobs page.  Scheduled jobs work like Immediate Jobs, except that a schedule in crontab format must be specified in addition to other basic fields.  ","version":"Next","tagName":"h3"},{"title":"Parameterizing Notebooks​","type":1,"pageTitle":"Jobs - Deploy","url":"/shakudo-platform-core/jobs#parameterizing-notebooks","content":"Jupyter notebooks run as jobs can be parameterized. Parameters can then be specified in the job creation menu in the Parameters tab. To prepare a notebook to use parameters in jupyterlab, click on the cog icon on the top right corner of the cell to parameterize and add a tag called parameters. In vscode, this is the &quot;mark cell as parameters&quot; option in the breadcrumb menu for the cell of interest.  The cell's contents will be overwritten by the parameter's value as specified in the Parameters tab, if applicable. ","version":"Next","tagName":"h2"},{"title":"Public Logo CDN Setup","type":0,"sectionRef":"#","url":"/shakudo-platform-core/public-logo-cdn-setup","content":"","keywords":"","version":"Next"},{"title":"Why This Matters​","type":1,"pageTitle":"Public Logo CDN Setup","url":"/shakudo-platform-core/public-logo-cdn-setup#why-this-matters","content":"Stack Component Icon Issue: The Stack Component page currently cannot display icons for newly uploaded SVG or PNG files.Ephemeral URLs: GitHub no longer generates direct public URLs for images uploaded in comments or issues. Now, it generates links like https://private-user-images.githubusercontent.com/..., which are restricted and may expire.Inconsistent Access: These URLs are not reliable for documentation or production use since they can change or stop working over time. To ensure consistency and reliability, we host public logos directly in our repository and access them via GitHub's raw CDN. ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Public Logo CDN Setup","url":"/shakudo-platform-core/public-logo-cdn-setup#prerequisites","content":"Write access to the CDN repository (no fork needed).Git installed and configured with your GitHub credentials. ","version":"Next","tagName":"h2"},{"title":"Icon Requirements​","type":1,"pageTitle":"Public Logo CDN Setup","url":"/shakudo-platform-core/public-logo-cdn-setup#icon-requirements","content":"File Format: Prefer SVG for sharp, scalable logos. PNG files also work.File Naming: Use lowercase letters with hyphens (e.g., my-company-logo.svg) for consistency.Updates: When updating the logo, the URL remains unchanged. Simply open a new PR to overwrite the existing file. ","version":"Next","tagName":"h2"},{"title":"Uploading a Public Logo​","type":1,"pageTitle":"Public Logo CDN Setup","url":"/shakudo-platform-core/public-logo-cdn-setup#uploading-a-public-logo","content":"Follow the steps below to upload a logo and use it as a CDN asset. Clone the repository and create a feature branch git clone https://github.com/devsentient/cdn.git cd cdn git checkout -b add-my-logo Copy your logo into the repository cp ~/Downloads/my-logo.svg stack-component-logos/ Commit and push your changes git add stack-component-logos/my-logo.svg git commit -m &quot;Add my logo to stack-component-logos&quot; git push -u origin add-my-logo Open a pull request Go to https://github.com/devsentient/cdn/pulls and open a new PR from add-my-logo into main. Provide a clear title and description so reviewers understand the change. Merge and Publish Once your PR is merged, your logo will be publicly accessible at: https://github.com/devsentient/cdn/blob/main/stack-component-logos/my-logo.svg Get the Raw URL When you view the file on GitHub, you'll see a Raw button near the top right of the file preview. Clicking it will load the file’s contents directly (with the browser URL updated): https://raw.githubusercontent.com/devsentient/cdn/main/stack-component-logos/my-logo.svg This is the URL you should use for embedding the image as a CDN asset.  ","version":"Next","tagName":"h2"},{"title":"Tips​","type":1,"pageTitle":"Public Logo CDN Setup","url":"/shakudo-platform-core/public-logo-cdn-setup#tips","content":"Prefer SVG for crisp, infinitely‑scalable logos. PNG also works.If you ever update the file, the URL stays the same—just overwrite the asset in a new PR.Keep filenames lowercase and hyphen‑separated for consistency (e.g., my-company-logo.svg). ","version":"Next","tagName":"h3"},{"title":"Shakudo CLI (sctl)","type":0,"sectionRef":"#","url":"/shakudo-platform-core/sctl","content":"","keywords":"","version":"Next"},{"title":"Features​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sctl#features","content":"List workloadsView workloads logsList microservicesCreate microservicesRestart microservicesView microservices logs ","version":"Next","tagName":"h2"},{"title":"Manage Shakudo Objects​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sctl#manage-shakudo-objects","content":"","version":"Next","tagName":"h2"},{"title":"Immediate Jobs​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sctl#immediate-jobs","content":"Manage immediate jobs by listing or viewing their logs. Commands: sctl immediate-jobs ls --limit [number of items] --offset [offset of items]: List all immediate jobs.sctl immediate-jobs logs --id [job-id] --lines [number of lines - defaults to 100]: Tail logs of an existing immediate job by its ID. Examples: # List all immediate jobs $ sctl immediate-jobs ls # Tail logs for a specific immediate job $ sctl immediate-jobs logs --id 12345 # Tail logs for a specific immediate job and set the line limit $ sctl immediate-jobs logs --id 12345 --lines 1000 # Watch logs for a microservice every 2 seconds watch sctl immediate-jobs logs --id 12345  ","version":"Next","tagName":"h3"},{"title":"Scheduled Jobs​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sctl#scheduled-jobs","content":"View and manage scheduled jobs. Commands: sctl scheduled-jobs ls --limit [number of items] --offset [offset of items]: List all scheduled jobs. Examples: # List all scheduled jobs $ sctl scheduled-jobs ls  ","version":"Next","tagName":"h3"},{"title":"Microservices​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sctl#microservices","content":"Manage microservices by listing, creating, restarting, and viewing logs. Commands: sctl microservices ls --limit [number of items] --offset [offset of items]: List all microservices.sctl microservices create [options]: Create a new microservice.sctl microservices restart --id [service-id]: Restart an existing microservice by ID.sctl microservices logs --id [service-id] --lines [number of lines - defaults to 100]: Tail logs of a specific microservice by ID. Examples: Basic creation of a microservice: $ sctl microservices create --gitServerName shakudo-dev-repo --ai Creating a custom microservice with job name, subdomain, and script path: $ sctl microservices create --scriptPath fastapi-openapi-example/run.sh \\ --pipelineType BASH --jobName my-custom-ms --subdomain custom-subdomain Creating a microservice with notifications enabled: $ sctl microservices create --gitServerName shakudo-dev-repo \\ --notificationsEnabled \\ --notificationTargetIds 8ebc86f3-c76e-4ce7-99f9-666151923b0c,14960534-15dd-4ffd-8d46-2e00b2f55f36 \\ --healthCheckThreshold 5 --serviceAlertCooldownPeriod 7000 \\ --scriptPath fastapi-openapi-example/run.sh --pipelineType BASH Creating a microservice with autoscaling: $ sctl microservices create --gitServerName shakudo-dev-repo \\ --minReplicas 2 --maxHpaRange 5 Combining multiple flags for advanced microservice creation: $ sctl microservices create \\ --gitServerName shakudo-dev-repo \\ --jobName test-service-example-cli \\ --jobType basic-system \\ --scriptPath fastapi-openapi-example/run.sh \\ --pipelineType BASH \\ --subdomain test-subdomain \\ --notificationsEnabled \\ --notificationTargetIds 8ebc86f3-c76e-4ce7-99f9-666151923b0c,14960534-15dd-4ffd-8d46-2e00b2f55f36 \\ --healthCheckThreshold 2 --serviceAlertCooldownPeriod 2000 \\ --minReplicas 1 --maxHpaRange 3 \\ --port 8888 \\ --billingProjectName prod-bp \\ --serviceAccountName admin-sa \\ --parameter ENV1=VAL1 --parameter ENV2=VAL2 \\ --customSecretNames testing,pass-openai \\ --readme &quot;&gt; this is a testing readme\\n\\n### this is a header\\n&quot; Watch logs of a microservice # Watch logs for a microservice every 2 seconds watch sctl microservices logs --id 12345  ","version":"Next","tagName":"h3"},{"title":"General Usage​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sctl#general-usage","content":"$ sctl [COMMAND]  Use the help command for detailed information about a specific topic or command. Examples: # Display help for the microservices topic $ sctl microservices --help # Display help for the immediate-jobs topic $ sctl immediate-jobs --help  ","version":"Next","tagName":"h2"},{"title":"Version​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sctl#version","content":"The current version of the Shakudo CLI is 0.0.1. shakudo-cli/0.0.1 darwin-arm64 node-v18.18.0  ","version":"Next","tagName":"h2"},{"title":"Additional Notes​","type":1,"pageTitle":"Shakudo CLI (sctl)","url":"/shakudo-platform-core/sctl#additional-notes","content":"Ensure that the appropriate flags and parameters are provided when creating or managing microservices.Invalid inputs, such as incorrect pipelineType, will throw an error. This documentation provides an overview of the Shakudo CLI’s primary functionality, command usage, and examples to help users manage Shakudo deployments effectively. ","version":"Next","tagName":"h2"},{"title":"Secrets","type":0,"sectionRef":"#","url":"/shakudo-platform-core/secrets","content":"","keywords":"","version":"Next"},{"title":"How to create a secret?​","type":1,"pageTitle":"Secrets","url":"/shakudo-platform-core/secrets#how-to-create-a-secret","content":"Go to the secrets tab on the Shakudo dashboard and click on &quot;Create Secret&quot; on the top right corner. Add the name and description of the secret. Note: The name should not contain blank space or underscore, since it's a mount directory name when you attach a secret to jobs/sessions. Adding a purpose: A secret created by a user is often used in the microservice or within sessions. So, adding a purpose allows users to scope the usage of the secret to a job or session or both. Development refers to sessions which creates a secret in the hyperplane-jhub namespace and can only be used in sessions.Workloads refers to jobs/microservice which creates a secret in the hyperplane-pipelines namespace and can only be used in microservices.Workloads &amp; Development helps create a secret in both of the above namespaces and can be used in all of Sessions, Jobs and Microservices. ","version":"Next","tagName":"h3"},{"title":"How to access a secret in your code?​","type":1,"pageTitle":"Secrets","url":"/shakudo-platform-core/secrets#how-to-access-a-secret-in-your-code","content":"You can access the secret attached to your code via the mount directory. For example, if you create a secret minio-creds with the key as username and values as HelloWorld then it will be mounted at /etc/hyperplane/secrets/minio-creds/username file. Alternatively, you can access secrets with environment variables.HYPERPLANE_CUSTOM_SECRET_KEY_USERNAME The key which you define will be added as an environment variable with the prefix HYPERPLANECUSTOM_SECRET_KEY. ","version":"Next","tagName":"h3"},{"title":"pyshakudo to manage secrets.​","type":1,"pageTitle":"Secrets","url":"/shakudo-platform-core/secrets#pyshakudo-to-manage-secrets","content":"Shakudo offers a Python client pyshakudo to manage Shakudo resources. Note: This is still under development and currently capable of managing secrets. pip install --index-url http://pypiserver-pypiserver.hyperplane-pypiserver.svc.cluster.local:8080/simple/ --trusted-host pypiserver-pypiserver.hyperplane-pypiserver.svc.cluster.local pyshakudo==0.1.0 Contact the Shakudo team to get access. In your Python code, use this package to dynamically operate on the secrets. # Example code to use pyshakudo to operate on Shakudo secrets in your sessions or microservice. from pyshakudo.secrets import ShakudoSecretsManager def main(): # Initialize the ShakudoSecretsManager with in-cluster configuration manager = ShakudoSecretsManager() # Get a specific secret print(&quot;Getting specific secret...&quot;) secret_name = &quot;test-secret&quot; secret = manager.get_secret(secret_name) print(&quot;Retrieved Secret Data:&quot;, secret.data) if __name__ == &quot;__main__&quot;: main()  ","version":"Next","tagName":"h3"},{"title":"tsshakudo to manage secrets in your typescript code.​","type":1,"pageTitle":"Secrets","url":"/shakudo-platform-core/secrets#tsshakudo-to-manage-secrets-in-your-typescript-code","content":"tsshakudo is a npm package developed by shakudo team to manage shakudo secrets in your node code. Note: This is still under development and currently capable to managing secrets. npm install npm install tsshakudo@0.1.0 import { ShakudoSecretsManager } from 'tsshakudo'; const main = async () =&gt; { // Initialize the secrets manager with the default namespace and in-cluster config const secretsManager = new ShakudoSecretsManager('hyperplane-jhub'); // Get the created secret const fetchedSecret = await secretsManager.getSecret('test-secret'); console.log('Fetched Secret:', fetchedSecret); main().catch((error) =&gt; { console.error('Error:', error); });  ","version":"Next","tagName":"h3"},{"title":"Microservices - Serving","type":0,"sectionRef":"#","url":"/shakudo-platform-core/service","content":"","keywords":"","version":"Next"},{"title":"Creating a Microservice​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#creating-a-microservice","content":"Services can be created from the Shakudo landing page by clicking the &quot;start a service&quot; button.  Alternatively, services can be created from the service page by clicking &quot;create service&quot;.  ","version":"Next","tagName":"h2"},{"title":"Basic settings​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#basic-settings","content":"A basic service can be started by simply providing the service name (automatically generated by default), the desired service endpoint, an environment configuration (which work the same way as in Shakudo Sessions), and a path to the service configuration yaml file, relative to the git repository associated with the service (configurable in the advanced tab) ","version":"Next","tagName":"h3"},{"title":"Advanced settings​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#advanced-settings","content":"The advanced tab allows configuring some more advanced service parameters, described in the table below: Name\tDescriptionPort\tEnter a port to expose between 1 and 65535. The default for Shakudo Platform services is port 8787. Min Replicas\tMinimum amount of replicas running the service simultaneously Max Replicas\tMaximum amount of replicas running the service simultaneously Git repository\tThe git repository associated with the service Branch\tGit branch name for the service Commit\tGit commit ID hash to use for the service The git repository, branch and commit will be used to clone a project into the service environment, and the serivce yaml will be located and run from the root of the cloned project. Additional arbitrary parameters for the service runner configuration can be set in the Parameters tab. ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#parameters","content":"Similarly to Jobs, you can add Parameters to inject some additional information, which you can use in your code. When you attach Parameters, they will be available as environment variables, in upper case, with all non-alphanumeric characters replaced by _. For example, if you add an parameter with name &quot;model-name&quot;, it will be accessible as the MODEL_NAME in the Service's environment variables. To maintain backward compatibility with legacy services, Parameters are also available with the HYPERPLANE_JOB_PARAMETER_. Using the example above, this would be HYPERPLANE_JOB_PARAMETER_MODEL_NAME. Note that these values are stored in plain text, it's recommended that you use Secrets for API keys, keyfile strings, access credentials, and other secret values. ","version":"Next","tagName":"h3"},{"title":"Secrets​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#secrets","content":"When you attach Secrets, they will be available both as environment variables and as a file. In the case of environment variable, the secret key will be converted to upper case, with all non-alphanumeric characters replaced by _, and prefixed with HYPERPLANE_CUSTOM_SECRET_KEY_. For example, if you add an parameter with name &quot;openai-key&quot;, it will be accessible as the HYPERPLANE_CUSTOM_SECRET_KEY_OPENAI_KEY in the Service's environment variables. Secrets are available as files, in the format of /etc/hyperplane/secrets/{secret_name}/{secret_key}. ","version":"Next","tagName":"h3"},{"title":"Starting with a bash script​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#starting-with-a-bash-script","content":"If you only need to run a single step consisting of a bash script to start your app, select &quot;Shell&quot; instead of &quot;Multi-step&quot; in the Job creation dialogue.  As you would for multi-step pipeline YAMLs, specify the path to the bash script relative to the root of your git repo. An example bash script might look like the following: #!/bin/bash set -e PROJECT_DIR=&quot;$(cd -P &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot; cd &quot;$PROJECT_DIR&quot; pip install foo-package==bar-version python ./app.py  Note the shebang for bash. The -e flag detects errors in the currently running script, and stops the script when one of the commands within returns a non-zero status. PROJECT_DIR finds the current directory of the current bash script. It is useful when you want to reference things relative to the current bash script. This bash script will start your microservice (e.g. a Flask app) and run until app.py exits with an error. In the case of an error, the pod will be restarted and the script will rerun from the top. ","version":"Next","tagName":"h3"},{"title":"Service actions​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#service-actions","content":"The service actions menu can be used to operate on a current or past service.  ","version":"Next","tagName":"h2"},{"title":"Cancel​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#cancel","content":"A current service can be cancelled by choosing the &quot;cancel&quot; action in the action menu. ","version":"Next","tagName":"h3"},{"title":"Clone​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#clone","content":"A new service can be created based on the configuration of an existing service by choosing the &quot;clone&quot; action. This can also be used on past services that have been cancelled to recreate a service of the same type, or to use the service settings as a template for faster iteration. ","version":"Next","tagName":"h3"},{"title":"Restart​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#restart","content":"Restarting a service (which will cause it to pull from the associated repository as per its settings) can be done by selecting the &quot;restart&quot; action. ","version":"Next","tagName":"h3"},{"title":"Updating a service​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#updating-a-service","content":"Shakudo does not yet support a single-action update operation on services. Instead, when a service configuration is modified, services are updated by first closing a service and then starting it anew with an updated configuration. For updates to the service code (from the repository associated with the repository), the service should be restarted with the restart action. ","version":"Next","tagName":"h2"},{"title":"Accessing a service from outside the cluster​","type":1,"pageTitle":"Microservices - Serving","url":"/shakudo-platform-core/service#accessing-a-service-from-outside-the-cluster","content":"Services can be accessed externally using a jwt token using the bearer method. Prepare to make a POST request to https://{your_shakudo_domain}/auth/realms/{realm_name}/protocol/openid-connect/token, for example using curl or postman {your_shakudo_domain} is the domain at which your keycloak is available, which is usually the same domain at which you access your cluster running ShakudoYou must know your {realm_name} to obtain the jwt token. Set the Content-Type header to application/x-www-form-urlencodedSet the following parameters in the request body: client_id: the client used to get the token (probably istio)grant_type: value should be passwordusername: Your usernamepassword: Your password note The Access Type must be public to obtain a jwt this way The response jwt can then be used to access service endpoints: simply add Authorization: Bearer {token} to your Headers, with {token} being the jwt obtained in the previous step. ","version":"Next","tagName":"h2"},{"title":"Sessions","type":0,"sectionRef":"#","url":"/shakudo-platform-core/sessions/","content":"","keywords":"","version":"Next"},{"title":"Get started with Sessions​","type":1,"pageTitle":"Sessions","url":"/shakudo-platform-core/sessions/#get-started-with-sessions","content":"From the homepage, click the &quot;start a session&quot; button note Alternatively, click the &quot;start a session&quot; button on the Sessions screen Choose which type of session you’d like to use as your development environment note Optionally, setup any other details, like which drive or docker image to use Click &quot;start session&quot; at the bottom of the page note The session spin-up may take a few seconds to a few minutes depending on your session type. Once your session is ready, click an access button to connect to the session. Pictured from left to right: in-browser jupyterlab, in-browser vscode, ssh connection string. note while a session may appear as &quot;running&quot; or &quot;active&quot;, until the access buttons beyond ssh appear (e.g. the jupyterlab and vscode access buttons), it may not yet have initialized all its components. If a connection cannot be established, it may simply be because the session is completing initialization. That is most likely the case if you do not see an error in the session logs.  ","version":"Next","tagName":"h2"},{"title":"Configurations​","type":1,"pageTitle":"Sessions","url":"/shakudo-platform-core/sessions/#configurations","content":"","version":"Next","tagName":"h2"},{"title":"Session Type​","type":1,"pageTitle":"Sessions","url":"/shakudo-platform-core/sessions/#session-type","content":" The session type that you choose in the session creation popup will prepare your Session with a pre-configured development environment. Relevant packages, frameworks and tools will be made available and the environment will expose the selected hardware. Details on what resources, volumes, and YAML are used for each of the session types can be found by navigating to the Environment Configs tab on the dashboard and clicking on the card for more details. Contact us for any images that we do not currently support. ","version":"Next","tagName":"h3"},{"title":"Timeout​","type":1,"pageTitle":"Sessions","url":"/shakudo-platform-core/sessions/#timeout","content":" Set the Session timeout to your desired session lifetime. The time may be specified as an arbitrary value, in seconds. To disable the timeout completely, enter -1. Alternatively, a dropdown list of common expiry times is provided for convenience. In the case of in-browser sessions, this value will be interpreted as an idle timeout and will reset every time activity starts again in the session. For SSH connections, it is an absolute timeout: the session will terminate after the specified amount of time has elapsed since session spin-up. ","version":"Next","tagName":"h3"},{"title":"Drive Name - Spinning up multiple Sessions​","type":1,"pageTitle":"Sessions","url":"/shakudo-platform-core/sessions/#drive-name---spinning-up-multiple-sessions","content":" Each Session is connected to a drive containing your files and folders. Drives may only be used in one session at a time. Each user has a default drive (called &lt;username&gt;'s default drive). Users can spin up multiple sessions by selecting a new drive or creating one if all drives are in use. The default drive is only accessible by the corresponding user, other (created) drives are accessible by anyone. To create a drive, click on the drive management icon next to the Drive selection dropdown, then click the &quot;add drive&quot; button.  ","version":"Next","tagName":"h3"},{"title":"Integrating git into Sessions Jupyterlab","type":0,"sectionRef":"#","url":"/shakudo-platform-core/sessions/versionControl","content":"","keywords":"","version":"Next"},{"title":"Features​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#features","content":"All of the following features work under the hood to ensure that git commits are clean and allow for better integration between Jupyter and git. ","version":"Next","tagName":"h2"},{"title":"Merging notebooks with git​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#merging-notebooks-with-git","content":"Jupyter has often had a problem with handling merge conflicts, often causing notebooks to break. nbdev iincludes a merge driver that leaves conflicts in a state that is appropriate for Jupyter. It works in all git commands that use merge under the hood, including merge, pull, rebase, and stash. Here’s what the conflict looks like in Jupyter with nbdev’s merge driver:  For more information on the underlying functionality, read the following docs. ","version":"Next","tagName":"h3"},{"title":"Making commits with clean diffs​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#making-commits-with-clean-diffs","content":"Jupyter notebooks store a variety of metadata that tend to pollute diffs in pull requests and git histories that often cause merge conflicts. nbdev includes a hook that cleans up unnessesary metadata that reduces the presence of insignificant changes.  { &quot;cell_type&quot;: &quot;code&quot;, - &quot;execution_count&quot;: 1, + &quot;execution_count&quot;: 2, &quot;metadata&quot;: { &quot;hide_input&quot;: false }  vs { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {} }  For more details on functionality, read the following blog post. Tutorial ","version":"Next","tagName":"h3"},{"title":"1. Start a Session​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#1-start-a-session","content":"Navigate to the Sessions tab on the dashboardClick the + Start a Session button. You will see a dialog window to start a session like the image below.  Image: Choose the image type to use in the session. In this example, we are going to use the Basic image in the dropdown. For more information on Session Types and other configurations check out the Guide on Sessions. ImageURL: You can paste any image URL in the Image Url field. This will overwrite the Image above field that we have chosen and use the ImageURL instead. As we will be using the Basic image, we will be skipping this. Timeout: Choose the idle timeout for the session. Idle timeout is defined as the number of seconds from which the session has been continuously idling. The default is 15 minutes. Drive: Drive is the persistent volume that this session will use. Persistent volumes is a Kubernetes term, imagine it as a hard drive in a laptop. You can have multiple drives and manage your drives by clicking on the icon to the right of the Drive field. Select the drive you'd like to stick with for this session. ","version":"Next","tagName":"h2"},{"title":"2. Access the Session​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#2-access-the-session","content":"Once the Session is ready, you'll see a Jupyterlab icon among other options. Select the Jupyterlab option to begin using Jupyterlab.  ","version":"Next","tagName":"h2"},{"title":"3. Set up git​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#3-set-up-git","content":"Within the Session Jupyterlab, users are able to use a terminal application to enter Linux commands. Select the terminal application  ","version":"Next","tagName":"h2"},{"title":"3.1 Set up ssh access to GitHub repositories​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#31-set-up-ssh-access-to-github-repositories","content":"The standard method of connecting to GitHub securely is to set up Secure Shell Protocol (SSH) connections with GitHub when making any changes to a repository. This will only have to be done once per drive, as the portion necessary to establish connections will reside within the drive. If you have done this previously, feel free to move on to creating the git repository. Run the following command, substituting in your GitHub email. ssh-keygen -t ed25519 -C &lt;your_email@example.com&gt;  When prompted with &quot;Enter a file in which to save the key&quot;, feel free to press &quot;Enter&quot; to accept the default file location. If you have previously created SSH keys it may cause you to rewrite that other key, so feel free to chance the location of the new key. When prompted to type in a secure passphrase, feel free to enter in a passphrase of your choice. To avoid entering the passphrase every time you connect, you can securely save your passphrase in the SSH agent. Here's more information on how to work with the passphrase. Start the ssh-agent. eval &quot;$(ssh-agent -s)&quot;  Add your SSH private key to the ssh-agent. If you changed the name of your key, feel free to substitute out id_ed25519 with the name. ssh-add ~/.ssh/id_ed25519  Copy the SSH public key to your clipboard.  cat ~/.ssh/id_ed25519.pub # Then select and copy the contents of the id_ed25519.pub file # displayed in the terminal to your clipboard  In GitHub, in the upper-right corner of any page, click on your profile photo, then click on Settings.  In the &quot;Access&quot; section of the sidebar, click &quot;SSH and GPG keys&quot;. Click New SSH key. In the &quot;Title&quot; field, add a descriptive label for the key. In the &quot;Key&quot; field, paste your public key. For more details, read the following docs. ","version":"Next","tagName":"h3"},{"title":"3.2 Set up git repository​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#32-set-up-git-repository","content":"Create a new repository ","version":"Next","tagName":"h3"},{"title":"New repository​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#new-repository","content":"mkdir &lt;new directory&gt; cd &lt;new directory&gt; echo &quot;# New repository&quot; &gt;&gt; README.md git init git add README.md git commit -m &quot;first commit&quot; git branch -M main git remote add origin &lt;remote repository url starting with git@github.com&gt; git push -u origin main  or clone an existing repository within the current directory. ","version":"Next","tagName":"h3"},{"title":"Existing repository​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#existing-repository","content":"git clone &lt;remote repository url starting with git@github.com&gt; cd &lt;repo name&gt;  ","version":"Next","tagName":"h3"},{"title":"4. Verify that nbdev2 is installed​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#4-verify-that-nbdev2-is-installed","content":"nbdev 2.0+ is the package required to use the current functionality. All default Shakudo images currently have nbdev2. Run this command to determine whether you have nbdev 2.0+ pip show nbdev  If not, install by running pip install nbdev  Then check by running pip show nbdev  ","version":"Next","tagName":"h2"},{"title":"5. Set up nbdev git hooks​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#5-set-up-nbdev-git-hooks","content":"As mentioned, nbdev uses git hooks to clean up git commits and allow for version control compatibility. Within the base directory of the repo, type in the following: nbdev_install_hooks  The response after running this command should be Hooks are installed. Now you should be able to use git alongside Jupyter properly. For further support in installing nbdev git hooks, follow this tutorial. ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#example","content":"Here is an example of creating a Jupyter notebook file and seeing merge conflicts occur within the notebook. ","version":"Next","tagName":"h2"},{"title":"1. Verify that you are within the git repository within the graphical interface​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#1-verify-that-you-are-within-the-git-repository-within-the-graphical-interface","content":"The sample git repository has the name nbdev_test, so that is the directory that will be used.   ","version":"Next","tagName":"h3"},{"title":"2. Create a notebook through Jupyterlab​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#2-create-a-notebook-through-jupyterlab","content":" ","version":"Next","tagName":"h3"},{"title":"3. Commit the new notebook to git​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#3-commit-the-new-notebook-to-git","content":"From the base directory of the repo. git add . git commit -m &quot;Add blank notebook&quot;  ","version":"Next","tagName":"h3"},{"title":"4. Create a new branch from the current branch​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#4-create-a-new-branch-from-the-current-branch","content":"git checkout -b merge_branch_1  Where merge_branch_1 is a sample branch name. ","version":"Next","tagName":"h3"},{"title":"5. Enter text within the first cell and press save (CTRL/CMD+S)​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#5-enter-text-within-the-first-cell-and-press-save-ctrlcmds","content":" ","version":"Next","tagName":"h3"},{"title":"6. Commit the file​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#6-commit-the-file","content":"git add . git commit -m &quot;Add foo print&quot;  ","version":"Next","tagName":"h3"},{"title":"7. Checkout the parent branch​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#7-checkout-the-parent-branch","content":"git checkout main  Feel free to replace main with whatever branch merge_branch_1 was based on. ","version":"Next","tagName":"h3"},{"title":"8. Create another branch from the current/base branch​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#8-create-another-branch-from-the-currentbase-branch","content":"git checkout -b merge_branch_2  Where merge_branch_2 is a sample branch name. ","version":"Next","tagName":"h3"},{"title":"10. Enter differing text within the first cell and press save (CTRL/CMD+S)​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#10-enter-differing-text-within-the-first-cell-and-press-save-ctrlcmds","content":" ","version":"Next","tagName":"h3"},{"title":"11. Commit the file​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#11-commit-the-file","content":"git add . git commit -m &quot;Add bar print&quot;  ","version":"Next","tagName":"h3"},{"title":"12. Run a merge from merge_branch_1 and check for merge conflict in Jupyter​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#12-run-a-merge-from-merge_branch_1-and-check-for-merge-conflict-in-jupyter","content":"git merge merge_branch_1  You will need to re-open the Jupyter notebook to see the new update. You will now be able to see a merge conflict displayed cleanly in your notebook.  ","version":"Next","tagName":"h3"},{"title":"13. Resolve the merge conflict​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#13-resolve-the-merge-conflict","content":"Remove the diff lines &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt; merge_branch_1 along with any lines you'd like to remove. Press Save to save changes.  ","version":"Next","tagName":"h3"},{"title":"14. Commit merge conflict changes​","type":1,"pageTitle":"Integrating git into Sessions Jupyterlab","url":"/shakudo-platform-core/sessions/versionControl#14-commit-merge-conflict-changes","content":"Then from the terminal, type: git add . git commit  and type CTRL+X to exit the nano editor. ","version":"Next","tagName":"h3"},{"title":"Shakudo API","type":0,"sectionRef":"#","url":"/shakudo-platform-core/shakudoApi","content":"","keywords":"","version":"Next"},{"title":"Notebook_Common​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#notebook_common","content":"notebook_common is part of the Shakudo Platform Hyperplane API that contains convenience functions for Dask and pipeline jobs. It contains functions to manage Dask clusters, pipeline jobs, and Slack messages, and GraphQL operations. ","version":"Next","tagName":"h2"},{"title":"Dask​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#dask","content":"Dask is a flexible open source distributed framework for parallel computing. It has similar APIs to NumPy and Pandas, is an ideal choice for parallelizing NumPy, Pandas and List based code. ","version":"Next","tagName":"h3"},{"title":"quickstart_dask()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#quickstart_dask","content":"Use quickstart_dask to quickly spin up a Dask cluster using t-shirt sizes. Returns a tuple [Client, KubeCluster]. from hyperplane.notebook_common import quickstart_dask client, cluster = quickstart_dask( num_workers = 4, size = 'hyperplane-med-high-mem' )  Parameters Name\tType\tDescriptionnum_workers Required integer\tNumber of workers size Required string\tPre-configured worker pools   Pre-configured Worker pools The preconfigured worker pools are the following Name\tWorker Pool\tAllocatable cores\tAllocatable ramhyperplane-xs-high-mem\tPOOL_4_32\t3.5\t7.0 hyperplane-small\tPOOL_8_8\t7.0\t5.0 hyperplane-small-mid-mem\tPOOL_8_16\t7.5\t12.0 hyperplane-small-high-mem\tPOOL_8_64\t7.5\t58.0 hyperplane-med\tPOOL_16_16\t15.0\t12.0 hyperplane-med-mid-mem\tPOOL_16_32\t15.0\t27.0 hyperplane-med-high-mem\tPOOL_16_128\t15.0\t110.0 hyperplane-large\tPOOL_32_32\t28.0\t27.0 hyperplane-xxl-high-mem\tPOOL_96_768\t94.0\t675.0  ","version":"Next","tagName":"h3"},{"title":"initialize_dask_cluster()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#initialize_dask_cluster","content":"Initialize a distributed DASK cluster. Returns a tuple [Client, KubeCluster]. You may use the returned client and cluster like any other dask cluster. from hyperplane.notebook_common import initialize_dask_cluster client, cluster = initialize_dask_cluster( num_workers:int=2, local_mode:bool=False, worker_spec_yaml:str=WORKER_SPEC_TEMPLATE_1_1, timeout:int=1200, nthreads:int=1, nprocs:int=15, ram_gb_per_proc:float=0.7, cores_per_worker:int=15, scheduler_deploy_mode:str=&quot;remote&quot;, dashboard_port:str=&quot;random&quot;, logging:str=&quot;quiet&quot; )   Parameters  Name\tType\tDescriptionnum_workers\tinteger\t(Default value: 2) Number of Dask worker nodes. local_mode\tbool\tWhether to use local cluster or distributed KubeCluster worker_spec_yaml\tstring\tA string YAML for cluster configs timeout\tinteger\tTime limit (seconds) for a scheduler to wait to connect before returning a timeout error nthreads\tinteger\tNumber of threads per worker in your cluster nprocs\tinteger\tNumber of processes per worker in your cluster ram_gb_per_proc\tfloat\tGB of Ram per process, per worker cores_per_worker\tinteger\tNumber of cores per worker scheduler_deploy_mode\tstring\tWhere to deploy the scheduler (remote in its own worker, or locally in jhub). Choose remote when the Dask graph dashboard_port\tstring\tChoose a port number for your dashboard, or leave as &quot;random&quot; to have a random port, which will not conflict logging\tstring\tLogging level for printouts when initializing. Available options are verbose or quiet. note The number of dask workers in the cluster will be the num_workers x num_procs. Shakudo platform will automatically choose the closest pool from the pre-configured node pool based on the combination of parameters specified. Example from hyperplane import notebook_common as nc client, cluster = nc.initialize_dask_cluster(num_workers=2)  from hyperplane import notebook_common as nc client, cluster = nc.initialize__dask_cluster( num_workers=2, nthreads=1, nprocs=15, ram_gb_per_proc=0.7, cores_per_worker=15 )   ","version":"Next","tagName":"h3"},{"title":"daskpool_candidates​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#daskpool_candidates","content":"Use daskpool_candidates when you'd like to access the list of available dask pools to choose from to spin up a Dask cluster. candidates = nc.daskpool_candidates candidates   ","version":"Next","tagName":"h3"},{"title":"get_dask_cluster()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#get_dask_cluster","content":"Retrieve a Dask cluster. Use this function if there's a Dask cluster that's already spun up that you would like to connect. from hyperplane import notebook_common as nc client = nc.get_dask_cluster(&quot;dask_cluster_name&quot;) client   Parameters  Name\tType\tDescriptiondask_cluster_name\tstring\tName of Dask cluster To retrieve the Dask cluster name, navigate to the Ray &amp; Dask tab on the platform and click the copy button in the table column Cluster Name.  ","version":"Next","tagName":"h3"},{"title":"cluster.close() & client.close()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#clusterclose--clientclose","content":"Use cluster.close() and client.close() to destroy or shut down a dask cluster after it is no longer needed to free up resources. The platform comes with an automatic garbage collection functionality - if you forget to close the cluster the platform will automatically close it after a few minutes of idle time. Starting a cluster and shutting it down: from hyperplane import notebook_common as nc client, cluster = nc.initialize_dask_cluster(num_workers=2) cluster.close() client.close()  Retrieving a forgotten Dask cluster and closing it: from hyperplane import notebook_common as nc client = nc.get_dask_cluster(&quot;dask-cluster-with-some-random-hash&quot;) cluster.close() client.close()   ","version":"Next","tagName":"h3"},{"title":"client.restart()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#clientrestart","content":"Use client.restart whenever you want to clean up dask memory.  client.restart()  note Dask remembers every line of code that was run since initializing the cluster. If you'd like to edit a line of code after it's already been run once, then restart the dask client to ensure that the script runs smoothly.  ","version":"Next","tagName":"h3"},{"title":"Pipeline Jobs​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#pipeline-jobs","content":"There are many ways pipeline jobs can be controlled: dashboard interface, GraphQL Playground, and Hyperplane API notebook_commons. You can submit, cancel, get output, and check status on jobs from your Sessions. ","version":"Next","tagName":"h3"},{"title":"submit_pipeline_job​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#submit_pipeline_job","content":"Use this function to submit a job from your Sessions. See Create a pipeline for details on submission query fields. The function returns a job id and runID. Navigate to the Shakudo Platform Jobs tab and filter by the returned job ID if you'd like to check that the job has been successfully submitted. from hyperplane import notebook_common as nc newjob = await nc.submit_pipeline_job( jobName = 'name_of_newjob', pipelineYamlPath = 'yaml_path.yaml', jobType = 'basic', timeout = 1800, active_timeout = 1800, max_retries = 2, parameters = { &quot;a&quot;:1, &quot;b&quot;:1 } ) newjob   Parameters  Name\tType\tDescriptionjobName\tstring\tcustom name for your job pipelineYamlPath Required integer\t(Default value: 2) Number of Dask worker nodes. jobType\tstring\tJob (EC) type to use timeout\tinteger\tMaximum time that the pipeline may run, starting from the moment of job submission active_timeout\tinteger\tMaximum time that the pipeline may run once it is picked up max_retries\tinteger\tNumber of times to retry the job if the job run has failed or timed out. parameters\tdictionary\tKey value pairs for any parameters in your script you'd like to overwrite for this pipeline job  ","version":"Next","tagName":"h3"},{"title":"checkjobs()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#checkjobs","content":"Check status on jobs using job IDs. Returns a summary of job status and links to Dask dashboards if Dask is used. ids = ['a-job-id', 'another-job-id'] res = await nc.checkjobs(ids, loop = True) print(res)  Output will look like the following: #### Jobs summary 0 / 2 in progress 0 / 2 pending 2 / 2 processed 2 done | 0 timed out | 0 cancelled | 0 failed Progress: 100.0% #### Dask dashboards a-job-id done None another-job-id done None   Parameters  Name\tType\tDescriptionids Required list\tList of job IDs to check status loop\tboolean\t(Default value: False) True will refresh the output every 5 seconds until all jobs are processed interval\tinteger\tRefresh frequency for loop = True  ","version":"Next","tagName":"h3"},{"title":"cancel_pipeline_jobs()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#cancel_pipeline_jobs","content":"Use cancel_pipeline_jobs() to cancel a pipeline job from Sessions. Returns {'id': 'job-id', 'status': 'cancelled} await nc.cancel_pipeline_job('job-id')   Parameters  Name\tType\tDescriptionjobID Required string\tID of pipeline job to cancel  ","version":"Next","tagName":"h3"},{"title":"GraphQL​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#graphql","content":"Submit a GraphQL query using graphql_operations() function. Example for submitting a job gql_query = ''' mutation { createShakudoPipelineJob( jobName: &quot;unruffled_ardinghelli&quot;, jobType: &quot;basic&quot;, timeout: 1800, activeTimeout: 1800, maxRetries: 2, pipelineYamlPath: &quot;yaml_path.yaml&quot;, defaultCommands: true, gitInit: true, commitId: &quot;&quot;, branchName: &quot;&quot;, parameters: { create: [ { key: &quot;a&quot;, value: &quot;1&quot; }, ]} ) { id jobName schedule parameters { key value } noGitInit noHyperplaneCommands } } ''' gql_queries = [gql_query] results = await nc.graphql_operation(gql_queries)   Parameters  Name\tType\tDescriptiongql_queries Required Union[str, List[str]]\tGraphQL query for relevant operation  ","version":"Next","tagName":"h3"},{"title":"Ray_Common​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#ray_common","content":"ray_common is part of the Shakudo Platform Hyperplane API that contains convenience functions to manage Ray clusters. We support extensions to the basic Ray framework by supporting Ray Tune, Ray Spark, Ray with RAPIDS, and more. ","version":"Next","tagName":"h2"},{"title":"Ray​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#ray","content":"Ray is an open source project that distributed frameworks that has a more support for deep learning and reinforcement learning. It has a rich set of libraries and integrations built on a flexible distributed execution framework, is ideal choice for parallelizing model training and hyper-parameter tuning. ","version":"Next","tagName":"h3"},{"title":"quickstart_ray()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#quickstart_ray","content":"Use quickstart_ray to quickly spin up a Ray cluster using t-shirt sizes (Sizes are the same as quick start for Dask clusters). from hyperplane import ray_common as rc ray_cluster = rc.quickstart_ray( num_workers = 4, size = 'hyperplane-med-high-mem' )  Parameters Name\tType\tDescriptionnum_workers Required integer\tNumber of workers size Required object\tPre-configured worker pools   ","version":"Next","tagName":"h3"},{"title":"initialize_ray_cluster()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#initialize_ray_cluster","content":"Initialize a distributed Ray cluster with ease and more customizability. You can also run this function to clean up the Ray nodes and re-initialize. from hyperplane import ray_common as rc ray_cluster = rc.initialize_ray_cluster( num_workers = 4, cpu_core_per_worker = 4, ram_gb_per_worker = 4, n_gpus = 0 )   Parameters  Name\tType\tDescriptionnum_workers\tinteger\t(Default value: 2) Number of Ray nodes to be initialized cpu_core_per_worker\tinteger\tNumber of CPU cores in each Ray node ram_gb_per_worker\tfloat\tMemory size in GB for each Ray node n_gpus\tinteger\tNumber of Nvidia GPUs in each Ray node (if n_gpus &gt; 0, cpu_core_per_worker and ram_gb_per_worker are ignored) use_existing\tboolean\t(Default: use_existing = False) Whether to connect to/ reinitialize existing Ray cluster or spin up a new one note If you are aiming for a specific pool, ensure your cpu_core_per_worker = the number of allocatable cores and ram_gb_per_worker = the allocatable ram. For example, if you would like to use a POOL_16_16 worker, you may want to use the following cluster initialization. from hyperplane import ray_common as rc ray_cluster = rc.initialize_ray_cluster( num_workers = 4, cpu_core_per_worker = 15, ram_gb_per_worker = 12 )   ","version":"Next","tagName":"h3"},{"title":"stop_ray_cluster()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#stop_ray_cluster","content":"Use stop_ray_cluster to shutdown a Ray cluster. After computation is finished, it's a good idea to shutdown the distributed cluster and release the resources back to the node pool. If any Ray nodes are left hanging, Shakudo Platform's garbage collection function will also automatically shutdown the Ray workers when the Session or job is finished. from hyperplane import ray_common as rc rc.stop_ray_cluster(ray_cluster)   Parameters  Name\tType\tDescriptionray_cluster Required object\tRay cluster to shutdown  ","version":"Next","tagName":"h3"},{"title":"get_ray_cluster()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#get_ray_cluster","content":"Reconnect to a Ray cluster by using the get_ray_cluster to retrieve the cluster. You can use this function if you've already spun up a Ray cluster and want to connect to the same cluster (for example: in another notebook in the same session). This function will connect to an existing cluster. There are two ways to reconnect to Ray clusters. from hyperplane import ray_common as rc rc.get_ray_cluster(extra_workers = 1)   Parameters  Name\tType\tDescriptionextra_workers\tinteger\tAdds nodes to your existing cluster (Default: extra_workers = 0) The nodes that are added to the cluster will be of the same specification as the original cluster. There are two ways to reconnect to Ray clusters. The method using the function get_ray_cluster() is the simpler and recommended way. You can also use the initialize_ray_cluster() to accomplish the same. Note, the arguments for cpu_core_per_worker and ram_gb_per_worker must be the same as when you initialized the cluster originally. from hyperplane import ray_common as rc ray_cluster = rc.initialize_ray_cluster( num_workers = 0, cpu_core_per_worker = 15, ram_gb_per_worker = 12, use_existing = True )   ","version":"Next","tagName":"h3"},{"title":"find_ray_workers​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#find_ray_workers","content":"Use find_ray_workers() function to see if there are any Ray workers already spun up. Returns a list of Ray workers running. from hyperplane import ray_common as rc rc.find_ray_workers()   ","version":"Next","tagName":"h3"},{"title":"Utils​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#utils","content":"","version":"Next","tagName":"h2"},{"title":"get_service_url​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#get_service_url","content":"The get_service_url function will return the internal service URL for a running service on the platform. This is useful when another service or job uses an existing service. Using the internal service URL allows bypassing the authentication and lowers latency. from hyperplane import utils import requests model_inference_url = utils.get_service_url('model_inference_endpoint') requests.get(model_inference_url)  ","version":"Next","tagName":"h3"},{"title":"Slack​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#slack","content":"Shakudo's Hyperplane API includes a function allowing jobs to post messages to your Slack channel. You can use this to notify you of finished jobs, or even to post job results. On Shakudo there are two ways to send Slack notification messages: Use a slack token or use a webhook. Method 1: Slack tokenTo use the Slack alert integration you first need to set up a Slack token. A slack token can be set up following this tutorial on Slack by creating a quick pre-configured App. Once the App is created and installed to the workspace, the token can be found at the Install App tab in the App's homepage. The token looks like this xoxb-694301530724-2549825675367-Zn4NNP34r3c7aN3EkPDLMiNX Method 2: Slack Webhook URL To send messages to Slack with webhooks, first follow this tutorial on Slack to create a new Slack App and obtain the webhook URL like this https://hooks.slack.com/services/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 note You can also ask the Shakudo Platform admin to add or change the WEBHOOK_URL variable in your environment permanently. ","version":"Next","tagName":"h3"},{"title":"SlackHelper​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#slackhelper","content":"Initialize a Slack Helper object after specifying your Slack token. Method 1: Token method #Specify your Slack token import os os.environ[&quot;SLACK_TOKEN&quot;] = &quot;your-slack-t0ken&quot; #Initialize Slack Helper from hyperplane import utils sh = utils.SlackHelper()  note When using Method 2: Slack Webhook URL then you do not need to initialize SlackHelper. Just add the webhook URL as an environment variable. import os os.environ['WEBHOOK_URL']='your_webhook_url'   ","version":"Next","tagName":"h3"},{"title":"post_message​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#post_message","content":"Post a Slack Message with Slack Token. Method 1: Token method from hyperplane import utils sh = utils.SlackHelper() sh.post_message('Testing!', '#some-channel')  Method 2: Webhook method #Post a message from hyperplane import utils utils.post_message('Testing!', '#some-channel')   Parameters  Name\tType\tDescriptionmessage Required string\tString of message to send (can include formatting) channel\tstring\t(Default: &quot;&quot;) A public channel to post the message. If left empty, it will default to the webhook URL's default channel  ","version":"Next","tagName":"h3"},{"title":"post_table()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#post_table","content":"Post a Slack Message in table format with Slack Token. Method 1: Token method from hyperplane import utils sh = utils.SlackHelper() message_dict = { &quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot; } sh.post_table(message_dict, channel = &quot;alert-channel&quot;)  Method 2: Webhook method from hyperplane import utils message_dict = { &quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot; } sh.post_table(message_dict, channel = &quot;alert-channel&quot;)   Parameters  Name\tType\tDescriptionmessage_dict Required dictionary\tDictionary of values to send in a table format channel\tstring\t(Default: &quot;&quot;) A public channel to post the msg. If left empty, it will default to the webhook URL's default channel  ","version":"Next","tagName":"h3"},{"title":"NLP models​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#nlp-models","content":"Hyperplane comes with some pre-packged NLP models for common tasks. The hyperplane nlp tools can be accessed through from hyperplane import hyper_nlp as nlp  ","version":"Next","tagName":"h2"},{"title":"Extract topics and themes​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#extract-topics-and-themes","content":"Find the main topics and themes with the function get_topics() given a list of sentences, documents, or texts. Returns: Topic_Model, List[str] get_topics(data:List[str], ntopic:int=10, sample_size:int=50000, method:str=&quot;USE&quot; )   Parameters  Name\tType\tDescriptiondata Required List[str]\tA list of strings (sentences or phrases) from which you want to discover topics and themes ntopic\tinteger\tThe number of topics/themes to return sample_size\tinteger\tThe number of samples to discover topics from method\tstring\tThe method for theme extraction. Choose from TFIDF, LDA, BERT, LDA_BERT, fasttext, USE Example: %matplotlib inline ## use this in a notebook to display charts nlp.get_topics([&quot;I need to buy some green and red apples.&quot;, &quot;Oranges are also useful for juices&quot;, &quot;I really like bananas and fruits&quot;, &quot;You need to clean up your car.&quot;, &quot;I am running out of out of gas&quot;], ntopic=2, sample_size=50, method=&quot;USE&quot; )  The function will return the topic model and a list of top words per cluster: (&lt;topic_discovery.Topic_Model at 0x7f88abc8e9a0&gt;, array([['out', 'of', 'I', 'am', 'running', 'gas', 'You', 'need', 'to', 'clean'], ['I', 'and', 'Oranges', 'are', 'also', 'useful', 'for', 'juices', 'need', 'to']], dtype=object))  ","version":"Next","tagName":"h3"},{"title":"Extract text​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#extract-text","content":"With the function extract_qa() when given some text, you can ask a question to extract an arbitrary field. Returns: List[Dict] from hyperplane import hyper_nlp as nlp nlp.get_topics(data:List[str], ntopic:int=10, sample_size:int=50000, method:str=&quot;USE&quot; )   Parameters  Name\tType\tDescriptiontext Required string\tContext string which the answer will be extracted from question Required string\tA question that you want to ask based on the context topk\tinteger\tThe number of top answers to return per question return_context\tboolean\tWhether or not to return context around the answer context_window\tinteger\tIf return_context how much of the context to return Example: text = ''' Hyperplane is an end-to-end platform designed to take AI teams from ideation to production at breakthrough speeds. We built Hyperplane because we needed a powerful platform for our scientists to design, develop, deploy and maintain their own work in production. Why Us? The Shakudo team grew out of advanced AI organizations across the industry. After having seen, tried and used every product out there, we came to the conclusion that there is a gap to be filled, and Hyperplane was born. What does this mean for you? If you are scaling up an AI organization, starting up an AI-powered product, or looking to get your existing solutions faster and more reliably to production, Hyperplane may be for you. ''' questions = [ &quot;What does Hyperplane do?&quot; ] nlp.extract_qa(text, questions[0])  This will return: [{'score': 0.04723832756280899, 'start': 51, 'end': 115, 'answer': 'take AI teams from ideation to production at breakthrough speeds'}]  ","version":"Next","tagName":"h3"},{"title":"Save and load​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#save-and-load","content":"To save serializable models, you can use the following: nlp.save_model(model=tm, filename=&quot;tm_model&quot;)  where tm is a serializable object. To load an existing model saved with hyper_nlp, use the following: loaded_model = nlp.load_model(filename='tm_model')  ","version":"Next","tagName":"h3"},{"title":"Find urls​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#find-urls","content":"Use the function find_urls() to look for strings that are urls Returns: List[str] nlp.find_urls(s:str)  Parameters Name\tType\tDescriptions Required string\tA string from which you would like to search for urls ","version":"Next","tagName":"h3"},{"title":"Remove stopwords​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#remove-stopwords","content":"Use the function clean_text() to remove stopwords from a string of text. Returns a string with stopwords removed. nlp.clean_text(s:str, remove_list:List[str]=en_stop_words)  Parameters Name\tType\tDescriptions Required string\tA string which you would like to clean by removing stopwords remove_list\tList[str]\tA list of strings to include as stopwords to remove ","version":"Next","tagName":"h3"},{"title":"Extract text from pdfs​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#extract-text-from-pdfs","content":"Use extract_digital_pdf() function to extract text from pdfs. Returns a string of text. extract_digital_pdf(filepath:str, auto_clean_threshold:int=0)  Parameters Name\tType\tDescriptionfilepath Required string\tA filepath location for the digital pdf to extract auto_clean_threshold\tinteger\tA threshold for removing words that are too short. Keep at 0 to keep all words that are not stopwords. Use any other positive integer to remove words containing fewer than auto_clean_threshold number of letters. ","version":"Next","tagName":"h3"},{"title":"Utils​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#utils-1","content":"","version":"Next","tagName":"h2"},{"title":"get_service_url​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#get_service_url-1","content":"The get_service_url function will return the internal service URL for a running service on the platform. This is useful when another service or job uses an existing service. Using the internal service URL allows bypassing the authentication and lowers latency. from hyperplane import utils import requests model_inference_url = utils.get_service_url('model_inference_endpoint') requests.get(model_inference_url)  ","version":"Next","tagName":"h3"},{"title":"Slack​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#slack-1","content":"Shakudo's Hyperplane API includes a function allowing jobs to post messages to your Slack channel. You can use this to notify you of finished jobs, or even to post job results. On Shakudo there are two ways to send Slack notification messages: Use a slack token or use a webhook. Method 1: Slack tokenTo use the Slack alert integration you first need to set up a Slack token. A slack token can be set up following this tutorial on Slack by creating a quick pre-configured App. Once the App is created and installed to the workspace, the token can be found at the Install App tab in the App's homepage. The token looks like this xoxb-694301530724-2549825675367-Zn4NNP34r3c7aN3EkPDLMiNX Method 2: Slack Webhook URL To send messages to Slack with webhooks, first follow this tutorial on Slack to create a new Slack App and obtain the webhook URL like this https://hooks.slack.com/services/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 note You can also ask the Shakudo Platform admin to add or change the WEBHOOK_URL variable in your environment permanently. ","version":"Next","tagName":"h3"},{"title":"SlackHelper​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#slackhelper-1","content":"Initialize a Slack Helper object after specifying your Slack token. Method 1: Token method #Specify your Slack token import os os.environ[&quot;SLACK_TOKEN&quot;] = &quot;your-slack-t0ken&quot; #Initialize Slack Helper from hyperplane import utils sh = utils.SlackHelper()  note When using Method 2: Slack Webhook URL then you do not need to initialize SlackHelper. Just add the webhook URL as an environment variable. import os os.environ['WEBHOOK_URL']='your_webhook_url'   ","version":"Next","tagName":"h3"},{"title":"post_message​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#post_message-1","content":"Post a Slack Message with Slack Token. Method 1: Token method from hyperplane import utils sh = utils.SlackHelper() sh.post_message('Testing!', '#some-channel')  Method 2: Webhook method #Post a message from hyperplane import utils utils.post_message('Testing!', '#some-channel')   Parameters  Name\tType\tDescriptionmessage Required string\tString of message to send (can include formatting) channel\tstring\t(Default: &quot;&quot;) A public channel to post the message. If left empty, it will default to the webhook URL's default channel  ","version":"Next","tagName":"h3"},{"title":"post_table()​","type":1,"pageTitle":"Shakudo API","url":"/shakudo-platform-core/shakudoApi#post_table-1","content":"Post a Slack Message in table format with Slack Token. Method 1: Token method from hyperplane import utils sh = utils.SlackHelper() message_dict = { &quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot; } sh.post_table(message_dict, channel = &quot;alert-channel&quot;)  Method 2: Webhook method from hyperplane import utils message_dict = { &quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot; } sh.post_table(message_dict, channel = &quot;alert-channel&quot;)   Parameters  Name\tType\tDescriptionmessage_dict Required dictionary\tDictionary of values to send in a table format channel\tstring\t(Default: &quot;&quot;) A public channel to post the msg. If left empty, it will default to the webhook URL's default channel ","version":"Next","tagName":"h3"},{"title":"Jupyter Shared Sessions","type":0,"sectionRef":"#","url":"/shakudo-platform-core/shared_sessions","content":"","keywords":"","version":"Next"},{"title":"Features of Jupyter Shared Sessions​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/shared_sessions#features-of-jupyter-shared-sessions","content":"Real-Time Collaboration: Multiple users can simultaneously edit and execute cells in notebooks or modify files in real time.Shared Cursors: Visualize where collaborators are working, with disappearing username indicators to maintain focus on the document.Automatic Saving: All changes are saved automatically after a short interval, ensuring no data is lost.Conflict-Free Editing: No warning dialogs or conflicts when editing the same file—synchronization is seamless.Environment Sharing: Collaborators gain access to the same runtime environment, ensuring consistency in data and code execution.  ","version":"Next","tagName":"h2"},{"title":"Step-by-Step Guide on How to Create and Share a Session​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/shared_sessions#step-by-step-guide-on-how-to-create-and-share-a-session","content":"","version":"Next","tagName":"h2"},{"title":"1. Create a Session with jhub-basic environment config​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/shared_sessions#1-create-a-session-with-jhub-basic-environment-config","content":"Navigate to the Sessions panel and click on the create session button. Select the Basic environment config and click on the create session button. Wait for the session to start and initialize the JupyterLab server. Once the JupyterLab icon appears, click on it to open the session. ","version":"Next","tagName":"h3"},{"title":"2. Within JupyterLab Create a RTC python Notebook​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/shared_sessions#2-within-jupyterlab-create-a-rtc-python-notebook","content":"Click on the Python 3 ipykernel icon. Start developing. ","version":"Next","tagName":"h3"},{"title":"3. Create a shared link to invite other users​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/shared_sessions#3-create-a-shared-link-to-invite-other-users","content":"Click on the link icon in the top right corner of the screen. Copy the link and share it with your additional users. Alternatively, include the session token in the shared link by selecting the checkbox. To share your Session with non-admins, you will need to publish your Session in the Shakudo dashboard. ","version":"Next","tagName":"h3"},{"title":"4. Access a session via a shared link​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/shared_sessions#4-access-a-session-via-a-shared-link","content":"Log into the session if the token was not provided in the URL (default password is hyperhub). ","version":"Next","tagName":"h3"},{"title":"5. Start developing together​","type":1,"pageTitle":"Jupyter Shared Sessions","url":"/shakudo-platform-core/shared_sessions#5-start-developing-together","content":"View the cursor and highlighted text of the other user ","version":"Next","tagName":"h3"},{"title":"Airflow","type":0,"sectionRef":"#","url":"/Shakudo-stack/CI_CD/airflow","content":"Airflow Coming soon","keywords":"","version":"Next"},{"title":"Prefect","type":0,"sectionRef":"#","url":"/Shakudo-stack/CI_CD/prefect","content":"Prefect Coming soon","keywords":"","version":"Next"},{"title":"Airbyte","type":0,"sectionRef":"#","url":"/Shakudo-stack/dataIngestion/airbyte","content":"Airbyte Coming Soon","keywords":"","version":"Next"},{"title":"DBT","type":0,"sectionRef":"#","url":"/Shakudo-stack/dataTransformation/dbt","content":"DBT Coming soon","keywords":"","version":"Next"},{"title":"DuckDB","type":0,"sectionRef":"#","url":"/Shakudo-stack/dataTransformation/duckdb","content":"DuckDB Coming soon","keywords":"","version":"Next"},{"title":"Cube.js","type":0,"sectionRef":"#","url":"/Shakudo-stack/datavisualization/cube","content":"Cube.js Coming soon","keywords":"","version":"Next"},{"title":"Streamlit","type":0,"sectionRef":"#","url":"/Shakudo-stack/datavisualization/streamlit","content":"Streamlit Coming soon","keywords":"","version":"Next"},{"title":"Superset","type":0,"sectionRef":"#","url":"/Shakudo-stack/datavisualization/superset","content":"Superset Coming soon","keywords":"","version":"Next"},{"title":"Dask","type":0,"sectionRef":"#","url":"/Shakudo-stack/distributedComputing/dask","content":"","keywords":"","version":"Next"},{"title":"Notebook_Common​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#notebook_common","content":"notebook_common is part of the Shakudo Platform Hyperplane API that contains convenience functions for Dask and pipeline jobs. It contains functions to manage Dask clusters, pipeline jobs, and Slack messages, and GraphQL operations. ","version":"Next","tagName":"h2"},{"title":"quickstart_dask()​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#quickstart_dask","content":"Use quickstart_dask to quickly spin up a Dask cluster using t-shirt sizes. Returns a tuple [Client, KubeCluster]. from hyperplane.notebook_common import quickstart_dask client, cluster = quickstart_dask( num_workers = 4, size = 'hyperplane-med-high-mem' )  Parameters Name\tType\tDescriptionnum_workers Required integer\tNumber of workers size Required string\tPre-configured worker pools   Pre-configured Worker pools The preconfigured worker pools are the following Name\tWorker Pool\tAllocatable cores\tAllocatable ramhyperplane-xs-high-mem\tPOOL_4_32\t3.5\t7.0 hyperplane-small\tPOOL_8_8\t7.0\t5.0 hyperplane-small-mid-mem\tPOOL_8_16\t7.5\t12.0 hyperplane-small-high-mem\tPOOL_8_64\t7.5\t58.0 hyperplane-med\tPOOL_16_16\t15.0\t12.0 hyperplane-med-mid-mem\tPOOL_16_32\t15.0\t27.0 hyperplane-med-high-mem\tPOOL_16_128\t15.0\t110.0 hyperplane-large\tPOOL_32_32\t28.0\t27.0 hyperplane-xxl-high-mem\tPOOL_96_768\t94.0\t675.0  ","version":"Next","tagName":"h2"},{"title":"initialize_dask_cluster()​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#initialize_dask_cluster","content":"Initialize a distributed DASK cluster. Returns a tuple [Client, KubeCluster]. You may use the returned client and cluster like any other dask cluster. from hyperplane.notebook_common import initialize_dask_cluster client, cluster = initialize_dask_cluster( num_workers:int=2, local_mode:bool=False, worker_spec_yaml:str=WORKER_SPEC_TEMPLATE_1_1, timeout:int=1200, nthreads:int=1, nprocs:int=15, ram_gb_per_proc:float=0.7, cores_per_worker:int=15, scheduler_deploy_mode:str=&quot;remote&quot;, dashboard_port:str=&quot;random&quot;, logging:str=&quot;quiet&quot; )   Parameters  Name\tType\tDescriptionnum_workers\tinteger\t(Default value: 2) Number of Dask worker nodes. local_mode\tbool\tWhether to use local cluster or distributed KubeCluster worker_spec_yaml\tstring\tA string YAML for cluster configs timeout\tinteger\tTime limit (seconds) for a scheduler to wait to connect before returning a timeout error nthreads\tinteger\tNumber of threads per worker in your cluster nprocs\tinteger\tNumber of processes per worker in your cluster ram_gb_per_proc\tfloat\tGB of Ram per process, per worker cores_per_worker\tinteger\tNumber of cores per worker scheduler_deploy_mode\tstring\tWhere to deploy the scheduler (remote in its own worker, or locally in jhub). Choose remote when the Dask graph dashboard_port\tstring\tChoose a port number for your dashboard, or leave as &quot;random&quot; to have a random port, which will not conflict logging\tstring\tLogging level for printouts when initializing. Available options are verbose or quiet. note The number of dask workers in the cluster will be the num_workers x num_procs. Shakudo platform will automatically choose the closest pool from the pre-configured node pool based on the combination of parameters specified. Example from hyperplane import notebook_common as nc client, cluster = nc.initialize_dask_cluster(num_workers=2)  from hyperplane import notebook_common as nc client, cluster = nc.initialize__dask_cluster( num_workers=2, nthreads=1, nprocs=15, ram_gb_per_proc=0.7, cores_per_worker=15 )   ","version":"Next","tagName":"h2"},{"title":"daskpool_candidates​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#daskpool_candidates","content":"Use daskpool_candidates when you'd like to access the list of available dask pools to choose from to spin up a Dask cluster. candidates = nc.daskpool_candidates candidates   ","version":"Next","tagName":"h2"},{"title":"get_dask_cluster()​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#get_dask_cluster","content":"Retrieve a Dask cluster. Use this function if there's a Dask cluster that's already spun up that you would like to connect. from hyperplane import notebook_common as nc client = nc.get_dask_cluster(&quot;dask_cluster_name&quot;) client   Parameters  Name\tType\tDescriptiondask_cluster_name\tstring\tName of Dask cluster To retrieve the Dask cluster name, navigate to the Ray &amp; Dask tab on the platform and click the copy button in the table column Cluster Name.   ","version":"Next","tagName":"h2"},{"title":"cluster.close() & client.close()​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#clusterclose--clientclose","content":"Use cluster.close() and client.close() to destroy or shut down a dask cluster after it is no longer needed to free up resources. The platform comes with an automatic garbage collection functionality - if you forget to close the cluster the platform will automatically close it after a few minutes of idle time. Starting a cluster and shutting it down: from hyperplane import notebook_common as nc client, cluster = nc.initialize_dask_cluster(num_workers=2) cluster.close() client.close()  Retrieving a forgotten Dask cluster and closing it: from hyperplane import notebook_common as nc client = nc.get_dask_cluster(&quot;dask-cluster-with-some-random-hash&quot;) cluster.close() client.close()   ","version":"Next","tagName":"h2"},{"title":"client.restart()​","type":1,"pageTitle":"Dask","url":"/Shakudo-stack/distributedComputing/dask#clientrestart","content":"Use client.restart whenever you want to clean up dask memory.  client.restart()  note Dask remembers every line of code that was run since initializing the cluster. If you'd like to edit a line of code after it's already been run once, then restart the dask client to ensure that the script runs smoothly. ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/Shakudo-stack/distributedComputing/overview","content":"","keywords":"","version":"Next"},{"title":"Dask​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#dask","content":"Dask is a flexible open source distributed framework for parallel computing. It has similar APIs to NumPy and Pandas, is an ideal choice for parallelizing NumPy, Pandas and List based code. Shakudo Platform comes with a number of useful APIs to make using Dask easy. See the Hyperplane API page for a full list. ","version":"Next","tagName":"h2"},{"title":"Dask Collections​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#dask-collections","content":"Dask collections are useful for large datasets because they support delayed tasks. We will explore three types— Dask bags, Dask dataframes, and Dask arrays. Dask bags​ Dask bags (synonymous with multisets) are unordered collections of immutable objects. Below are some common operations: Select records where: b.filter(lambda record: record['num_clicks'] &gt; 2).take(2)  ({'id': '01mz489cnkd', 'area': 'Aerial Alaska', 'num_clicks': 3, 'info': {'a_field': 0}}, {'id': '25z48t9cfaf', 'area': 'Bustling Birktown', 'num_clicks': 5, 'info': {'a_field': 1}})  Select one field: b.map(lambda record: record['area']).take(2)  ('Aerial Alaska', 'Bustling Birktown')  Aggregate the number of records in your bag: b.count().compute()  100000  Note that the .take(n) function will return the first n records from the bag, only in the first partition. For more info, see https://examples.dask.org/bag.html Dask dataframes​ Dask dataframes are collections of pandas dataframes. It can be used in cases where one pandas dataframe is too large to fit in memory and to speed up expensive computations by using multiple cores. To read multiple csvs, use the * or a list of files. Each file will be read into a separate partition. import dask.dataframe as dd df = dd.read_csv('2014-*.csv')  A common workflow is the following: Load large datasets from filesFilter to a subset of dataShuffle data to get an intelligent indexPerform queries or aggregations using the indices For more information on Dask dataframes, see https://docs.dask.org/en/latest/dataframe.html. ","version":"Next","tagName":"h3"},{"title":"Lazy calculations​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#lazy-calculations","content":"Dask operates with lazy collections, meaning operations on a collection are simply scheduled by the scheduler, but the actual calculation will not be triggered until explicitly called. At first, you will notice that dataframe operations or functions seem to happen almost instantaneously, but nothing will be calculated until one of the following is used: .persist().compute().load() .compute() will trigger a computation on the Dask cluster without returning anything. You can use this if some of your functions include saving to a location. .persist() will trigger a computation on the Dask cluster and store the results in ram. Use this sparingly, only if you need to use an intermediate collection, or after a computationally expensive operation such as index, groupby, etc. .load() will trigger a computation on the Dask cluster when you are working with Dask xarrays. For example, you can trigger all computations on your dataframe like the following: df = dask.df.read_parquet('file*.csv') df = df[['col_a', 'col_b']] df = df.drop_duplicates() df = client.persist(df)  ","version":"Next","tagName":"h3"},{"title":"Repartitioning​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#repartitioning","content":"After a few computations, your Dask df may need to be repartitioned, due to the partition size-number tradeoff. Partitions that are too large will cause out of memory errors, while too many partitions will incure a larger overhead time for the schedule to process. See more on best practices at https://docs.dask.org/en/latest/dataframe-best-practices.html. ","version":"Next","tagName":"h3"},{"title":"Split out​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#split-out","content":"Dataframe aggregation operations can get slow. Try to use split_out in aggregation operationg like groupbys to spread the aggregation work. ","version":"Next","tagName":"h3"},{"title":"Cheap vs. expensive computations​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#cheap-vs-expensive-computations","content":"Examples of fast and cheap computations: Element-wise ops (addition, multiplication)Row-wise operations and filtering: df[df.x &gt; 0]Joining Dask dfs along indexed fields, or joining with a one-partition Dask dfMax, min, count, common aggregations (df.groupby(df.x).y.max())isin: df[df.x.isin([1, 2, 3])]drop_duplicatesgroupby-apply on an index: df.groupby(['idx', 'x']).apply(myfunc) Examples of slow and expensive computations (for this reason, it is often recommended to use persist your data after these steps for stability): setting an index: df.set_index(df.x)groupby-apply on non-index fields: df.groupby(df.x).apply(myfunc)joining two dataframes along non-index columns ","version":"Next","tagName":"h3"},{"title":"File types​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#file-types","content":"parquet files​ Parquet is a columnar storage format for Hadoop, which enables parallel reading and writing, and is most useful for efficiently filtering a subset of fields in a Dask df. avro files​ Avro is a row-based storage format for Hadoop, which is most efficient if you intend to retrieve and use all fields or columns in the dataset. ","version":"Next","tagName":"h3"},{"title":"Saving to cloud storage​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#saving-to-cloud-storage","content":"Remember that new workers are spun up when use .initialize_cluster(), and they are destroyed on cluster.close(). This means you should ensure your intermediate and output files are saved in a cloud storage location that can be accessed outside of each node. This can be achieved through the following code example: gcp_project = YOUR_GCP_PROJECT gcs_client = storage.Client(project=gcp_project) bucket = gcs_client.get_bucket(bucket) blob = bucket.blob(yourfile) blob.upload_from_string(filename, content_type='application/x-www-form-urlencoded;charset=UTF-8')  ","version":"Next","tagName":"h3"},{"title":"Choosing Dask workers and specs​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#choosing-dask-workers-and-specs","content":"If you are aiming for a specific pool, ensure that nprocs x nthreads ≤ cores_per_worker ≤ the number of allocatable cores and nprocs x ram_gb_per_proc ≤ allocatable ram. For example, if you would like to use a DASK_POOL_16_16 worker, you may want to choose the following cluster initialization nprocs = 5 nthreads = 3 ram_gb_proc = 2.4  Rules of Thumb For a job with aggregation (data transfer) choose a setup with a minimum of 10x the data size. Worker memory usage is about 10% initially. If at any point any worker’s memory usage exceeds 75%, the job is very likely to fail (see Worker freeze policies). For a job that consists of only parallel-friendly operations (no sorting, shuffling, or moving large chunks of data), use more CPU (for example 16_16, 32_32). Otherwise use more memory (for example 16_128). For a job that requires both huge data and a large number of tasks, split it into multiple jobs to avoid errors. For example, an optimized setting for 1TB group-by job is to split into 10 pieces (100GB each) and use 24 of 32_32 nodes. You can further convert the piece indicator to a parameter like chunk_id and convert the code into a pipeline job, then run 10 pipeline jobs concurrently to save more time. Use the steps below to estimate how many nodes and workers you will need: Check data size (uncompressed). For example, 100GBChoose operation type to find a multiplier of memory: light (x4), medium (x8), heavy(x48). Multiply your data size from step 1 by this multiplier. For example, a group-by will be medium, which requires 100G x 8 ~ 800GB total memoryUse the number of tasks (heavy vs. light) to determine the number of nodes. If the sequence of operations has many tasks, (computationally heavy), use 32_32. Otherwise use 16_128. Multiply your required memory from step 2 by 32 or 128 depending on computation load. For example, 800GB/32GB = 25 nodes, or 800GB/128GB = 8 of 16_128 nodes.  At this point, you should have an approximate Dask pool spec and number of workers. Add-on step: Setup automatic retry if in pipeline mode. Sometimes pipelines error out when spinning up nodes, or HTTP error, canceled error. These can be fixed by retrying. Worker freeze policies​ Below are the defaults for worker memory limits and actions to avoid memory blowup. distributed: worker: memory: target: 0.60 # target fraction to stay below spill: 0.70 # fraction at which we spill to disk pause: 0.80 # fraction at which we pause worker threads terminate: 0.95 # fraction at which we terminate the worker  ","version":"Next","tagName":"h3"},{"title":"Examples​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#examples","content":"Pandas to Dask.dataframe​ Below are some common examples of converting pandas operations to dask-friendly code. Column to_datetime​ # pandas to_datetime import pandas as pd df1['date'] = pd.to_datetime(df1['date'])  # dask to_datetime from dask import dataframe as dd df1['date'] = dd.to_datetime(df1['date'])  Dataframe groupby​ # pandas groupby df_metrics = df_metrics.groupby(pd.Grouper(timeframe='1day', closed='right',label='right') ).agg({'id':pd.Series.nunique, 'num_entries':'sum', 'total_runs':'sum')  # dask is exactly the same df_metrics = df_metrics.groupby(pd.Grouper(timeframe='1day', closed='right',label='right') ).agg({'id':pd.Series.nunique, 'num_entries':'sum', 'total_runs':'sum')  Get dummies​ The following example features a more complicated groupby; get_dummies will explode in dimension with large amount of data (i.e. possibly explode the data into many columns) # pandas version do dummies first then groupby time interval to get aggregation per time interval dfr= pd.get_dummies(df,['col_a','col_b','col_c']) dfr = dfr.merge(df['date'],right_index=True,left_index=True) dfr = dfr.sort_values(by='date') dfr = dfr.groupby(pd.Grouper(timeframe='1day',closed='right',label='right')).sum()  # dask version do value counts instead of getting dummies, and do pivot after groupby def agg_func(df: pd.DataFrame, timeframe: str, ts_col: str, sec_id: str, target_col:str) -&gt; pd.DataFrame: &quot;&quot;&quot; function that group data by required timeframe for one target column df: dataframe to be aggregated timeframe: aggregation timeframe ts_col: column name of the index sec_id: column name of a secondary id target_col: target column name for processing e.g. col_a &quot;&quot;&quot; df = df.groupby([sec_id, ts_col,target_col]).size().reset_index().set_index(ts_col) df.columns = [sec_id, target_col,'count'] df_agg = df.groupby(pd.Grouper(timeframe=timeframe, closed='right', label='right')).apply( lambda x: x.groupby(target_col).agg({'count': 'sum'})).reset_index() return df_agg meta_df = agg_func(df.head(10), timeframe, ts_col, sec_id, target_col).dtypes.to_dict() df = df.map_partitions(agg_func, timeframe, ts_col, sec_id, target_col, meta = meta_df) df.columns = [ts_col, target_col, 'count'] # further groupby session_ts and event as there will be duplicates among partition df = df.groupby([ts_col, target_col]).agg({'count': 'sum'}) # create pivot table for end results df = df.reset_index() df = df.pivot_table(values=&quot;count&quot;, index=ts_col, columns=target_col) df.columns = [target_col+'_'+i for i in list(df.columns)]  Dask map​ The following example optimizes a function that reads a list of files one by one. ## this snippet reads a list of files one by one import xarray as xr import gcsfs fs = gcsfs.GCSFileSystem(project='myproject', token=None) files_list = ['file1', 'file2', 'file3', 'file4'] gcsmap = gcsfs.mapping.GCSMap(f'gs://my-bucket/{files_list[0]}', gcs=fs) Glob = xr.open_zarr(store=gcsmap).load() ## add other datasets sequentially for filepath in files_list[1:]: gcsmap = gcsfs.mapping.GCSMap(f'gs://my-bucket/{filepath}', gcs=fs) ds = xr.open_zarr(store=gcsmap).load() Glob = xr.merge([Glob, ds], compat=&quot;no_conflicts&quot;, combine_attrs = &quot;no_conflicts&quot;)  ## in dask, create a function to read one file, then use client.map the function and list of files import xarray as xr import gcsfs fs = gcsfs.GCSFileSystem(project='myproject', token=None) files_list = ['file1', 'file2', 'file3', 'file4'] def read_files(gsfilepath): gcsmap = gcsfs.mapping.GCSMap(f&quot;gs://my-bucket/{gsfilepath}&quot;, gcs=fs) ds = xr.open_zarr(store=gcsmap).load().persist() return ds dss = client.map(read_files, files_list) ds_list = client.gather(dss) print(len(ds_list)) # output: 4 Glob = xr.merge(ds_list, compat=&quot;no_conflicts&quot;, combine_attrs = &quot;no_conflicts&quot;)  Parallel training and preprocessing on dask​ Sklearn training can be easily converted to distributed training with dask using joblib. import joblib with joblib.parallel_backend('dask'): grid_search.fit(X, y)  Many sklearn preprocessing modules (e.g. OneHotEncoder, Categorize, StandardScaler, etc.), models (NaiveBayes, xgboost, clustering, etc.), and model selection utilities (KFold, train_test_split, etc.) have dask equivalents. See https://ml.dask.org/index.html for full list of equivalents. ","version":"Next","tagName":"h3"},{"title":"Ray​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#ray","content":"Ray is an distributed frameworks open source project that has a more support for deep learning and reinforcement learning. It has a rich set of libraries and integrations built on a flexible distributed execution framework, is ideal choice for parallelizing model training and hyper-parameter tuning. ","version":"Next","tagName":"h2"},{"title":"Spark​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#spark","content":"Apache Spark is an open source platform for large-scale SQL, batch processing, stream processing, and machine learning. PySpark is the python API for Spark and in the recent releases PySpark adopted more Pandas like APIs. Spark is great for data processing especially for the computations that involves shuffling joining type of operations. Shakudo Platform provides simple APIs to use Spark on distributed Ray clusters using RayDP. RayDP combines your Spark and Ray clusters, making it easy to do large scale data processing using the PySpark API and seamlessly use that data to train your models using TensorFlow and PyTorch. ","version":"Next","tagName":"h2"},{"title":"Initializing a distributed Ray cluster for Spark​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#initializing-a-distributed-ray-cluster-for-spark","content":"Initialize a distributed Ray cluster as usual using the following: from hyperplane.ray_common import initialize_ray_cluster ray_cluster = initialize_ray_cluster( num_workers = 4, cpu_core_per_worker = 15, ram_gb_per_worker = 12 )  num_workers (int) is the number of Ray nodes to be initialized cpu_core_per_worker (int) is the number of CPU cores in each Ray node ram_gb_per_worker (float) is the memory size in GB for each Ray node Read more about Ray and Ray on Shakudo Platform. ","version":"Next","tagName":"h3"},{"title":"Start a Spark session​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#start-a-spark-session","content":"spark = raydp.init_spark( 'example', num_executors=2, executor_cores=4, executor_memory='4G' )  ","version":"Next","tagName":"h3"},{"title":"Use PySpark​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#use-pyspark","content":"Once the Spark session is initialized, you can use pyspark as ususal from here on. The latest RayDP supports PySpark 3.2.0+, which provides simple Pandas-like APIs. import pyspark.pandas as pd df = pd.read_csv(&quot;data.csv&quot;)  ","version":"Next","tagName":"h3"},{"title":"Shutdown a Ray cluster​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#shutdown-a-ray-cluster","content":"After computation is finished, it's a good idea to shutdown the distributed cluster and release the resources back to the node pool. If any Ray nodes are left hanging, Shakudo Platform's garbage collection function will also automatically shutdown the Ray workers when the Jupyter session or job is finished. You can add this line to the end of your code to shutdown the Ray nodes. stop_ray_cluster(ray_cluster)  ","version":"Next","tagName":"h3"},{"title":"RAPIDS​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#rapids","content":"Rapids is a suite of open source libraries and APIs for doing data science on GPUs. Rapids can speed up common computation by 50x and has similar APIs to Pandas, NumPy and Scikit-learn and support multi-GPU scale up. They are very useful in significantly speed up long-running preprocessing loads. ","version":"Next","tagName":"h2"},{"title":"Get started​","type":1,"pageTitle":"Overview","url":"/Shakudo-stack/distributedComputing/overview#get-started","content":"Start by spinning up a Session with the GPU session type. Initialize cluster with GPUs On the Sessions GPU image, you can scale up a Dask cluster with GPUs by adding ngpus=1 to the cluster initialization. client, cluster = nc.initialize_cluster( nprocs=1, nthreads=8, ram_gb_per_proc=7, cores_per_worker=2, num_workers = 2, ngpus = 1, scheduler_deploy_mode=&quot;local&quot; )  Once the Dask cluster is spun up use the RAPIDS library by import importing relevant packages. For example dask_cudf df = dask_cudf.read_csv(file_path, assume_missing=True)  ","version":"Next","tagName":"h3"},{"title":"Ray","type":0,"sectionRef":"#","url":"/Shakudo-stack/distributedComputing/ray","content":"","keywords":"","version":"Next"},{"title":"Ray_Common​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#ray_common","content":"ray_common is part of the Shakudo Platform Hyperplane API that contains convenience functions to manage Ray clusters. We support extensions to the basic Ray framework by supporting Ray Tune, Ray Spark, Ray with RAPIDS, and more. ","version":"Next","tagName":"h2"},{"title":"quickstart_ray()​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#quickstart_ray","content":"Use quickstart_ray to quickly spin up a Ray cluster using t-shirt sizes (Sizes are the same as quick start for Dask clusters). from hyperplane import ray_common as rc ray_cluster = rc.quickstart_ray( num_workers = 4, size = 'hyperplane-med-high-mem' )  Parameters Name\tType\tDescriptionnum_workers Required integer\tNumber of workers size Required object\tPre-configured worker pools   ","version":"Next","tagName":"h2"},{"title":"initialize_ray_cluster()​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#initialize_ray_cluster","content":"Initialize a distributed Ray cluster with ease and more customizability. You can also run this function to clean up the Ray nodes and re-initialize. from hyperplane import ray_common as rc ray_cluster = rc.initialize_ray_cluster( num_workers = 4, cpu_core_per_worker = 4, ram_gb_per_worker = 4, n_gpus = 0 )   Parameters  Name\tType\tDescriptionnum_workers\tinteger\t(Default value: 2) Number of Ray nodes to be initialized cpu_core_per_worker\tinteger\tNumber of CPU cores in each Ray node ram_gb_per_worker\tfloat\tMemory size in GB for each Ray node n_gpus\tinteger\tNumber of Nvidia GPUs in each Ray node (if n_gpus &gt; 0, cpu_core_per_worker and ram_gb_per_worker are ignored) use_existing\tboolean\t(Default: use_existing = False) Whether to connect to/ reinitialize existing Ray cluster or spin up a new one note If you are aiming for a specific pool, ensure your cpu_core_per_worker = the number of allocatable cores and ram_gb_per_worker = the allocatable ram. For example, if you would like to use a POOL_16_16 worker, you may want to use the following cluster initialization. from hyperplane import ray_common as rc ray_cluster = rc.initialize_ray_cluster( num_workers = 4, cpu_core_per_worker = 15, ram_gb_per_worker = 12 )   ","version":"Next","tagName":"h2"},{"title":"stop_ray_cluster()​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#stop_ray_cluster","content":"Use stop_ray_cluster to shutdown a Ray cluster. After computation is finished, it's a good idea to shutdown the distributed cluster and release the resources back to the node pool. If any Ray nodes are left hanging, Shakudo Platform's garbage collection function will also automatically shutdown the Ray workers when the Session or job is finished. from hyperplane import ray_common as rc rc.stop_ray_cluster(ray_cluster)   Parameters  Name\tType\tDescriptionray_cluster Required object\tRay cluster to shutdown  ","version":"Next","tagName":"h2"},{"title":"get_ray_cluster()​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#get_ray_cluster","content":"Reconnect to a Ray cluster by using the get_ray_cluster to retrieve the cluster. You can use this function if you've already spun up a Ray cluster and want to connect to the same cluster (for example: in another notebook in the same session). This function will connect to an existing cluster. There are two ways to reconnect to Ray clusters. from hyperplane import ray_common as rc rc.get_ray_cluster(extra_workers = 1)   Parameters  Name\tType\tDescriptionextra_workers\tinteger\tAdds nodes to your existing cluster (Default: extra_workers = 0) The nodes that are added to the cluster will be of the same specification as the original cluster. There are two ways to reconnect to Ray clusters. The method using the function get_ray_cluster() is the simpler and recommended way. You can also use the initialize_ray_cluster() to accomplish the same. Note, the arguments for cpu_core_per_worker and ram_gb_per_worker must be the same as when you initialized the cluster originally. from hyperplane import ray_common as rc ray_cluster = rc.initialize_ray_cluster( num_workers = 0, cpu_core_per_worker = 15, ram_gb_per_worker = 12, use_existing = True )   ","version":"Next","tagName":"h2"},{"title":"find_ray_workers​","type":1,"pageTitle":"Ray","url":"/Shakudo-stack/distributedComputing/ray#find_ray_workers","content":"Use find_ray_workers() function to see if there are any Ray workers already spun up. Returns a list of Ray workers running. from hyperplane import ray_common as rc rc.find_ray_workers()  ","version":"Next","tagName":"h2"},{"title":"Apache Spark","type":0,"sectionRef":"#","url":"/Shakudo-stack/distributedComputing/spark","content":"Apache Spark Coming soon","keywords":"","version":"Next"},{"title":"FastAPI","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelServing/fastapi","content":"FastAPI Coming soon","keywords":"","version":"Next"},{"title":"Flask","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelServing/flask","content":"Flask Coming soon","keywords":"","version":"Next"},{"title":"TensorFlow Serving","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelServing/tensorflowserving","content":"TensorFlow Serving Coming soon","keywords":"","version":"Next"},{"title":"TorchServe","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelServing/torchserve","content":"TorchServe Coming soon","keywords":"","version":"Next"},{"title":"NVIDIA Triton","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelServing/triton","content":"NVIDIA Triton The Shakudo Platform comes with a build-in NVIDIA Triton Inference Server that simplifies the deployment of AI models at scale in production. Triton is an open-source inference serving software that lets teams deploy trained AI models from any framework (TensorFlow, NVIDIA® TensorRT®, PyTorch, ONNX Runtime, or custom) from local storage or cloud platform on any GPU- or CPU-based infrastructure (cloud, data center, or edge). To serve your model with the Triton server, you need to upload your model to the triton server model repository and write a client file. The default path of the triton model repository is {your_cloud_bucket}/triton-server/model-repository/. The official Triton client examples will help you with different client files for popular machine learning tasks such as image recognition and NLP. Please find a simple App in the Shakudo example repository that serves an image recognition model.","keywords":"","version":"Next"},{"title":"MLFlow","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelTracking/mlflow","content":"MLFlow Coming soon","keywords":"","version":"Next"},{"title":"Weights and Biases","type":0,"sectionRef":"#","url":"/Shakudo-stack/modelTracking/w&b","content":"Weights and Biases Coming soon","keywords":"","version":"Next"},{"title":"Grafana","type":0,"sectionRef":"#","url":"/Shakudo-stack/monitoring/grafana","content":"Grafana Coming soon","keywords":"","version":"Next"},{"title":"Slack Alerts","type":0,"sectionRef":"#","url":"/Shakudo-stack/monitoring/slack","content":"Slack Alerts Slack alerts are available for failed or timeout jobs. Slack messages are sent when all retries have failed. To get Slack alerts set up you must contact our customer success team. Many teams set up a dedicated Slack channel to send alerts to. Wherever you would like the alerts sent, once you have the channel setup follow the steps below (for more detailed guide by Slack on getting a webhook URL check this page) Create a new Slack app in the workspace you want to the Shakudo Platform messages to be postedFrom Features page, toggle on Activate Incoming WebhooksClick Add New Webhook to WorkspacePick a channel that you would like the notifications in, then click AuthorizeSend the Shakudo customer success team your Slack Incoming Webhook URL to start getting job failure notifications. The Shakudo Slack bot will post a message for any errored or timed out jobs. The message will include: Job nameJob IDTimestamp of failure or timeoutReason for failureLink to Grafana logs for the jobA snippet of the specific error message A preview sample message:","keywords":"","version":"Next"},{"title":"Milvus","type":0,"sectionRef":"#","url":"/Shakudo-stack/vectorStores/milvus","content":"","keywords":"","version":"Next"},{"title":"Connections​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#connections","content":"A milvus connection must be established before further operations can be performed. The alias names the connection for future reference. Functions that use the connection will typically have a using parameter with a default value of 'default', so opening a connection with alias='default' allows us to operate other pymilvus facilities while omitting the connection name. ","version":"Next","tagName":"h2"},{"title":"Creating a connection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#creating-a-connection","content":"Connections are created with the pymilvus connections module. from pymilvus import connections connections.connect( alias=&quot;default&quot;, host=os.environ['MILVUS_HOST'], port=os.environ['MILVUS_PORT'] )  For more details on the connection parameters, see the official pymilvus documentation ","version":"Next","tagName":"h3"},{"title":"Closing a connection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#closing-a-connection","content":"Since there is no management object for Milvus connections, they must be released explicitly through the connections manager: connections.disconnect(&quot;default&quot;)  ","version":"Next","tagName":"h3"},{"title":"Databases​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#databases","content":"It is optionally possible to create Databases, which allows setting user permissions ranging over a set of collections. Details on database management are available in the milvus documentation ","version":"Next","tagName":"h2"},{"title":"Collections​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#collections","content":"Milvus data is stored in collections, which have to be loaded in memory before they can be searched against. Loading is not necessary when filling the collection, however. ","version":"Next","tagName":"h2"},{"title":"Creating a Collection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#creating-a-collection","content":"Milvus can operate in schema or schemaless mode depending on how the collection is created. Simply set allow_dynamic_fields=True to enable schemaless. from pymilvus import Collection, FieldSchema, CollectionSchema, DataType MAX_TITLE = 512 MAX_TEXT = 1024 MAX_VEC = 384 NAME = &quot;WikiHow&quot; whschema = CollectionSchema( fields=[ FieldSchema(name=&quot;pk&quot;, dtype=DataType.INT64, is_primary=True, auto_id=True), FieldSchema(name=&quot;title&quot;, dtype=DataType.VARCHAR, max_length=65535, default_value=&quot;&quot;), FieldSchema(name=&quot;text&quot;, dtype=DataType.VARCHAR, max_length=65535, default_value=&quot;&quot;), FieldSchema(name=&quot;vector&quot;, dtype=DataType.FLOAT_VECTOR, dim=384, description=&quot;embedding vector&quot;) ], enable_dynamic_fields=False, description=&quot;WikiHow collection&quot; ) whcollection = Collection( name=NAME, schema=whschema, consistency_level=&quot;Session&quot; )  As for connections, the official pymilvus documentation provides more extensive details. Note that the field size limits are in bytes and depend on the encoding used in milvus, it is not based on character count for VARCHAR. The list of available datatypes is available here. Importantly, the primary key may be either INT64 or VARCHAR and vectors can be either FLOAT_VECTOR or BINARY_VECTOR. The consistency level of the collection is discussed further in the Consistency article at milvus.io. Briefly, consistency_level=&quot;Session&quot; is a good default which means that queries will always happen after reads in our current session, even though they could happen before writes from other sessions are actualized. By comparison, Strong consistency ensures queries will always happen after all writes are completed. Eventually is the weakest consistency level and will process reads immediately, against whatever values are available in the replica at the time. ","version":"Next","tagName":"h3"},{"title":"Inserting data in a collection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#inserting-data-in-a-collection","content":"Given a collection, its insert function can be used to insert a batch of data. If using an auto_id key, the key field should be elided from the input. The argument to insert is a list of lists of field values, positionally ordered as in the schema, such as the following example: def insert_data(data): vecs = embed_documents([d['title'] for d in data]) entries = [[], [], []] for i in range(len(data)): entries[0].append(data[i]['title']) entries[1].append(data[i]['text']) entries[2].append(vecs[i]) whcollection.insert(entries)  Note that if a Milvus worker crashes (e.g. OOM) during operations, although Milvus features redundancy and a second node will come online to keep smooth operations, the default timeout value (in the insert function) may be too low and may cause failure. Increasing it to a much larger value will allow the process to keep running across a worker crash. Milvus will not finalize an insertion (i.e. &quot;seal a segment&quot;) unless enough data has been inserted since the last sealed segment. To force Milvus to seal a segment, it is important to flush the collection: whcollection.flush()  ","version":"Next","tagName":"h3"},{"title":"Creating an index​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#creating-an-index","content":"Bruteforce searches against vectros in the database can be very slow. Setting up an index can drastically speed up the search. whcollection.create_index(field_name=&quot;vector&quot;, index_params={&quot;metric_type&quot;: &quot;L2&quot;, &quot;index_type&quot;: &quot;IVF_FLAT&quot;, &quot;nlist&quot;: &quot;1024&quot;})  In the above example, we have created an index on the field named vector with a flat index using an inverted file, a maximum of 1024 clusters, and an L2 metric. More details about how to parameterize index creation can be found at this link Milvus also supports creating indexes on (and searching against) scalar data (possibly in combination with the vector search). ","version":"Next","tagName":"h3"},{"title":"Referring to an existing collection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#referring-to-an-existing-collection","content":"An existing collection on the 'default' connection can be loaded with a simple NAME = &quot;WikiHow&quot; whcollection = Collection(NAME)  ","version":"Next","tagName":"h3"},{"title":"Loading a collection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#loading-a-collection","content":"Collections cannot be queried against unless they are loaded first. This is simply achieved as follows: whcollection.load()  ","version":"Next","tagName":"h3"},{"title":"Releasing a collection​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#releasing-a-collection","content":"The collection will stay loaded until it is released, either programmatically or through Attu. whcollection.release()  ","version":"Next","tagName":"h3"},{"title":"Search​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#search","content":"Search is conducted on a loaded collection as follows: hits = whcollection.search( [embed_query(what)], # Vector for the query anns_field=&quot;vector&quot;, # Name of the field to search against param={'metric_type': 'L2', # Search params... 'offset': 0, 'params': {'nprobe': 1} }, limit=1, output_fields=['text', 'title']) # Fields to retrieve  The full documentation for the search parameters can be found here. In the above example, we obtain the top search result provided the embeddings for the data to search for. Milvus also supports filter expressions (discribed in the documentation). The param field relates to the index(es) defined on the collection. A consistency_level can also be specified for the query. The hits returned by a Milvus search contains a list of hits as specified by the search parameters for each input vector. Since we provided a single input vector in this case, we can obtain more details about the hits corresponding to this vector as follows: query_hits = hits[0] top_query_hit = query_hits[0] print(f&quot;Title: {top_query_hit.entity.get('title')}&quot;) print(f&quot;Text: {top_query_hit.entity.get('text')}&quot;) print(f&quot;Distance between query embedding and document embedding: {top_query_hit.distance}&quot;)  Since we specified that we only wanted the top hit, we only need to care about the first (i.e. only) hit returned for the first (once again, only) input vector in our search. In the above, we print out the fields retrieved from the search as specified in output_fields in our call, and the distance between the embedding we used to search the database and the document's embedding. ","version":"Next","tagName":"h2"},{"title":"Query​","type":1,"pageTitle":"Milvus","url":"/Shakudo-stack/vectorStores/milvus#query","content":"Milvus can also do scalar searches, termed &quot;query&quot;. For details, see the Milvus documentation on Query ","version":"Next","tagName":"h2"},{"title":"Create a React App","type":0,"sectionRef":"#","url":"/tutorials/buildareactapp","content":"","keywords":"","version":"Next"},{"title":"1. Prepare your environment​","type":1,"pageTitle":"Create a React App","url":"/tutorials/buildareactapp#1-prepare-your-environment","content":"Start a Session with the NodeJs imageOpen your sessions and check your node version using: node —versionInstall npx using npm install npxCreate a React app using npx create-react-app my-react-app ","version":"Next","tagName":"h2"},{"title":"2. Create a pipeline YAML​","type":1,"pageTitle":"Create a React App","url":"/tutorials/buildareactapp#2-create-a-pipeline-yaml","content":"Create a .yaml file to list your steps. You can start with adding the following template to react_pipeline.yaml: pipeline: name: &quot;Example pipeline&quot; tasks: - ...  Add a Bash script step by adding the following block to your YAML. The Bash script should be runnable with bash [bash_script_path]. This will install the dependencies and start your Node server:  - name: &quot;[another_step_name]&quot; type: &quot;bash script&quot; bash_script_path: &quot;[sh/file/relative/to/top/level/of/repo.sh]&quot;  An example bash script to start your React application is: PROJECT_DIR=&quot;$(cd -P &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot; cd &quot;$PROJECT_DIR&quot; yes Y | curl -sL https://deb.nodesource.com/setup_14.x | bash - apt update apt install nodejs npm install npm start  ","version":"Next","tagName":"h2"},{"title":"3. Edit your package.json to host your React app​","type":1,"pageTitle":"Create a React App","url":"/tutorials/buildareactapp#3-edit-your-packagejson-to-host-your-react-app","content":"When you want to host a React application on the Shakudo platform you have to specify the homepage of your application. This will be the root domain plus the the prefix which will host your React application. You should also set your host as 0.0.0.0 and the port as 8787 on your start script, and turn of VS Rewrite. An example package.json file: { &quot;name&quot;: &quot;my-react-app&quot;, &quot;version&quot;: &quot;0.1.0&quot;, &quot;private&quot;: true, &quot;homepage&quot;: &quot;https://[CLUSTER_NAME].hyperplane.dev/[my-app]/&quot;, ... &quot;scripts&quot;: { &quot;start&quot;: &quot;HOST=0.0.0.0 PORT=8787 react-scripts start&quot;, &quot;build&quot;: &quot;react-scripts build&quot;, &quot;test&quot;: &quot;react-scripts test&quot;, &quot;eject&quot;: &quot;react-scripts eject&quot; }, ... }  note The [my-app] prefix should match the dashboard URL prefix when creating your service in step 4 ","version":"Next","tagName":"h2"},{"title":"4. Create your React App on the dashboard​","type":1,"pageTitle":"Create a React App","url":"/tutorials/buildareactapp#4-create-your-react-app-on-the-dashboard","content":"Use the Services tab on the dashboard to start your React application Set the pipeline YAML path and the endpoint for your React app. The endpoint should be the same as the one specified in previous step. Also turn off the Virtual Service Path Rewrite. Then click Create on the top right corner. You React App should appear in the services tab in the dashboard. Important notes when cloning other git repositories Exclude the node_modules folder when committing your code to your Shakudo repositoryRemove the package-lock.json to avoid mismatching packages.Ensure that your bash script can start your React app when your run it on your session.  You can use our GraphQL mutation query in the GraphQL playground to create your React App Service. Copy the GraphQL mutation created on the left handside of the service creation dialogue or simply copy the code block below. mutation { createShakudoService( jobName: &quot;my-react-app&quot;, maxRetries: 2, urlPrefix: &quot;my-app&quot;, jobType: &quot;basic&quot;, pipelineYamlPath: &quot;my-react-app/react_pipeline.yaml&quot;, defaultCommands: true, gitInit: true, vsRewrite: false, parameters: { create: [ ]} ) { id jobName dashboardPrefix parameters { key value } noGitInit noHyperplaneCommands } }  Open the GraphQL playground from the dashboard and paste the code above into the lefthand side and press the play button. Note the the urlPrefix should match the dashboard URL prefix set in your package.json file as the homepage of your application. ","version":"Next","tagName":"h2"},{"title":"GraphQL","type":0,"sectionRef":"#","url":"/shakudo-platform-core/graphql","content":"","keywords":"","version":"Next"},{"title":"Get Sessions​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#get-sessions","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description","content":"Retrieves a list of Sessions query hyperhubSessions($limit: Int!, $email: String, $status: String, $imageType: String) { hyperHubSessions(orderBy:{startTime: desc}, take: $limit, where: { hyperplaneUserEmail: {equals: $email}, imageType: {equals: $imageType}, status: {equals: $status}, }) { id hyperplaneUserEmail status imageType jLabUrl notebookURI estimatedCost resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime } countHyperHubSessions }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables","content":"{ &quot;limit&quot;: 10, &quot;email&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;in progress&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters","content":"Field\tType\tDescriptionlimit\tInt!\tThe maximum number of records to show in the result. (required) email\tString\tShakudo platform user email for the user who created the session imageType\tString\tName of the Shakudo platform EC. For example, &quot;basic&quot; status\tString\tUnderlying Kubernetes job status ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type","content":"Array of HyperHubSessions ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response","content":"{ &quot;data&quot;: { &quot;hyperHubSessions&quot;: [ { &quot;id&quot;: &quot;49475b67-3f8f-43c1-9f42-7b2175d1e679&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;in progress&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;jLabUrl&quot;: &quot;client.hyperplane.dev/jupyterlabUrl/&quot;, &quot;notebookURI&quot;: &quot;ssh demo-pvc-entry@demo.dev&quot;, &quot;estimatedCost&quot;: null, &quot;resourceCPUlimit&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;gpuRequest&quot;: null, &quot;startTime&quot;: &quot;2023-07-05T16:25:45.676Z&quot; } ], &quot;countHyperHubSessions&quot;: 22 } }  ","version":"Next","tagName":"h3"},{"title":"Create Session​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#create-session","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-1","content":"Creates a Session ","version":"Next","tagName":"h3"},{"title":"Creating using createHyperHubSession parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#creating-using-createhyperhubsession-parameters","content":" query GetHyperplaneUserId($hyperplaneUserEmail: String!){ hyperplaneUsers(where: {email: {equals: $hyperplaneUserEmail}}) { id email } } # billingProjectName optional query GetBillingProjectId($billingProjectName: String){ billingProjects(where: {name: {equals: $billingProjectName}}) { id name } } # userPvcName and displayName optional query GetUserPvcId($userPvcName: String, $displayName: String){ userPvcs(where: { pvcName: {equals: $userPvcName}, displayName: {equals: $displayName} }) { id pvcName displayName } } mutation createSession( $imageType: String! $hyperplaneUserId: String! $hyperplaneUserEmail: String! $timeout: Int! $collaborative: Boolean! $imageHash: String! $userPvcName: String = &quot;&quot; $userPvc: UserPvcCreateNestedOneWithoutHyperHubSessionInput $billingProjectId: String! ) { createHyperHubSession( data: { imageType: $imageType timeout: $timeout collaborative: $collaborative imageHash: $imageHash group: &quot;&quot; hyperplaneUser: { connect: { id: $hyperplaneUserId } } billingProject: { connect: { id: $billingProjectId } } userPvc: $userPvc userPvcName: $userPvcName hyperplaneUserEmail: $hyperplaneUserEmail } ) { id hyperplaneUserEmail status imageType jLabUrl estimatedCost resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime completionTime timeout group billingProjectId podSpec } }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-1","content":"Retrieve $hyperplaneUserId using GetHyperplaneUserId and $billingProjectId using GetBillingProjectId Default Drive { &quot;collaborative&quot;: false, &quot;imageType&quot;: &quot;basic&quot;, &quot;imageHash&quot;: &quot;&quot;, &quot;timeout&quot;: 900, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;hyperplaneUserId&quot;: &quot;93c6c00a-14b7-4cf7-845d-70d9e779b2cd&quot;, # From GetHyperplaneUserId &quot;billingProjectId&quot;: &quot;8359f1f9-2eca-465b-9ac5-7cdb0e97e73f&quot; # From GetBillingProjectId }  Custom Drive { &quot;collaborative&quot;: false, &quot;imageType&quot;: &quot;basic&quot;, &quot;imageHash&quot;: &quot;&quot;, &quot;timeout&quot;: 900, &quot;userPvcName&quot;: &quot;demo-user-pvc-name&quot;, &quot;displayName&quot;: &quot;demo drive&quot;, &quot;hyperplaneUserId&quot;: &quot;93c6c00a-14b7-4cf7-845d-70d9e779b2cd&quot;, # From GetHyperplaneUserId &quot;billingProjectId&quot;: &quot;8359f1f9-2eca-465b-9ac5-7cdb0e97e73f&quot; # From GetBillingProjectId &quot;userPvc&quot;: { &quot;connect&quot;: { &quot;id&quot;: &quot;bb2eeed2-6032-4036-9e8f-e757235533bb&quot; }}, # From GetUserPvcId &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters-1","content":"Field\tType\tDefinitionimageType\tString! (required)\tName of Shakudo platform EC hyperplaneUserId\tString! (required)\tShakudo platform user account ID hyperplaneUserEmail\tString! (required)\tShakudo platform user account email collaborative\tBoolean! (required)\tEnables collaborative mode. Collaborative mode allows multiple users to work together in the same session environment. timeout\tInt! (required)\tThe maximum time in seconds that the pipeline may run, starting from the moment of job submission. Default: -1, ie. never timeout; 86400 on dashboard imageHash\tString! (required)\tURL of custom image, &quot;&quot; if using a default image like basic userPvcName\tString (&quot;&quot; if not provided)\tPersistent volume name as found in Kubernetes. Typically includes the drive name found on the dashboard. Default: &quot;&quot; (empty string) which corresponds with default drive claim-{user-email} userPvc\tUserPvc\tShakudo session persistent volume (drive) details. Can either provide identifiers to connect to an existing drive or can provide values to create a new drive. Default: not present, which corresponds with default drive claim-{user-email}. Note: userPvc ID must correspond with same userPvc as userPvcName. displayName\tString\tDrive (PVC) display name as visible on UI dashboard billingProjectId\tString! (required)\tID for billing project that user would like Session costs to contribute. Can either provide identifiers to connect to an existing billing project or can provide values to create a new billing project. Can get from GetBillingProjectId. billingProjectName\tString\tName of billing project as shown on UI dashboard ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-1","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-1","content":"{ &quot;data&quot;: { &quot;createHyperHubSession&quot;: { &quot;id&quot;: &quot;0b8b90c7-b3d6-43d7-a34a-27a9b17521b4&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;jLabUrl&quot;: null, &quot;estimatedCost&quot;: null, &quot;resourceCPUlimit&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;gpuRequest&quot;: null, &quot;startTime&quot;: &quot;2023-07-06T21:14:29.245Z&quot;, &quot;completionTime&quot;: null, &quot;timeout&quot;: 900, &quot;group&quot;: &quot;&quot;, &quot;billingProjectId&quot;: &quot;bb2eeed2-6032-4036-9e8f-e757235533bb&quot;, &quot;podSpec&quot;: null } } }  ","version":"Next","tagName":"h3"},{"title":"Creating using PodSpec JSON (getHyperhubSessionDefaultPodSpec)​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#creating-using-podspec-json-gethyperhubsessiondefaultpodspec","content":"**Getting PodSpec JSON** query GetHyperhubSessionPodSpec($imageType: String, $userPvcName: String, $userEmail: String!, $imageUrl: String) { getHyperhubSessionPodSpec( imageType: $imageType, userPvcName: $userPvcName, userEmail: $userEmail, imageUrl: $imageUrl ) }  Sample Variables { &quot;imageType&quot;: &quot;basic&quot;, &quot;userEmail&quot;: &quot;demo@shakudo.io&quot; }  ****Parameters**** Field\tType\tDescriptionimageUrl\tString\tURL of custom image, same as imageHash userPvcName\tString\tPersistent volume name as found in Kubernetes. Typically includes the drive name found on the dashboard. Default: &quot;&quot; (empty string) which corresponds with default drive claim-{user-email} userEmail\tString! (required)\tShakudo platform user account email imageType\tString\tName of Shakudo platform Podspec/Image Creating Session with PodSpec JSON query GetHyperplaneUserId($userEmail: String!){ hyperplaneUsers(where: {email: {equals: $userEmail}}) { id email } } mutation CreateSessionWithPodSpecJSON( $userEmail: String! $hyperplaneUserId: String! $userPvcName: String = &quot;&quot; $podSpec: JSON ) { createHyperHubSession( data: { hyperplaneUserEmail: $userEmail, hyperplaneUser: { connect: { id: $hyperplaneUserId } }, userPvcName: $userPvcName podSpec: $podSpec } ) { id hyperplaneUserEmail status imageType jLabUrl estimatedCost resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime completionTime timeout group billingProjectId } }  Sample Variables Note: podSpec field contains result of GetHyperhubSessionPodSpec and the corresponding getHyperHubSessionDefaultPodSpec field in the query’s result object. ie. { &quot;imageType&quot;: &quot;basic&quot;, &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;hyperplaneUserId&quot;: &quot;bb2eeed2-6032-4036-9e8f-e757235533bb&quot;, &quot;podSpec&quot;: &lt;getHyperHubSessionDefaultPodSpec result&gt; }  Parameters Field\tType\tDescriptionuserEmail\tString! (required)\tShakudo platform user account email hyperplaneUserId\tString! (required)\tShakudo platform user account ID podSpec\tJSON\tShakudo platform PodSpec config object as a JSON object, originates from getHyperHubSessionDefaultPodSpec userPvcName\tString (”” if not provided)\tAdded as a parameter to align field in UI ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-2","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-2","content":"{ &quot;data&quot;: { &quot;createHyperHubSession&quot;: { &quot;id&quot;: &quot;bb2eeed2-6032-4036-9e8f-e757235533bb&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;jLabUrl&quot;: null, &quot;estimatedCost&quot;: null, &quot;resourceCPUlimit&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;gpuRequest&quot;: null, &quot;startTime&quot;: &quot;2023-07-05T16:26:06.346Z&quot;, &quot;completionTime&quot;: null, &quot;timeout&quot;: -1, &quot;group&quot;: null, &quot;billingProjectId&quot;: null } } }  ","version":"Next","tagName":"h3"},{"title":"Stop Session​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#stop-session","content":" mutation stopSession($id: String!) { updateHyperHubSession(where: {id: $id}, data: { status: {set: &quot;cancelled&quot;} }) { id status } }  ","version":"Next","tagName":"h2"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-2","content":"{ &quot;id&quot;: &quot;9276a796-229f-4ede-a2cf-a7cf329dab6a&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters-2","content":"Field\tType\tDescriptionid\tString! (required)\tSession ID ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-3","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-3","content":"{ &quot;data&quot;: { &quot;updateHyperHubSession&quot;: { &quot;id&quot;: &quot;9276a796-229f-4ede-a2cf-a7cf329dab6a&quot;, &quot;status&quot;: &quot;cancelled&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"Count Sessions​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#count-sessions","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-2","content":"Count the number of sessions based on the filters provided by the parameters. query CountHyperhubSessions($email: String, $imageType: String, $status: String) { countHyperHubSessions(whereOveride: { hyperplaneUserEmail: {equals: $email}, imageType: {equals: $imageType} status: {equals: $status} }) }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-3","content":"{ &quot;email&quot;: &quot;demo@shakudo.io&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;status&quot;: &quot;in progress&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters-3","content":"Field\tType\tDescriptionemail\tString\tShakudo platform user email for the user who created the session imageType\tString\tName of the Shakudo platform Podspec/Image, e.g., &quot;basic&quot; status\tString\tThe status of the Kubernetes job that runs the pipeline job ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-4","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-4","content":"{ &quot;data&quot;: { &quot;countHyperHubSessions&quot;: 1 } }  ","version":"Next","tagName":"h3"},{"title":"Create a Pipeline Job using createPipelineJob Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#create-a-pipeline-job-using-createpipelinejob-parameters","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-3","content":"Creates a Shakudo platform job, which allows users to run task scripts using custom configurations, either immediately as an “Immediate job”, at scheduled intervals as a “Scheduled Job”, or indefinitely as a “Service”. Immediate jobs: schedule = &quot;immediate&quot; Scheduled jobs: schedule != &quot;immediate&quot;, schedule is set to cron schedule expression, eg. * * * * * for a job running every minute Service: timeout and activeTimeout set to -1 , schedule=&quot;immediate&quot; and exposedPort != null, particularly set to a valid port mutation CreatePipelineJob( $type: String! $timeout: Int! $activeTimeout: Int $maxRetries: Int! $yamlPath: String! $exposedPort: String $schedule: String $parameters: ParameterCreateNestedManyWithoutPipelineJobInput $gitServer: HyperplaneVCServerCreateNestedOneWithoutPipelineJobsInput $hyperplaneUserEmail: String! $branchName: String ) { createPipelineJob( data: { jobType: $type timeout: $timeout activeTimeout: $activeTimeout maxRetries: $maxRetries pipelineYamlPath: $yamlPath exposedPort: $exposedPort parameters: $parameters schedule: $schedule hyperplaneVCServer: $gitServer hyperplaneUserEmail: $hyperplaneUserEmail branchName: $branchName } ) { id jobName pipelineYamlPath schedule status statusReason output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries exposedPort jobType parameters { key value } } }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters-4","content":"Field\tType\tDescriptiontype\tString! (required)\tName of Shakudo platform Podspec/Image, default or custom. Example: &quot;basic&quot; timeout\tInt! (required)\tThe maximum time in seconds that the pipeline may run, starting from the moment of job submission. Default: -1 (never timeout). Example: 86400 activeTimeout\tInt\tThe maximum time in seconds that the pipeline may run once it is picked up. Default: -1 (never timeout). Example: 86400 maxRetries\tInt! (required)\tThe maximum number of attempts to run your pipeline job before returning an error, even if timeouts are not reached. Default: 2 yamlPath\tString\tThe relative path to the .yaml file used to run this pipeline job. Example: &quot;example_notebooks/pipelines/python_hello_world_pipeline/pipeline.yaml&quot; exposedPort\tString\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. schedule\tString\tEither &quot;immediate&quot; for an immediate job or a cron schedule expression for a scheduled job at the specified interval. parameters\tParameterCreateNestedManyWithoutPipelineJobInput\tKey-value pairs that can be used within the container environment gitServer\tHyperplaneVCServerCreateNestedOneWithoutPipelineJobsInput\tGit server object, retrievable by searching git servers by name (hyperplaneVCServers) and using resulting id in the following manner: { connect: { id: &lt;gitServerId&gt; } } hyperplaneUserEmail\tString! (required)\tShakudo platform user email branchName\tString\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. If commitID is not specified, the latest commit is used. If not specified, default branch is used. Please note that the exclamation mark ! indicates that the field is required. ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-4","content":"{ &quot;type&quot;: &quot;basic&quot;, &quot;timeout&quot;: 86400, &quot;activeTimeout&quot;: 86400, &quot;maxRetries&quot;: 2, &quot;yamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;main&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-5","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-5","content":"{ &quot;data&quot;: { &quot;createPipelineJob&quot;: { &quot;id&quot;: &quot;7b728979-71b7-426c-9847-6fe3e29a6438&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;schedule&quot;: &quot;immediate&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;output&quot;: null, &quot;startTime&quot;: &quot;2023-06-30T16:03:42.668Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;timeout&quot;: 86400, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: 86400, &quot;maxRetries&quot;: 2, &quot;exposedPort&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;parameters&quot;: [] } } }  ","version":"Next","tagName":"h3"},{"title":"Create a Scheduled Job using createPipelineJob Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#create-a-scheduled-job-using-createpipelinejob-parameters","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-4","content":"Create a scheduled job by specifying a cron schedule. Use the following guide to create a suitable expression for a specific schedule. ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-5","content":"{ &quot;type&quot;: &quot;basic&quot;, &quot;timeout&quot;: 86400, &quot;maxRetries&quot;: 2, &quot;schedule&quot;: &quot;* * * * *&quot;, &quot;yamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;demo&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-6","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-6","content":"{ &quot;data&quot;: { &quot;createPipelineJob&quot;: { &quot;id&quot;: &quot;9276a796-229f-4ede-a2cf-a7cf329dab6a&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;schedule&quot;: &quot;* * * * *&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;output&quot;: null, &quot;startTime&quot;: &quot;2023-06-30T16:03:42.668Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;timeout&quot;: 86400, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: 86400, &quot;maxRetries&quot;: 2, &quot;exposedPort&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;parameters&quot;: [] } } }  ","version":"Next","tagName":"h3"},{"title":"Create a PipelineJob using PodSpec JSON (getPipelineJobPodSpec)​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#create-a-pipelinejob-using-podspec-json-getpipelinejobpodspec","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-5","content":"Create an immediate or scheduled job using a PodSpec JSON object that is customizable. Use the following guide to create a suitable expression for a specific schedule. query GetPipelineJobPodSpec( $parameters: ParametersInput $gitServerName: String = &quot;&quot; $noGitInit: Boolean = false $imageUrl: String = &quot;&quot; $userEmail: String! $noHyperplaneCommands: Boolean = false $commitId: String = &quot;&quot; $branchName: String $pipelineYamlPath: String = &quot;&quot; $debuggable: Boolean = false $jobType: String = &quot;&quot; ) { getPipelineJobPodSpec( parameters: $parameters gitServerName: $gitServerName noGitInit: $noGitInit imageUrl: $imageUrl userEmail: $userEmail noHyperplaneCommands: $noHyperplaneCommands pipelineYamlPath: $pipelineYamlPath commitId: $commitId branchName: $branchName debuggable: $debuggable jobType: $jobType ) }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-6","content":"{ &quot;jobType&quot;: &quot;basic&quot;, &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;branchName&quot;: &quot;demo&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters-5","content":"Field\tType\tDescriptionparameters\tParametersInput\tList of key-value parameters that are injected into the Job environment and can be used as environment variables gitServerName\tString (&quot;&quot; if not provided)\tGit Server name, corresponds with name field in HyperplaneVCServer, which is the display name assigned on the dashboard noGitInit\tBoolean (false if not provided)\tFalse if git server is to be set up using default Shakudo platform workflow. Default: false imageUrl\tString (&quot;&quot; if not provided)\tIf the image is custom, then the image URL can be provided userEmail\tString! (required)\tShakudo platform user account email noHyperplaneCommands\tBoolean\tFalse if using default Shakudo platform commands on job creation. Required to use Shakudo platform jobs through the pipeline YAML, but not required if the image has its own setup. Default: false commitId\tString (&quot;&quot; if not provided)\tThe commit ID with the versions of the pipeline YAML file and pipeline scripts wanted. Ensure that both are present if the commit ID is used. If left empty, assume that the latest commit on the branch is used branchName\tString\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. If commitID is not specified, the latest commit is used. If not specified, default branch is used. pipelineYamlPath\tString (&quot;&quot; if not provided)\tThe relative path to the .yaml file used to run this pipeline job debuggable\tBoolean (false if not provided)\tWhether to enable SSH-based debugging for the job, check the following tutorial for more details jobType\tString (&quot;&quot; if not provided)\tName of Shakudo platform Podspec/Image, default or custom mutation CreatePipelineJob( $jobName: String $pipelineYamlPath: String! $podSpec: JSON! $schedule: String $userEmail: String! ) { createPipelineJob (data: { jobName: $jobName pipelineYamlPath: $pipelineYamlPath podSpec: $podSpec schedule: $schedule hyperplaneUserEmail: $userEmail } ) { id jobName pipelineYamlPath schedule status statusReason output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries exposedPort jobType parameters { key value } } }  Field\tType\tDefinitionjobName\tString\tPlain display name of job viewable from the dashboard, not necessarily unique. pipelineYamlPath\tString!\tThe relative path to the .yaml file used to run this pipeline job podSpec\tJSON!\tShakudo platform PodSpec environment config object as JSON schedule\tString\tEither &quot;immediate&quot; for an immediate job or a cron schedule expression for a scheduled job. userEmail\tString!\tShakudo user account email ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-7","content":"podSpec will be the result of GetPipelineJobPodSpec from the field getPipelineJobPodSpec { &quot;jobType&quot;: &quot;basic&quot;, &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;demo&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;jobName&quot;: &quot;test-create-pipeline-job-with-podSpec&quot;, &quot;podSpec&quot;: &lt;GetPipelineJobPodSpec getPipelineJobPodSpec field result&gt; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-7","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-7","content":"{ &quot;data&quot;: { &quot;createPipelineJob&quot;: { &quot;id&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;schedule&quot;: &quot;* * * * *&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;output&quot;: null, &quot;startTime&quot;: &quot;2023-06-30T16:03:42.668Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;timeout&quot;: 86400, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: 86400, &quot;maxRetries&quot;: 2, &quot;exposedPort&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;parameters&quot;: [] } } }  ","version":"Next","tagName":"h3"},{"title":"Cancel a Pipeline Job​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#cancel-a-pipeline-job","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-6","content":"Cancel a job (Stop job from running). Find PipelineJob by jobName or another non-unique identifier, optional if user has ID query ($jobName: String) { pipelineJobs(where: {jobName: {equals: $jobName} }) { id pipelineYamlPath schedule status statusReason startTime completionTime timeout outputNotebooksPath activeTimeout jobType parameters { key value } } }  Sample Variables { &quot;jobName&quot;: &quot;foo&quot; }  Parameters Field\tType\tDescriptionjobName\tString\tPlain display name of job viewable from the dashboard, not necessarily unique. Use PipelineJob ID to cancel the job mutation ($id: String!) { updatePipelineJob(where: {id: $id}, data: { status: {set: &quot;cancelled&quot;} }) { id } }  Sample Variables { &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; }  Parameters Field\tType\tDescriptionid\tString! (required)\tPipeline Job ID ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-8","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-8","content":"{ &quot;data&quot;: { &quot;updatePipelineJob&quot;: { &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; } **** } }  ","version":"Next","tagName":"h3"},{"title":"Get Job Status​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#get-job-status","content":"query GetPipelineJobStatus($id: String!){ pipelineJob(where: {id: $id }) { status statusReason } }  ","version":"Next","tagName":"h2"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-8","content":"{ &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters-6","content":"Field\tType\tDescriptionid\tString! (required)\tPipeline Job ID ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-9","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-9","content":"{ &quot;data&quot;: { &quot;pipelineJob&quot;: { &quot;status&quot;: &quot;done&quot;, &quot;statusReason&quot;: null } } }  ","version":"Next","tagName":"h3"},{"title":"Get Job Status Statistics​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#get-job-status-statistics","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-7","content":"Count the number of jobs based on their statuses. For example, failed, pending, or cancelled jobs. The timeFrame parameter specifies the timeframe which will be considered. For instance: T_10M = past 10 minutesT_24H = past 24 hours query { COUNT_ALL_TOTAL: getJobStat(stat: COUNT_ALL, timeFrame: TOTAL) COUNT_CANCELLED_TOTAL: getJobStat(stat: COUNT_CANCELLED, timeFrame: TOTAL) COUNT_DONE_TOTAL: getJobStat(stat: COUNT_DONE, timeFrame: TOTAL) COUNT_FAILED_TOTAL: getJobStat(stat: COUNT_FAILED, timeFrame: TOTAL) COUNT_IN_PROGRESS_TOTAL: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: TOTAL ) COUNT_PENDING_TOTAL: getJobStat(stat: COUNT_PENDING, timeFrame: TOTAL) COUNT_SCHEDULED_TOTAL: getJobStat(stat: COUNT_SCHEDULED, timeFrame: TOTAL) COUNT_TIMED_OUT_TOTAL: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: TOTAL) COUNT_ALL_T_10M: getJobStat(stat: COUNT_ALL, timeFrame: T_10M) COUNT_CANCELLED_T_10M: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_10M) COUNT_DONE_T_10M: getJobStat(stat: COUNT_DONE, timeFrame: T_10M) COUNT_FAILED_T_10M: getJobStat(stat: COUNT_FAILED, timeFrame: T_10M) COUNT_IN_PROGRESS_T_10M: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_10M ) COUNT_PENDING_T_10M: getJobStat(stat: COUNT_PENDING, timeFrame: T_10M) COUNT_SCHEDULED_T_10M: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_10M) COUNT_TIMED_OUT_T_10M: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_10M) COUNT_ALL_T_1H: getJobStat(stat: COUNT_ALL, timeFrame: T_1H) COUNT_CANCELLED_T_1H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_1H) COUNT_DONE_T_1H: getJobStat(stat: COUNT_DONE, timeFrame: T_1H) COUNT_FAILED_T_1H: getJobStat(stat: COUNT_FAILED, timeFrame: T_1H) COUNT_IN_PROGRESS_T_1H: getJobStat(stat: COUNT_IN_PROGRESS, timeFrame: T_1H) COUNT_PENDING_T_1H: getJobStat(stat: COUNT_PENDING, timeFrame: T_1H) COUNT_SCHEDULED_T_1H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_1H) COUNT_TIMED_OUT_T_1H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_1H) COUNT_ALL_T_24H: getJobStat(stat: COUNT_ALL, timeFrame: T_24H) COUNT_CANCELLED_T_24H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_24H) COUNT_DONE_T_24H: getJobStat(stat: COUNT_DONE, timeFrame: T_24H) COUNT_FAILED_T_24H: getJobStat(stat: COUNT_FAILED, timeFrame: T_24H) COUNT_IN_PROGRESS_T_24H: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_24H ) COUNT_PENDING_T_24H: getJobStat(stat: COUNT_PENDING, timeFrame: T_24H) COUNT_SCHEDULED_T_24H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_24H) COUNT_TIMED_OUT_T_24H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_24H) }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-9","content":"getJobStat(stat: COUNT_ALL, timeFrame: TOTAL)  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-10","content":"Int ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-10","content":"{ &quot;data&quot;: { &quot;getJobStat&quot;: 105179 } }  ","version":"Next","tagName":"h3"},{"title":"Get Scheduled Jobs Status Statistics​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#get-scheduled-jobs-status-statistics","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-8","content":"Count the number of scheduled jobs based on their statuses, particularly failed, pending, or cancelled jobs. status: SCHEDULED to each getJobStat query to isolated *scheduled* jobs. query { COUNT_ALL_TOTAL: getJobStat(stat: COUNT_ALL, timeFrame: TOTAL, status: SCHEDULED) COUNT_CANCELLED_TOTAL: getJobStat(stat: COUNT_CANCELLED, timeFrame: TOTAL, status: SCHEDULED) COUNT_DONE_TOTAL: getJobStat(stat: COUNT_DONE, timeFrame: TOTAL, status: SCHEDULED) COUNT_FAILED_TOTAL: getJobStat(stat: COUNT_FAILED, timeFrame: TOTAL, status: SCHEDULED) COUNT_IN_PROGRESS_TOTAL: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: TOTAL ) COUNT_PENDING_TOTAL: getJobStat(stat: COUNT_PENDING, timeFrame: TOTAL, status: SCHEDULED) COUNT_SCHEDULED_TOTAL: getJobStat(stat: COUNT_SCHEDULED, timeFrame: TOTAL, status: SCHEDULED) COUNT_TIMED_OUT_TOTAL: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: TOTAL, status: SCHEDULED) COUNT_ALL_T_10M: getJobStat(stat: COUNT_ALL, timeFrame: T_10M) COUNT_CANCELLED_T_10M: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_10M, status: SCHEDULED) COUNT_DONE_T_10M: getJobStat(stat: COUNT_DONE, timeFrame: T_10M, status: SCHEDULED) COUNT_FAILED_T_10M: getJobStat(stat: COUNT_FAILED, timeFrame: T_10M, status: SCHEDULED) COUNT_IN_PROGRESS_T_10M: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_10M ) COUNT_PENDING_T_10M: getJobStat(stat: COUNT_PENDING, timeFrame: T_10M, status: SCHEDULED) COUNT_SCHEDULED_T_10M: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_10M, status: SCHEDULED) COUNT_TIMED_OUT_T_10M: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_10M, status: SCHEDULED) COUNT_ALL_T_1H: getJobStat(stat: COUNT_ALL, timeFrame: T_1H, status: SCHEDULED) COUNT_CANCELLED_T_1H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_1H, status: SCHEDULED) COUNT_DONE_T_1H: getJobStat(stat: COUNT_DONE, timeFrame: T_1H, status: SCHEDULED) COUNT_FAILED_T_1H: getJobStat(stat: COUNT_FAILED, timeFrame: T_1H, status: SCHEDULED) COUNT_IN_PROGRESS_T_1H: getJobStat(stat: COUNT_IN_PROGRESS, timeFrame: T_1H, status: SCHEDULED) COUNT_PENDING_T_1H: getJobStat(stat: COUNT_PENDING, timeFrame: T_1H, status: SCHEDULED) COUNT_SCHEDULED_T_1H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_1H, status: SCHEDULED) COUNT_TIMED_OUT_T_1H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_1H, status: SCHEDULED) COUNT_ALL_T_24H: getJobStat(stat: COUNT_ALL, timeFrame: T_24H) COUNT_CANCELLED_T_24H: getJobStat(stat: COUNT_CANCELLED, timeFrame: T_24H, status: SCHEDULED) COUNT_DONE_T_24H: getJobStat(stat: COUNT_DONE, timeFrame: T_24H, status: SCHEDULED) COUNT_FAILED_T_24H: getJobStat(stat: COUNT_FAILED, timeFrame: T_24H, status: SCHEDULED) COUNT_IN_PROGRESS_T_24H: getJobStat( stat: COUNT_IN_PROGRESS timeFrame: T_24H , status: SCHEDULED ) COUNT_PENDING_T_24H: getJobStat(stat: COUNT_PENDING, timeFrame: T_24H, status: SCHEDULED) COUNT_SCHEDULED_T_24H: getJobStat(stat: COUNT_SCHEDULED, timeFrame: T_24H, status: SCHEDULED) COUNT_TIMED_OUT_T_24H: getJobStat(stat: COUNT_TIMED_OUT, timeFrame: T_24H, status: SCHEDULED) }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-10","content":"getJobStat(stat: COUNT_ALL, timeFrame: TOTAL, status: SCHEDULED)  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-11","content":"Int ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-11","content":"{ &quot;data&quot;: { &quot;getJobStat&quot;: 179 } }  ","version":"Next","tagName":"h3"},{"title":"Create a Service​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#create-a-service","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-9","content":"Services are currently pipeline jobs which have an activeTimeout and timeout of -1, ie. never ending jobs, schedule = &quot;immediate&quot;, and with exposedPort != null mutation CreateService( $type: String! $maxRetries: Int! $yamlPath: String! $jobName: String! = &quot;&quot; $exposedPort: String = &quot;8787&quot; $parameters: ParameterCreateNestedManyWithoutPipelineJobInput $gitServer: HyperplaneVCServerCreateNestedOneWithoutPipelineJobsInput $hyperplaneUserEmail: String! $branchName: String ) { createPipelineJob( data: { jobType: $type, jobName: $jobName, maxRetries: $maxRetries, pipelineYamlPath: $yamlPath, parameters: $parameters, hyperplaneVCServer: $gitServer, hyperplaneUserEmail: $hyperplaneUserEmail, branchName: $branchName, exposedPort: $exposedPort, timeout: -1, activeTimeout: -1, schedule: &quot;immediate&quot; } ) { id pipelineYamlPath schedule status statusReason output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries exposedPort jobType parameters { key value } } }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-11","content":"{ &quot;type&quot;: &quot;basic&quot;, &quot;maxRetries&quot;: 2, &quot;jobName&quot;: &quot;test&quot;, &quot;yamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;demo&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters-7","content":"Field\tType\tDefinitiontype\tString!\tName of Shakudo platform Podspec/Image, default or custom. Example: &quot;basic&quot; timeout\tInt!\tThe maximum time in seconds that the pipeline may run, starting from the moment of job submission. Set to -1 for Services. activeTimeout\tInt\tThe maximum time in seconds that the pipeline may run once it is picked up. Set to -1 for Services. maxRetries\tInt!\tThe maximum number of attempts to run your pipeline job before returning an error, even if timeouts are not reached. Default: 2 yamlPath\tString\tThe relative path to the .yaml file used to run this pipeline job. Example: &quot;example_notebooks/pipelines/python_hello_world_pipeline/pipeline.yaml&quot; exposedPort\tString\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. schedule\tString\tSet to immediate for Services parameters\tParameterCreateNestedManyWithoutPipelineJobInput\tKey-value pairs that can be used within the container environment gitServer\tHyperplaneVCServerCreateNestedOneWithoutPipelineJobsInput\tGit server object, retrievable by searching git servers by name (hyperplaneVCServers) and using resulting id in the following manner: { connect: { id: $gitServerId } } hyperplaneUserEmail\tString!\tShakudo platform user email branchName\tString\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. If commitID is not specified, the latest commit is used. If not specified, default branch is used. ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-12","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-12","content":"{ &quot;data&quot;: { &quot;createPipelineJob&quot;: { &quot;id&quot;: &quot;9276a796-229f-4ede-a2cf-a7cf329dab6a&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;schedule&quot;: &quot;immediate&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;output&quot;: null, &quot;startTime&quot;: &quot;2023-07-06T14:51:35.506Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;timeout&quot;: -1, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: -1, &quot;maxRetries&quot;: 2, &quot;exposedPort&quot;: &quot;8787&quot;, &quot;jobType&quot;: &quot;basic&quot;, &quot;parameters&quot;: [] } } }  ","version":"Next","tagName":"h3"},{"title":"Create a Service using PodSpec JSON (getUserServicePodSpec)​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#create-a-service-using-podspec-json-getuserservicepodspec","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-10","content":"Create a Service using a PodSpec JSON object that is customizable. Retrieve UserServicePodSpec query GetUserServicePodSpec( $exposedPort: String $parameters: ParametersInput $gitServerName: String $noGitInit: Boolean $imageUrl: String $userEmail: String! $noHyperplaneCommands: Boolean $commitId: String $branchName: String! $pipelineYamlPath: String! $jobType: String! ) { getUserServicePodSpec( exposedPort: $exposedPort parameters: $parameters gitServerName: $gitServerName noGitInit: $noGitInit imageUrl: $imageUrl userEmail: $userEmail noHyperplaneCommands: $noHyperplaneCommands commitId: $commitId branchName: $branchName pipelineYamlPath: $pipelineYamlPath jobType: $jobType ) }  Sample Variables { &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;demo&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;jobType&quot;: &quot;basic&quot; }  Parameters Field\tType\tDescriptionexposedPort\tString\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. parameters\tParametersInput\tKey-value pairs that can be used within the container environment. gitServerName\tString\tThe name of the Git server used for the pipeline job. noGitInit\tBoolean\tSpecifies whether the Git server initialization is skipped for the pipeline job. imageUrl\tString\tThe URL of the image used for the pipeline job. userEmail\tString!\tShakudo platform user email for the user who created the session. noHyperplaneCommands\tBoolean\tSpecifies whether default Shakudo platform commands are used for the pipeline job creation. commitId\tString\tThe commit hash for the specific commit used to pull the latest files for the pipeline. branchName\tString!\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. pipelineYamlPath\tString!\tThe relative path to the .yaml file used to run this pipeline job. jobType\tString!\tName of the Shakudo platform Podspec/Image used for the pipeline job. eg. &quot;basic&quot; Create Service using UserServicePodSpec result mutation CreateService( $podSpec: JSON! $jobName: String! = &quot;&quot; $userEmail: String! $exposedPort: String! = &quot;8787&quot; $pipelineYamlPath: String! ) { createPipelineJob (data: { jobName: $jobName, podSpec: $podSpec, hyperplaneUserEmail: $userEmail, pipelineYamlPath: $pipelineYamlPath, exposedPort: $exposedPort, timeout: -1, activeTimeout: -1, schedule: &quot;immediate&quot; } ) { id pipelineYamlPath schedule status statusReason output startTime completionTime daskDashboardUrl timeout outputNotebooksPath activeTimeout maxRetries exposedPort jobType parameters { key value } } }  Sample Variables { &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;exposedPort&quot;: &quot;8787&quot;, &quot;branchName&quot;: &quot;main&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;jobType&quot;: &quot;basic&quot;, &quot;jobName&quot;: &quot;test-service&quot;, &quot;podSpec&quot;: &lt;getUserServicePodSpec field result&gt; }  Parameters Field\tType\tDescriptionpodSpec\tJSON!\tThe JSON object representing the PodSpec configuration for the pipeline job. jobName\tString!\tThe name of the pipeline job. userEmail\tString!\tShakudo platform user email for the user who created the session. exposedPort\tString!\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. Default value: &quot;8787&quot;. pipelineYamlPath\tString!\tThe relative path to the .yaml file used to run this pipeline job. ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-13","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-13","content":"{ &quot;data&quot;: { &quot;createPipelineJob&quot;: { &quot;id&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot;, &quot;pipelineYamlPath&quot;: &quot;examples/pipeline.yaml&quot;, &quot;schedule&quot;: &quot;immediate&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;output&quot;: null, &quot;startTime&quot;: &quot;2023-06-30T16:03:42.668Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;timeout&quot;: -1, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: -1, &quot;maxRetries&quot;: 2, &quot;exposedPort&quot;: &quot;8787&quot;, &quot;jobType&quot;: &quot;basic&quot;, &quot;parameters&quot;: [] } } }  ","version":"Next","tagName":"h3"},{"title":"Get a List of Services​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#get-a-list-of-services","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-11","content":"Get a list of services — services are pipeline jobs which have an activeTimeout and timeout of -1, ie. never ending jobs, schedule = &quot;immediate&quot;, and with exposedPort != null query services($offset: Int, $limit: Int!, $status: String!) { pipelineJobs(orderBy: [{pinned: desc},{ startTime: desc}], take: $limit, skip: $offset, where: { AND: [ {activeTimeout: {equals: -1}}, {timeout: {equals: -1}}, {timeout: {equals: &quot;immediate&quot;}}, {status: {equals: $status}} ] }) { id exposedPort pinned pipelineYamlPath schedule status statusReason startTime completionTime daskDashboardUrl timeout output outputNotebooksPath activeTimeout duration jobType schedule estimatedCost owner maxRetries } }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-12","content":"{ &quot;limit&quot;: 10, &quot;status&quot;: &quot;in progress&quot; }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters-8","content":"Field\tType\tDescriptionoffset\tInt\tThe number of records to skip from the original result. limit\tInt! (required)\tThe number of records to retrieve. status\tString! (required)\tThe status of the Kubernetes job that runs the pipeline job. ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-14","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-14","content":"{ &quot;data&quot;: { &quot;pipelineJobs&quot;: [ { &quot;id&quot;: &quot;9276a796-229f-4ede-a2cf-a7cf329dab6a&quot;, &quot;exposedPort&quot;: &quot;8787&quot;, &quot;pinned&quot;: false, &quot;pipelineYamlPath&quot;: &quot;service.yaml&quot;, &quot;schedule&quot;: &quot;immediate&quot;, &quot;status&quot;: &quot;in progress&quot;, &quot;statusReason&quot;: null, &quot;startTime&quot;: &quot;2023-03-23T02:34:51.850Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: &quot;client.hyperplane.dev/dashboard/&quot;, &quot;timeout&quot;: -1, &quot;output&quot;: null, &quot;outputNotebooksPath&quot;: null, &quot;activeTimeout&quot;: -1, &quot;duration&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;estimatedCost&quot;: null, &quot;owner&quot;: &quot;demo&quot;, &quot;maxRetries&quot;: 0 }, { &quot;id&quot;: &quot;abeee208-c717-42d9-81f9-9448cdf1473e&quot;, &quot;exposedPort&quot;: &quot;8787&quot;, &quot;pinned&quot;: false, &quot;pipelineYamlPath&quot;: &quot;service2.yaml&quot;, &quot;schedule&quot;: &quot;immediate&quot;, &quot;status&quot;: &quot;in progress&quot;, &quot;statusReason&quot;: null, &quot;startTime&quot;: &quot;2022-11-18T19:21:23.504Z&quot;, &quot;completionTime&quot;: null, &quot;daskDashboardUrl&quot;: &quot;client.hyperplane.dev/dashboard2/&quot;, &quot;timeout&quot;: -1, &quot;output&quot;: null, &quot;outputNotebooksPath&quot;: &quot;gs://outputNotebookPath&quot;, &quot;activeTimeout&quot;: -1, &quot;duration&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;estimatedCost&quot;: null, &quot;owner&quot;: &quot;demo&quot;, &quot;maxRetries&quot;: 0 } ] } }  ","version":"Next","tagName":"h3"},{"title":"Cancel all Scheduled Jobs​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#cancel-all-scheduled-jobs","content":"Cancel all Scheduled Jobs mutation cancelScheduledJobs { updateManyPipelineJob( where: { status: { equals: &quot;scheduled&quot; } } data: { status: { set: &quot;cancelled&quot; } } ) { count } }  Cancel all Scheduled Jobs for a Specific User Users can also add hyperplaneUserEmail: { equals: $userEmail } to cancel all scheduled jobs created by a particular user. mutation cancelScheduledJobsForUser($userEmail: String!) { updateManyPipelineJob( where: { status: { equals: &quot;scheduled&quot; }, hyperplaneUserEmail: { equals: $userEmail } } data: { status: { set: &quot;cancelled&quot; } } ) { count } }  Sample Variables { &quot;userEmail&quot;: &quot;demo@shakudo.io&quot; }  Parameters Field\tType\tDescriptionuserEmail\tString! (required)\tThe email corresponding to the user who created all the scheduled jobs to be cancelled. ","version":"Next","tagName":"h2"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-15","content":"AffectedRowsOutput ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-15","content":"{ &quot;data&quot;: { &quot;updateManyPipelineJob&quot;: { &quot;count&quot;: 2 } } }  ","version":"Next","tagName":"h3"},{"title":"Get Job Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#get-job-parameters","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-12","content":"Get the list of parameters for a pipeline job query jobParameters($id: String!) { pipelineJobs(where: {id: {equals: $id}}) { parameters { key value id pipelineJobId } } }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters-9","content":"Field\tType\tDescriptionid\tString! (required)\tThe ID of the job for which the parameters are being listed. ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-13","content":"{ &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields","content":"Array of Parameters ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-16","content":"{ &quot;data&quot;: { &quot;pipelineJobs&quot;: [ { &quot;parameters&quot;: [ { &quot;key&quot;: &quot;key&quot;, &quot;value&quot;: &quot;value&quot;, &quot;id&quot;: &quot;abeee208-c717-42d9-81f9-9448cdf1473e&quot;, &quot;pipelineJobId&quot;: &quot;d1e5cd20-05d3-4517-b009-ec2e8e4f171d&quot; } ] } ] } }  ","version":"Next","tagName":"h3"},{"title":"Delete a Job Parameter​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#delete-a-job-parameter","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-13","content":"Delete a parameter for a pipeline job # Retrieve parameterId query GetPipelineJobParameters($jobId: String!){ pipelineJob(where:{id: $jobId}){ jobName parameters{ id key value } } } mutation DeletePipelineJobParameter($parameterId: String!) { deleteParameter(where: { id: $parameterId }) { id key value } }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters-10","content":"Field\tType\tDescriptionjobId\tString! (required)\tThe ID of the job from which the parameter is being deleted. parameterId\tString! (required)\tThe ID of the parameter being deleted. Retrieved from GetPipelineJobParameters ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-14","content":"{ &quot;jobId&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;parameterId&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-16","content":"Parameter ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-17","content":"{ &quot;data&quot;: { &quot;deleteParameter&quot;: { &quot;id&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot;, &quot;key&quot;: &quot;foo&quot;, &quot;value&quot;: &quot;bar&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"Update a Job Parameter​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#update-a-job-parameter","content":"","version":"Next","tagName":"h2"},{"title":"Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#description-14","content":"Updates the key and/or value of a parameter. mutation ($parameterId: String!, $keyValue: String, $valueValue: String) { updateParameter(where: {id: $parameterId}, data: { key: {set: $keyValue} value: {set: $valueValue} }) { id key value } }  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameters-11","content":"Field\tType\tDescriptionparameterId\tString! (required)\tID of the parameter being updated. keyValue\tString\tNew value for the &quot;key&quot; field of the parameter. valueValue\tString\tNew value for the &quot;value&quot; field of the parameter. ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-15","content":"{ &quot;parameterId&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;keyValue&quot;: &quot;newKey&quot;, &quot;valueValue&quot;: &quot;newValue&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-17","content":"Parameter ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-18","content":"{ &quot;data&quot;: { &quot;updateParameter&quot;: { &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; &quot;keyValue&quot;: &quot;newKey&quot;, &quot;valueValue&quot;: &quot;newValue&quot; } } }  Types ","version":"Next","tagName":"h3"},{"title":"HyperHubSession​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#hyperhubsession","content":"Metadata for Sessions billingProject: BillingProject billingProjectId: String collaborative: Boolean completionTime: DateTime currentPodEvents: String department: String duration: Int estimatedCost: Float gpuRequest: String group: String hyperplaneUser: HyperplaneUser! hyperplaneUserEmail: String! hyperplanepodspecName: String id: String imageHash: String imageType: String jLabUrl: String notebookURI: String owner: String podEventsLog: String podSpec: String podSpecTemplate: HyperplanePodSpec premptableNode: Boolean resourceCPUlimit: String resourceCPUrequest: String resourceRAMlimit: String resourceRAMrequest: String runId: String sshCommand: String startTime: DateTime status: String statusReason: String timeout: Int useHyperplanepodspec: Boolean userPvc: UserPvc userPvcName: String workerPodName: String  Field\tType\tDefinitionbillingProject\tBillingProject\tBilling project that user would like Session costs to contribute. Can either provide identifiers to connect to an existing billing project or can provide values to create a new billing project. billingProjectId\tString\tBilling Project ID. collaborative\tBoolean\tToggle collaborative mode completionTime\tDateTime\tCompletion time of the pipeline job currentPodEvents\tString\tDisplays log of states of pod (current events in pod) department\tString\tDisabled, not used duration\tInt\tDuration of the pipeline job estimatedCost\tFloat\tDisabled, plan on using it for tracking estimated cost of the job gpuRequest\tString\tNumber of gpus requested group\tString\tNot used, leave as an empty string hyperplaneUser\tHyperplaneUser!\tShakudo platform user account details. Can either provide identifiers to connect to an existing account or can provide values to create a new user account. hyperplaneUserEmail\tString!\tShakudo platform user account email hyperplanepodspecName\tString\tDisabled id\tString\tHyperHubSession object identifier imageHash\tString\tURL of custom image, same as imageUrl imageType\tString\tName of Shakudo platform Podspec/Image jLabUrl\tString\tURL for JupyterLab version of the Session environment notebookURI\tString\tBase url to access jupyter notebook and vscode notebooks owner\tString\tUsername of the user account that owns the session, currently the user that created the session podEventsLog\tString\tSession pod event status log details podSpec\tJson?\tShakudo platform PodSpec environment config object as JSON podSpecTemplate\tHyperplanePodSpec\tNot used, similar use to jobType premptableNode\tBoolean\tDisabled resourceCPUlimit\tString\tLimit to the number of CPUs to be allocated resourceCPUrequest\tString\tNumber of CPUs requested to be allocated resourceRAMlimit\tString\tMemory allocation limit resourceRAMrequest\tString\tMemory allocation amount request runId\tString sshCommand\tString startTime\tDateTime\tSession environment creation time status\tString\tThe status of the Kubernetes job that runs the pipeline job statusReason\tString\tKubernetes job status details timeout\tInt\tThe maximum time in seconds that the pipeline may run, starting from the moment of job submission. Default: -1, i.e., never timeout; 86400 on the dashboard useHyperplanepodspec\tBoolean userPvc\tUserPvc\tShakudo session persistent volume (drive) details. Can either provide identifiers to connect to an existing drive or can provide values to create a new drive. Default: not present, which corresponds with default drive claim-{user-email}. userPvcName\tString\tPersistent volume name as found in Kubernetes. Typically includes the drive name found on the dashboard. Default: an empty string, which corresponds with the default drive claim-{user-email}. workerPodName\tString\t Example: { &quot;billingProject&quot;: null, &quot;collaborative&quot;: false, &quot;completionTime&quot;: &quot;2023-05-17T21:26:02.310Z&quot;, &quot;currentPodEvents&quot;: null, &quot;department&quot;: null, &quot;duration&quot;: 901, &quot;estimatedCost&quot;: null, &quot;gpuRequest&quot;: null, &quot;group&quot;: &quot;&quot;, &quot;hyperplaneUser&quot;: { &quot;id&quot;: &quot;3661bd41-6ca9-4b20-a74b-c46ce6ff6951&quot;, &quot;email&quot;: &quot;demo@shakudo.io&quot; }, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;hyperplanepodspecName&quot;: null, &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;imageHash&quot;: &quot;gcr.io/imageHash&quot;, &quot;imageType&quot;: &quot;test-custom-image&quot;, &quot;jLabUrl&quot;: &quot;&quot;, &quot;notebookURI&quot;: &quot;&quot;, &quot;owner&quot;: &quot;demo&quot;, &quot;podEventsLog&quot;: &quot;Stopping container hyperhub-user&quot;, &quot;podSpec&quot;: null, &quot;podSpecTemplate&quot;: null, &quot;premptableNode&quot;: false, &quot;resourceCPUlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;runId&quot;: null, &quot;sshCommand&quot;: null, &quot;startTime&quot;: &quot;2023-05-17T21:11:00.948Z&quot;, &quot;status&quot;: &quot;cancelled&quot;, &quot;statusReason&quot;: &quot;Ready--true&quot;, &quot;timeout&quot;: 900, &quot;useHyperplanepodspec&quot;: false, &quot;userPvc&quot;: null, &quot;userPvcName&quot;: &quot;&quot;, &quot;workerPodName&quot;: null }  ","version":"Next","tagName":"h2"},{"title":"PipelineJob​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#pipelinejob","content":"Shakudo platform job config docs type PipelineJob { TritonClient: TritonClient activeTimeout: Int! billingProject: BillingProject billingProjectId: String branchName: String branchNameOrCommit: BranchSelection childJobs(cursor: PipelineJobWhereUniqueInput, distinct: [PipelineJobScalarFieldEnum!], orderBy: [PipelineJobOrderByInput!], skip: Int, take: Int, where: PipelineJobWhereInput): [PipelineJob!]! cloudRunner: String commitId: String completionTime: DateTime customCommand: String customTrigger: String dashboardPrefix: String daskDashboardUrl: String debuggable: Boolean! department: String displayedOwner: String! duration: Int estimatedCost: Float exposedPort: String grafanaLink: String! group: String hyperplaneUser: HyperplaneUser hyperplaneUserEmail: String hyperplaneUserId: String hyperplaneVCServer: HyperplaneVCServer hyperplaneVCServerId: String hyperplanepodspecName: String icon: String id: String! imageHash: String jobCommand: String jobName: String jobType: String! mappedUrl: String maxHpaRange: Int! maxRetries: Int! maxRetriesPerStep: Int! minReplicas: Int! noGitInit: Boolean noHyperplaneCommands: Boolean noVSRewrite: Boolean output: String outputNotebooksPath: String owner: String parameters(cursor: ParameterWhereUniqueInput, distinct: [ParameterScalarFieldEnum!], orderBy: [ParameterOrderByInput!], skip: Int, take: Int, where: ParameterWhereInput): [Parameter!]! parentJob: PipelineJob parentJobId: String pinned: Boolean! pipelineYamlPath: String podSpecTemplate: HyperplanePodSpec podSpecTemplateId: String preemptible: Boolean! premptableNode: Boolean! priorityClass: String! runId: String schedule: String! sendNotification: Boolean slackChannelName: String sshCommand: String startTime: DateTime! status: String! statusReason: String steps(cursor: PipelineStepWhereUniqueInput, distinct: [PipelineStepScalarFieldEnum!], orderBy: [PipelineStepOrderByInput!], skip: Int, take: Int, where: PipelineStepWhereInput): [PipelineStep!]! timeout: Int! timeoutPerStep: Int timezone: String! useHyperplanepodspec: Boolean workerPodName: String }  Field\tType\tDefinitionTritonClient\tTritonClient\tIf TritonClient is non-null, then this PipelineJob is a Triton Job. TritonClient stores Triton client instance object metadata. activeTimeout\tInt!\tThe maximum time in seconds that the pipeline may run once it is picked up. Default: 86400, use -1 to never timeout. billingProject\tBillingProject\tBilling project that user would like Job costs to contribute. Can either provide identifiers to connect to an existing billing project or can provide values to create a new billing project. billingProjectId\tString\tBilling Project ID. branchName\tString\tName of git branch. branchNameOrCommit\tBranchSelection\tEnum that states whether the image is based on branch or commit. childJobs\t[PipelineJob!]!\tJobs that spawned based on this job. cloudRunner\tString\tCurrently disabled, but will be used to determine which cloud the job will run on. Will be added as part of the multicloud feature. commitId\tString\tCommit hash for the commit used to pull the latest files for the pipeline. completionTime\tDateTime\tCompletion time of the pipeline job. customCommand\tString\tNot used customTrigger\tString\tCurrently disabled on cluster, but will be used on KEDA jobs. dashboardPrefix\tString\tWhich URL subpath you want a Service to run in with respect to your Shakudo service domain. e.g., http://shakudoservice.io/modelV1. daskDashboardUrl\tString\tURL for dask dashboard debuggable\tBoolean!\tWhether the debuggable service is enabled. department\tString\tNot used displayedOwner\tString!\tUsername of the user account that owns the job, currently the user that created the job. Username is based on email. duration\tInt\tDuration of the pipeline job. estimatedCost\tFloat\tNot used exposedPort\tString\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. grafanaLink\tString!\tLink to Grafana logs. group\tString\tDisabled, not used. Leave as empty string hyperplaneUser\tHyperplaneUser\tShakudo platform user account details. Can either provide identifiers to connect to an existing account or can provide values to create a new user account. hyperplaneUserEmail\tString\tShakudo platform user email. hyperplaneUserId\tString\tShakudo platform user ID. hyperplaneVCServer\tHyperplaneVCServer\tShakudo Platform Git Server object. hyperplaneVCServerId\tString\tGit server object ID. hyperplanepodspecName\tString icon\tString id\tString! imageHash\tString\timageUrl jobCommand\tString jobName\tString\tPlain name of job viewable from the dashboard, is not necessarily unique. jobType\tString!\tName of Shakudo platform Podspec/Image. mappedUrl\tString maxHpaRange\tInt!\tMaximum number of replicas for HPA (horizontal pod autoscaling). maxRetries\tInt!\tMaximum number of job retries. maxRetriesPerStep\tInt!\tMaximum number of retries per job step. minReplicas\tInt!\tMinimum number of K8s ReplicaSets. noGitInit\tBoolean\tFalse if git server is to be set up using Shakudo platform workflow. Default: false. noHyperplaneCommands\tBoolean\tFalse if using default Shakudo platform commands on job creation. Required to use Shakudo platform jobs through the pipeline yaml, but not required if the image has its own setup. Default: false. noVSRewrite\tBoolean\tOnly supported for Services. If enabled, the external prefix/subpath on the Shakudo domain directly corresponds to the same subpath within the Service. output\tString outputNotebooksPath\tString owner\tString\tTypically mirrors displayedOwner, refer primarily to displayedOwner. parameters\t[Parameter!]!\tList of key-value parameters that are injected into the Job environment and can be used as environment variables. parentJob\tPipelineJob\tThe info of that parent job if the current job spawned from another job. parentJobId\tString\tParent job ID. pinned\tBoolean!\tWhether the job is pinned on the dashboard. pipelineYamlPath\tString\tRelative path to .yaml file for running pipeline podSpecTemplate\tHyperplanePodSpec\tNot used, similar use to jobType. podSpecTemplateId\tString\tID for the corresponding podSpecTemplate. preemptible\tBoolean!\tDetermines whether the job can be preempted, i.e., timed out. This means that a Preemptible VM will be used. premptableNode\tBoolean!\tNot used priorityClass\tString!\tK8s pod priority classification. runId\tString schedule\tString!\tEither &quot;immediate&quot; for an immediate job or a cron schedule expression for a scheduled job at the specified interval. sendNotification\tBoolean slackChannelName\tString sshCommand\tString startTime\tDateTime!\tJob start time. status\tString!\tThe status of the Kubernetes job that runs | statusReason | String | Kubernetes job status detail | | steps | (cursor: PipelineStepWhereUniqueInput, distinct: [PipelineStepScalarFieldEnum!], orderBy: [PipelineStepOrderByInput!], skip: Int, take: Int, where: PipelineStepWhereInput): [PipelineStep!]! | pipeline step objects that correspond with individual script steps | | timeoutPerStep | Int | | | timezone | String | eg. UTC | | useHyperplanepodspec | Boolean | Not used | | workerPodName | String | Not used | Example: { &quot;TritonClient&quot;: null, &quot;activeTimeout&quot;: 82400, &quot;billingProject&quot;: null, &quot;billingProjectId&quot;: null, &quot;branchName&quot;: null, &quot;branchNameOrCommit&quot;: null, &quot;childJobs&quot;: [], &quot;cloudRunner&quot;: &quot;&quot;, &quot;commitId&quot;: null, &quot;completionTime&quot;: null, &quot;customCommand&quot;: null, &quot;customTrigger&quot;: null, &quot;dashboardPrefix&quot;: null, &quot;daskDashboardUrl&quot;: null, &quot;debuggable&quot;: false, &quot;department&quot;: null, &quot;displayedOwner&quot;: &quot;&quot;, &quot;duration&quot;: null, &quot;estimatedCost&quot;: null, &quot;exposedPort&quot;: null, &quot;grafanaLink&quot;: &quot;https://grafana.sample.hyperplane.dev/explore&quot;, &quot;group&quot;: null, &quot;hyperplaneUser&quot;: null, &quot;hyperplaneUserEmail&quot;: null, &quot;hyperplaneUserId&quot;: null, &quot;hyperplaneVCServer&quot;: null, &quot;hyperplaneVCServerId&quot;: null, &quot;hyperplanepodspecName&quot;: null, &quot;icon&quot;: null, &quot;id&quot;: &quot;b50e8ea9-1627-4a5d-b7c7-ebad6c801d0a&quot;, &quot;imageHash&quot;: &quot;&quot;, &quot;jobCommand&quot;: null, &quot;jobName&quot;: null, &quot;jobType&quot;: &quot;basic&quot;, &quot;mappedUrl&quot;: null, &quot;maxHpaRange&quot;: 1, &quot;maxRetries&quot;: 2, &quot;maxRetriesPerStep&quot;: 0, &quot;minReplicas&quot;: 1, &quot;noGitInit&quot;: false, &quot;noHyperplaneCommands&quot;: false, &quot;noVSRewrite&quot;: false, &quot;output&quot;: null, &quot;outputNotebooksPath&quot;: null, &quot;owner&quot;: null, &quot;parameters&quot;: [], &quot;parentJob&quot;: null, &quot;parentJobId&quot;: null, &quot;pinned&quot;: false, &quot;pipelineYamlPath&quot;: &quot;example_pipeline.yaml&quot;, &quot;podSpecTemplate&quot;: null, &quot;podSpecTemplateId&quot;: null, &quot;preemptible&quot;: true, &quot;premptableNode&quot;: true, &quot;priorityClass&quot;: &quot;shakudo-priority-class&quot;, &quot;runId&quot;: null, &quot;schedule&quot;: &quot;immediate&quot;, &quot;sendNotification&quot;: null, &quot;slackChannelName&quot;: null, &quot;sshCommand&quot;: null, &quot;startTime&quot;: &quot;2023-06-29T16:38:10.829Z&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;statusReason&quot;: null, &quot;steps&quot;: [], &quot;timeout&quot;: 82400, &quot;timeoutPerStep&quot;: null, &quot;timezone&quot;: &quot;UTC&quot;, &quot;useHyperplanepodspec&quot;: false, &quot;workerPodName&quot;: null }  ","version":"Next","tagName":"h2"},{"title":"Parameter​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#parameter","content":"Key-value pairs that are injected into Jobs and Session environments type Parameter { PipelineJob: PipelineJob id: String! key: String! pipelineJobId: String value: String }  Field\tType\tDefinitionPipelineJob\tPipelineJob\tPipeline job that has this parameter id\tString!\tThe ID of the parameter key\tString!\tThe key of the parameter pipelineJobId\tString\tThe ID of the pipeline job that has this parameter value\tString\tThe value of the parameter { &quot;key&quot;: &quot;key&quot;, &quot;value&quot;: &quot;value&quot;, &quot;id&quot;: &quot;b50e8ea9-1627-4a5d-b7c7-ebad6c801d0a&quot;, &quot;pipelineJobId&quot;: &quot;833bd3d1-bb63-4289-99d8-25c2856f2fba&quot; }  ","version":"Next","tagName":"h2"},{"title":"HyperplaneVCServer​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#hyperplanevcserver","content":"Git servers tied to remote git repositories type HyperplaneVCServer { defaultBranch: String! id: String! name: String! pipelineJobs(cursor: PipelineJobWhereUniqueInput, distinct: [PipelineJobScalarFieldEnum!], orderBy: [PipelineJobOrderByInput!], skip: Int, take: Int, where: PipelineJobWhereInput): [PipelineJob!]! serviceUrl: String status: HyperplaneVCServerStatus! url: String! }  Field\tType\tDefinitiondefaultBranch\tString!\tThe default git branch of the git server id\tString!\tThe ID of the git server (HyperplaneVCServer) object name\tString!\tThe name of the git server pipelineJobs\t[PipelineJob!]!\tThe pipeline jobs that are connected to this git server. Mirrors pipelineJobs query serviceUrl\tString\tThe service URL (DNS record) for in-cluster connection access status\tHyperplaneVCServerStatus!\tThe status of the git server resource url\tString!\tThe remote repository SSH URL ","version":"Next","tagName":"h2"},{"title":"HyperplanePodSpec​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#hyperplanepodspec","content":"Environment configs for defining Shakudo resources; surrounds image, hardware, storage, kubernetes settings, etc. type HyperplanePodSpec { description: String! displayName: String! extraEnvars: String extraTolerations: String extraVolumeMounts: String extraVolumes: String gpuResourceType: String hyperhubSessions(cursor: HyperHubSessionWhereUniqueInput, distinct: [HyperHubSessionScalarFieldEnum!], orderBy: [HyperHubSessionOrderByInput!], skip: Int, take: Int, where: HyperHubSessionWhereInput): [HyperHubSession!]! hyperplaneImage: HyperplaneImage hyperplaneImageId: String hyperplaneUser: HyperplaneUser hyperplaneUserEmail: String hyperplaneUserId: String icon: String! id: String! imagePullPolicy: String imageUrl: String nodeSelector: String nodeSelectorKey: String nodeSelectorValue: String pipelineJobs(cursor: PipelineJobWhereUniqueInput, distinct: [PipelineJobScalarFieldEnum!], orderBy: [PipelineJobOrderByInput!], skip: Int, take: Int, where: PipelineJobWhereInput): [PipelineJob!]! podSpec: String podspecName: String! pv: String pvc: String resourceCPUlimit: String resourceCPUrequest: String resourceGPUrequest: String resourceRAMlimit: String resourceRAMrequest: String show: Boolean! status: String! statusReason: String workingDir: String }  Field\tType\tDefinitiondescription\tString!\tPodSpec description displayName\tString!\tGeneral purpose display name for PodSpec that appears as a title on dashboard extraEnvars\tString\tList of key-value parameters that are injected into any Shakudo resource environment extraTolerations\tString\tAdditional pod toleration rules extraVolumeMounts\tString\tAdditional storage mounting rules. These are relative to the provided Volumes extraVolumes\tString\tAdditional persistent storage spaces gpuResourceType\tString\tType of GPU resource hyperhubSessions\t[HyperHubSession!]!\tSessions that currently use this PodSpec. Mirrors hyperhubSessions query hyperplaneImage\tHyperplaneImage\tHyperplane image associated with the PodSpec hyperplaneUser\tHyperplaneUser\tShakudo platform user account details icon\tString!\tNot used id\tString!\tID of the PodSpec imagePullPolicy\tString imageUrl\tString\tURL of image nodeSelector\tString nodeSelectorKey\tString nodeSelectorValue\tString pipelineJobs\t[PipelineJob!]!\tThe pipeline jobs that are connected to this git server. Mirrors pipelineJobs query podSpec\tString podspecName\tString! pv\tString\tPersistent volume associated with the PodSpec pvc\tString\tPersistent volume claim associated with the PodSpec resourceCPUlimit\tString\tCPU limit for resource allocation resourceCPUrequest\tString\tCPU request for resource allocation resourceGPUrequest\tString\tGPU request for resource allocation resourceRAMlimit\tString\tRAM limit for resource allocation resourceRAMrequest\tString\tRAM request for resource allocation show\tBoolean! status\tString!\tStatus of the PodSpec statusReason\tString\tReason for the PodSpec status workingDir\tString\tWorking directory for the PodSpec Operations ","version":"Next","tagName":"h2"},{"title":"hyperHubSessions​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#hyperhubsessions","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature","content":"hyperHubSessions( cursor: HyperHubSessionWhereUniqueInput, distinct: [HyperHubSessionScalarFieldEnum!], orderBy: [HyperHubSessionOrderByInput!], skip: Int, take: Int, where: HyperHubSessionWhereInput ): [HyperHubSession!]!  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description","content":"Retrieves a list of Shakudo platform session metadata, allowing for pagination (cursor and offset-based) and filtering. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields","content":"Field\tType\tDefinitioncursor\tHyperHubSessionWhereUniqueInput\tStarting session value to paginate from using cursor-based pagination, i.e., the current result starts from this session record. distinct\t[HyperHubSessionScalarFieldEnum!]\tList of fields where their values will remain distinct per record. orderBy\t[HyperHubSessionOrderByInput!]\tList of fields that will be used to order the results, ordering precedence determined by the location in the list. skip\tInt\tThe number of records to skip from the original result. take\tInt\tThe maximum number of records to show in the result. where\tHyperHubSessionWhereInput\tConditional values to filter for a specific HyperHubSession object. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example","content":"query HyperhubSessions($limit: Int!, $email: String, $status: String) { hyperHubSessions(orderBy:{startTime: desc}, take: $limit, where: { hyperplaneUserEmail: {equals: $email}, status: {equals: $status}, }) { id hyperplaneUserEmail status imageType jLabUrl notebookURI estimatedCost department resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime completionTime } countHyperHubSessions }  variables { &quot;limit&quot;: 10, &quot;email&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;in progress&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields-1","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-example","content":"{ &quot;data&quot;: { &quot;hyperHubSessions&quot;: [ { &quot;id&quot;: &quot;78ba5679-1fd0-475a-88b0-d1877413747f&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;in progress&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;jLabUrl&quot;: &quot;client/hyperplane.dev/jlabUrl/&quot;, &quot;notebookURI&quot;: &quot;ssh demo-pvc-entry@demo.dev&quot;, &quot;estimatedCost&quot;: null, &quot;department&quot;: null, &quot;resourceCPUlimit&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;gpuRequest&quot;: null, &quot;startTime&quot;: &quot;2023-06-28T15:32:40.090Z&quot;, &quot;completionTime&quot;: null } ], &quot;countHyperHubSessions&quot;: 3006 } }  ","version":"Next","tagName":"h3"},{"title":"createHyperHubSession​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#createhyperhubsession","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature-1","content":"createHyperHubSession( data: HyperHubSessionCreateInput! ): HyperHubSession!  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description-1","content":"Creates a Shakudo Session environment, a data development environment comes with pre-configured environments, typically accessible in the form of a jupyter notebook. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields-1","content":"Field\tType\tDescriptiondata\tHyperHubSessionCreateInput\tHyperHubSession object that contains field values used to create a Session. Check HyperHubSessionCreateInput for specific fields. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example-1","content":"mutation CreateHyperHubSession($input: HyperHubSessionCreateInput!) { createHyperHubSession(data: $input) { id hyperplaneUserEmail status imageType jLabUrl estimatedCost department resourceCPUlimit resourceRAMlimit resourceCPUrequest resourceRAMrequest gpuRequest startTime completionTime timeout group billingProjectId } }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-16","content":"{ &quot;input&quot;: { &quot;collaborative&quot;: false, &quot;imageType&quot;: &quot;basic&quot;, &quot;imageUrl&quot;: &quot;&quot;, &quot;timeout&quot;: 900, &quot;userPvcName&quot;: &quot;&quot;, &quot;group&quot;: &quot;&quot;, &quot;hyperplaneUserId&quot;: &quot;2a9980d9-f43c-4369-b71e-70d12d369e47&quot;, &quot;billingProjectId&quot;: { &quot;connect&quot;: { &quot;id&quot;: &quot;284f0a8e-52d9-4a57-be42-f461fc4315c7&quot; } }, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot; } }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields-2","content":"GraphQL Docs ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-example-1","content":"{ &quot;data&quot;: { &quot;createHyperHubSession&quot;: { &quot;id&quot;: &quot;f48e3b18-bced-4a8c-85b9-b3c2fdd1a06a&quot;, &quot;hyperplaneUserEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;status&quot;: &quot;pending&quot;, &quot;imageType&quot;: &quot;basic&quot;, &quot;jLabUrl&quot;: null, &quot;estimatedCost&quot;: null, &quot;department&quot;: null, &quot;resourceCPUlimit&quot;: null, &quot;resourceRAMlimit&quot;: null, &quot;resourceCPUrequest&quot;: null, &quot;resourceRAMrequest&quot;: null, &quot;gpuRequest&quot;: null, &quot;startTime&quot;: &quot;2023-06-27T19:27:43.987Z&quot;, &quot;completionTime&quot;: null, &quot;timeout&quot;: 900, &quot;group&quot;: &quot;&quot;, &quot;billingProjectId&quot;: &quot;f6a3911d-e048-49f0-96d8-1abd930b66db&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"updateHyperHubSession​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#updatehyperhubsession","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature-2","content":"updateHyperHubSession( data: HyperHubSessionUpdateInput! where: HyperHubSessionWhereUniqueInput! ): HyperHubSession  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description-2","content":"Updates the fields for a specific Session based on the provided data and conditions provided. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields-2","content":"Field\tType\tDefinitiondata\tHyperHubSessionUpdateInput!\tHyperHubSession partial object that contains field values used to update, specified in the format [field]: {[action]: [value]} where\tHyperHubSessionWhereUniqueInput!\tConditional values to filter for a specific HyperHubSession object ","version":"Next","tagName":"h3"},{"title":"Request Example: Cancelling a Session​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example-cancelling-a-session","content":"mutation UpdateHyperHubSession($id: String!) { updateHyperHubSession(where: {id: $id}, data: { status: {set: &quot;cancelled&quot;} }) { id status } }  variables { &quot;id&quot;: &quot;7b728979-71b7-426c-9847-6fe3e29a6438&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields-3","content":"HyperHubSession ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-example-2","content":"{ &quot;data&quot;: { &quot;updateHyperHubSession&quot;: { &quot;id&quot;: &quot;7b728979-71b7-426c-9847-6fe3e29a6438&quot;, &quot;status&quot;: &quot;cancelled&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"getJobStat​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#getjobstat","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature-3","content":"getJobStat( stat: StatType!, status: StatusType, timeFrame: TimeFrame! ): Int  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description-3","content":"Retrieves job count statistics based on the conditions provided. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields-3","content":"Field\tType\tDescriptionstat\tStatType!\tStatistic type options. Possible values: COUNT_ALL, COUNT_CANCELLED, COUNT_DONE, COUNT_FAILED, COUNT_IN_PROGRESS, COUNT_PENDING, COUNT_SCHEDULED, COUNT_TIMED_OUT, COUNT_TRIGGERED. status\tStatusType\tStatus type options. Possible values: ALL, SCHEDULED, TRIGGERED. timeFrame\tTimeFrame!\tTimeframe options. Possible values: TOTAL, T_1H, T_10M, T_24H. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example-2","content":"query { getJobStat(stat: COUNT_ALL, timeFrame: TOTAL) }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields-4","content":"Int ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-example-3","content":"{ &quot;data&quot;: { &quot;getJobStat&quot;: 105179 } }  ","version":"Next","tagName":"h3"},{"title":"createPipelineJob​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#createpipelinejob","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature-4","content":"createPipelineJob( data: PipelineJobCreateInput! ): PipelineJob!  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description-4","content":"Creates a Shakudo platform job, which allows users to run task scripts using custom configurations, either immediately as an “Immediate job”, at scheduled intervals as a “Scheduled Job”, or indefinitely as a “Service”. Immediate jobs: schedule = “immediate” Scheduled jobs: schedule ≠ “immediate”, schedule is set to cron schedule expression, eg. 0 0 * * * for a job running every minute Service: timeout and activeTimeout set to -1 and exposedPort != null, particularly set to a valid port ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields-4","content":"data: PipelineJob object that contains field values used to create a PipelineJob. Check PipelineJobCreateInput for specific fields. Example fields: Field\tType\tDefinitiontype\tString!\tName of Shakudo platform Podspec/Image, default or custom. Example: &quot;basic&quot; timeout\tInt!\tThe maximum time in seconds that the pipeline may run, starting from the moment of job submission. Default: -1 (never timeout). Example: 86400 activeTimeout\tInt\tThe maximum time in seconds that the pipeline may run once it is picked up. Default: -1 (never timeout). Example: 86400 maxRetries\tInt!\tThe maximum number of attempts to run your pipeline job before returning an error, even if timeouts are not reached. Default: 2 yamlPath\tString\tThe relative path to the .yaml file used to run this pipeline job. Example: &quot;example_notebooks/pipelines/python_hello_world_pipeline/pipeline.yaml&quot; exposedPort\tString\tOnly enabled for Shakudo Services. The port that Services use to expose the pod to other pods within the cluster. Its presence is a current indicator of whether a job is a Service. schedule\tString\tEither &quot;immediate&quot; for an immediate job or a cron schedule expression for a scheduled job at the specified interval. parameters\tParameterCreateNestedManyWithoutPipelineJobInput\tKey-value pairs that can be used within the container environment gitServer\tHyperplaneVCServerCreateNestedOneWithoutPipelineJobsInput\tGit server object, retrievable by searching git servers by name (hyperplaneVCServers) and using resulting id in the following manner: { connect: { id: $gitServerId } } hyperplaneUserEmail\tString!\tShakudo platform user email branchName\tString\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. If commitID is not specified, the latest commit is used. If not specified, default branch is used. podSpec\tJSON\tShakudo platform PodSpec config object as a JSON object ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example-3","content":"Variables ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields-5","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-example-4","content":"","version":"Next","tagName":"h3"},{"title":"updatePipelineJob​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#updatepipelinejob","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature-5","content":"updatePipelineJob( data: PipelineJobUpdateInput! where: PipelineJobWhereUniqueInput! ): PipelineJob  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description-5","content":"Updates the database fields of a specific PipelineJob. Check PipelineJobUpdateInput to see how to do so. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields-5","content":"Field\tType\tDescriptiondata\tPipelineJobUpdateInput!\tPipelineJob partial object that contains field values used to update, specified in the following format [field]: {[action]: [value]}. Check the PipelineJobUpdateInput type for specific fields and their descriptions. where\tPipelineJobWhereUniqueInput!\tConditional values to filter for a specific PipelineJob object. Check the PipelineJobWhereUniqueInput type for specific fields and their descriptions. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example-4","content":"mutation UpdatePipelineJob($jobId: String!, $parameterId: String!) { updatePipelineJob(where: {id: $jobId}, data: { parameters: {disconnect: {id: $parameterId}}, }) { id } }  Variables { &quot;jobId&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;parameterId&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields-6","content":"PipelineJob ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-example-5","content":"{ &quot;data&quot;: { &quot;updatePipelineJob&quot;: { &quot;id&quot;: &quot;9f8f0524-6d67-4996-bd45-8a2434d97c1f&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"updateParameter​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#updateparameter","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature-6","content":"updateParameter( data: ParameterUpdateInput! where: ParameterWhereUniqueInput! ): Parameter  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description-6","content":"Updates the database fields of a specific Parameter. Parameters are objects that represent environment variables within Shakudo resources like Jobs and Sessions. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields-6","content":"Field\tType\tDescriptiondata\tPipelineJobUpdateInput!\tParameter partial object that contains field values used to update, specified in the following format [field]: {[action]: [value]} , where [field] is the name of the field to update, [action] is the update action (e.g., set, increment, decrement, etc.), and [value] is the new value for the field. Check the ParameterUpdateInput type for specific fields and their descriptions. where\tPipelineJobWhereUniqueInput!\tConditional values to filter for a specific PipelineJob object. Check the PipelineJobWhereUniqueInput type for specific fields and their descriptions. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example-5","content":"mutation ($parameterId: String!, $keyValue: String, $valueValue: String) { updateParameter(where: {id: $parameterId}, data: { key: {set: $keyValue} value: {set: $valueValue} }) { id key value } }  ","version":"Next","tagName":"h3"},{"title":"Sample Variables​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-variables-17","content":"{ &quot;parameterId&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;keyValue&quot;: &quot;newKey&quot;, &quot;valueValue&quot;: &quot;newValue&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Type​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-type-18","content":"Parameter ","version":"Next","tagName":"h3"},{"title":"Sample Response​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#sample-response-19","content":"{ &quot;data&quot;: { &quot;updateParameter&quot;: { &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot; &quot;keyValue&quot;: &quot;newKey&quot;, &quot;valueValue&quot;: &quot;newValue&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"createHyperplaneVCServer​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#createhyperplanevcserver","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature-7","content":"createHyperplaneVCServer(data: HyperplaneVCServerCreateInput!): HyperplaneVCServer!  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description-7","content":"Creates a git server connected to a specific git repository to make it accessible on the Shakudo platform. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields-7","content":"Field\tType\tDescriptiondata\tHyperplaneVCServerCreateInput!\tHyperplaneVCServer object that contains field values used to create a git server. Check the HyperplaneVCServerCreateInput type for specific fields and their descriptions. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example-6","content":"mutation($data: HyperplaneVCServerCreateInput!) { createHyperplaneVCServer(data: $data) { id defaultBranch name url } }  Variables { &quot;data&quot;: { &quot;defaultBranch&quot;: &quot;main&quot;, &quot;name&quot;: &quot;examples-graphql-test&quot;, &quot;url&quot;: &quot;git@github.com:org/sample.git&quot; } }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields-7","content":"HyperplaneVCServer ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-example-6","content":"{ &quot;data&quot;: { &quot;createHyperplaneVCServer&quot;: { &quot;id&quot;: &quot;3aff9f7c-c208-44e2-b389-495a11708349&quot;, &quot;defaultBranch&quot;: &quot;main&quot;, &quot;name&quot;: &quot;examples-graphql-test&quot;, &quot;pipelineJobs&quot;: [], &quot;status&quot;: &quot;CREATING&quot;, &quot;url&quot;: &quot;git@github.com:org/sample.git&quot; } } }  ","version":"Next","tagName":"h3"},{"title":"hyperplaneVCServers​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#hyperplanevcservers","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature-8","content":"hyperplaneVCServers( cursor: HyperplaneVCServerWhereUniqueInput, distinct: [HyperplaneVCServerScalarFieldEnum!], orderBy: [HyperplaneVCServerOrderByInput!], skip: Int, take: Int, where: HyperplaneVCServerWhereInput ): [HyperplaneVCServer!]!  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description-8","content":"Retrieves a list of git server instances based on conditions provided, allowing for pagination (cursor and offset-based) and filtering. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields-8","content":"Field\tType\tDescriptioncursor\tHyperplaneVCServerWhereUniqueInput\tStarting git server value to paginate from using cursor-based pagination. The current result starts from this git server record. distinct\t[HyperplaneVCServerScalarFieldEnum!]\tList of fields where their values will remain distinct per record. orderBy\t[HyperplaneVCServerOrderByInput!]\tList of fields that will be used to order the results. The ordering precedence is determined by the location in the list. skip\tInt\tThe number of records to skip from the original result. take\tInt\tThe maximum number of records to show in the result. where\tHyperplaneVCServerWhereInput\tConditional values to filter for a specific HyperplaneVCServer object. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example-7","content":" query ($name: String!) { hyperplaneVCServers(where: { name: {equals: $name } }){ id defaultBranch name pipelineJobs { id } status url serviceUrl } }  Variables { &quot;name&quot;: &quot;examples-graphql-test&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields-8","content":"Array of HyperplaneVCServer ","version":"Next","tagName":"h3"},{"title":"Response Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-example-7","content":"{ &quot;data&quot;: { &quot;hyperplaneVCServers&quot;: [ { &quot;id&quot;: &quot;65e3a289-1371-4009-9fb3-c03bfbcbebd8&quot;, &quot;defaultBranch&quot;: &quot;main&quot;, &quot;name&quot;: &quot;examples-graphql-test&quot;, &quot;pipelineJobs&quot;: [], &quot;status&quot;: &quot;CREATED&quot;, &quot;url&quot;: &quot;git@github.com:org/sample.git&quot;, &quot;serviceUrl&quot;: &quot;sample-service-url.namespace.svc.cluster.local&quot; } ] } }  ","version":"Next","tagName":"h3"},{"title":"getHyperhubSessionDefaultPodSpec​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#gethyperhubsessiondefaultpodspec","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature-9","content":"getHyperhubSessionPodSpec( imageUrl: String = &quot;&quot; userPvcName: String = &quot;&quot; userEmail: String = &quot;&quot; imageType: String = &quot;&quot; ): JSON  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description-9","content":"Retrieves full PodSpec as JSON string for Sessions, which can be used and customized with granularity in createHyperHubSession by itself, instead of relying on creating a PodSpec object using createPodSpec, which has more limited options. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields-9","content":"Field\tType\tDescriptionimageUrl\tString\tURL of custom image, same as imageHash userPvcName\tString\tPersistent volume name as found in Kubernetes. Typically includes the drive name found on the dashboard. Default: empty string, which corresponds with default drive claim-{user-email}. userEmail\tString\tShakudo platform user account email imageType\tString\tName of Shakudo platform Podspec/Image Note: userEmail is required in Request example to identify the respective user, but is not actually required to use query. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example-8","content":"query GetHyperhubSessionPodSpec($imageType: String, $userPvcName: String = &quot;&quot;, $userEmail: String!, $imageUrl: String) { getHyperhubSessionPodSpec( imageType: $imageType, userPvcName: $userPvcName, userEmail: $userEmail, imageUrl: $imageUrl ) }  Variables { &quot;imageType&quot;: &quot;basic&quot;, &quot;userEmail&quot;: &quot;demo@shakudo.io&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields-9","content":"JSON ","version":"Next","tagName":"h3"},{"title":"getPipelineJobPodSpec​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#getpipelinejobpodspec","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature-10","content":"getPipelineJobPodSpec( parameters: ParametersInput gitServerName: String = &quot;&quot; noGitInit: Boolean = false imageUrl: String = &quot;&quot; userEmail: String = &quot;&quot; noHyperplaneCommands: Boolean = false commitId: String = &quot;&quot; branchName: String = &quot;&quot; pipelineYamlPath: String = &quot;&quot; debuggable: Boolean = false jobType: String = &quot;&quot; ): JSON  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description-10","content":"Retrieves full PodSpec as JSON string for PipelineJobs, which can be used and customized with granularity in createPipelineJob by itself, instead of relying on creating a PodSpec object using createPodSpec, which has more limited options. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields-10","content":"Field\tType\tDescriptionparameters\tParametersInput\tList of key-value parameters that are injected into the Job environment and can be used as environment variables gitServerName\tString (&quot;&quot; if not provided)\tGit Server name, corresponds with HyperplaneVCServer.name, which is the display name assigned on the dashboard noGitInit\tBoolean (false if not provided)\tFalse if git server is to be set up using default Shakudo platform workflow. Default: false imageUrl\tString (&quot;&quot; if not provided)\tIf the image is custom, then the image URL can be provided userEmail\tString! (required)\tShakudo platform user account email noHyperplaneCommands\tBoolean\tFalse if using default Shakudo platform commands on job creation. Required to use Shakudo platform jobs through the pipeline YAML, but not required if the image has its own setup. Default: false commitId\tString (&quot;&quot; if not provided)\tThe commit ID with the versions of the pipeline YAML file and pipeline scripts wanted. Ensure that both are present if the commit ID is used. If left empty, assume that the latest commit on the branch is used branchName\tString\tThe name of the specific git branch that contains the pipeline YAML file and pipeline scripts. If commitID is not specified, the latest commit is used. If not specified, default branch is used. pipelineYamlPath\tString (&quot;&quot; if not provided)\tThe relative path to the .yaml file used to run this pipeline job debuggable\tBoolean (false if not provided)\tWhether to enable SSH-based debugging for the job, check the following tutorial for more details jobType\tString (&quot;&quot; if not provided)\tName of Shakudo platform Podspec/Image, default or custom ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example-9","content":"query GetPipelineJobPodSpec( $parameters: ParametersInput $gitServerName: String = &quot;&quot; $noGitInit: Boolean = false $imageUrl: String = &quot;&quot; $userEmail: String! $noHyperplaneCommands: Boolean = false $commitId: String = &quot;&quot; $branchName: String! $pipelineYamlPath: String! $debuggable: Boolean = false $jobType: String! ) { getPipelineJobPodSpec( parameters: $parameters gitServerName: $gitServerName noGitInit: $noGitInit imageUrl: $imageUrl userEmail: $userEmail noHyperplaneCommands: $noHyperplaneCommands commitId: $commitId branchName: $branchName pipelineYamlPath: $pipelineYamlPath debuggable: $debuggable jobType: $jobType ) }  Variables { &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;main&quot;, &quot;pipelineYamlPath&quot;: &quot;example/pipeline.yaml&quot;, &quot;jobType&quot;: &quot;basic&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields-10","content":"JSON ","version":"Next","tagName":"h3"},{"title":"getUserServicePodSpec​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#getuserservicepodspec","content":"","version":"Next","tagName":"h2"},{"title":"Signature​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#signature-11","content":"getUserServicePodSpec( exposedPort: String = &quot;8787&quot; parameters: ParametersInput gitServerName: String = &quot;&quot; noGitInit: Boolean = false imageUrl: String = &quot;&quot; userEmail: String = &quot;&quot; noHyperplaneCommands: Boolean = false commitId: String = &quot;&quot; branchName: String = &quot;&quot; pipelineYamlPath: String = &quot;&quot; jobType: String = &quot;&quot; ): JSON  ","version":"Next","tagName":"h3"},{"title":"Function Description​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#function-description-11","content":"Retrieves full PodSpec as JSON string for Services, which can be used and customized with granularity in createPipelineJob by itself, instead of relying on the parameters found in createPodSpec. ","version":"Next","tagName":"h3"},{"title":"Input Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#input-object-fields-11","content":"Field\tType\tDescriptionexposedPort\tString\tThe exposed port for the job. Default value is 8787. parameters\tParametersInput\tList of key-value parameters that are injected into the job environment and can be used as environment variables. gitServerName\tString\tThe name of the Git server. It corresponds with the display name assigned on the HyperplaneVCServer dashboard. Default value is an empty string. noGitInit\tBoolean\tSpecifies whether to set up the Git server using the default Shakudo platform workflow. Default value is false. imageUrl\tString\tThe URL of a custom image. If the image is custom, this field can be provided. Default: &quot;&quot; (empty string) userEmail\tString\tThe email of the Shakudo platform user account. This field is required. noHyperplaneCommands\tBoolean\tSpecifies whether to use default Shakudo platform commands on job creation. It is required to use Shakudo platform jobs through the pipeline YAML, but not required if the image has its own setup. Default value is false. commitId\tString\tThe commit ID with the versions of the pipeline YAML file and pipeline scripts wanted. Ensure that both are present if commit ID is used. If left empty, assume that the latest commit on the branch is used. Default value is an empty string. branchName\tString\tThe name of the specific Git branch that contains the pipeline YAML file and pipeline scripts. If commitId is not specified, the latest commit is used. This field is required. pipelineYamlPath\tString\tThe relative path to the .yaml file used to run this pipeline job. This field is required. jobType\tString\tThe name of the Shakudo platform Podspec/Image. If the empty string is provided, the Podspec used will be basic. ","version":"Next","tagName":"h3"},{"title":"Request Example​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#request-example-10","content":"query GetUserServicePodSpec( $exposedPort: String! = &quot;8787&quot; $parameters: ParametersInput $gitServerName: String $noGitInit: Boolean $imageUrl: String $userEmail: String! $noHyperplaneCommands: Boolean $commitId: String $branchName: String! $pipelineYamlPath: String! $jobType: String! ) { getUserServicePodSpec( exposedPort: $exposedPort parameters: $parameters gitServerName: $gitServerName noGitInit: $noGitInit imageUrl: $imageUrl userEmail: $userEmail noHyperplaneCommands: $noHyperplaneCommands commitId: $commitId branchName: $branchName pipelineYamlPath: $pipelineYamlPath jobType: $jobType ) }  Variables { &quot;userEmail&quot;: &quot;demo@shakudo.io&quot;, &quot;branchName&quot;: &quot;main&quot;, &quot;pipelineYamlPath&quot;: &quot;example/pipeline.yaml&quot;, &quot;jobType&quot;: &quot;basic&quot; }  ","version":"Next","tagName":"h3"},{"title":"Response Object Fields​","type":1,"pageTitle":"GraphQL","url":"/shakudo-platform-core/graphql#response-object-fields-11","content":"JSON ","version":"Next","tagName":"h3"},{"title":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","type":0,"sectionRef":"#","url":"/tutorials/confluenceapp","content":"","keywords":"","version":"Next"},{"title":"The problem:​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#the-problem","content":"ChatGPT's human-like capability to extract information from vast data has truly transformed the field of language models. But with a 4096-token context limit, extracting details from extensive text documents is still a challenge. There are multiple ways to get around this problem. Option one involves generating text snippets and sequentially prompting the large language model (LLM), refining the answer step by step. Although this method covers the text effectively, it falls short when it comes to time and cost efficiency due to its resource-intensive nature. Option two involves utilizing LLMs with larger context windows, such as the Claude model by Anthropic, offering a 100k-token window. However, it partially solves the problem as we need to ensure that the model can accurately and comprehensively extract from our extensive knowledge base. Option three capitalizes on the power of embeddings and similarity search and is the one we chose for the tutorial. It maintains an embedding vector store for each text snippet, calculates question embeddings, and retrieves the nearest text snippets via a similarity search on embedding. The retrieved text snippets are used to query the LLM with by constructing a prompt to obtain an answer ","version":"Next","tagName":"h2"},{"title":"Solution overview:​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#solution-overview","content":" Our proposed architecture operates as a pipeline that efficiently retrieves information from a knowledge base (in this case, Confluence) in response to user queries. It includes four main steps: Step 1: Knowledge base processing This step involves transforming information from a knowledge base into a more manageable format for subsequent stages. Information is segmented into smaller text snippets and vector representations (embeddings) of these snippets are generated for quick and easy comparison and retrieval. Here, we use Langchain's ConfluenceLoader with TextSplitter and TokenSplitter to efficiently split the documents into text snippets. Then, we create embeddings using OpenAI's ada-v2 model. Step 2: User query processing: When a user submits a question, it is transformed into an embedding using the same process applied to the text snippets. Langchain's RetrievalQA, in conjunction with ChromaDB, then identifies the most relevant text snippets based on their embeddings. Step 3: Answer generation Relevant text snippets, together with the user's question, are used to generate a prompt. This prompt is processed by our chosen LLM to generate an appropriate response to the user's query. Step 4: Streamlit service and Shakudo deployment Finally, we package everything into a Streamlit application, expose the endpoint, and deploy it on a cluster using Shakudo. This step ensures a seamless transition from development to production quickly and reliably, as Shakudo automates DevOps tasks and lets developers use Langchain, Hugging Face pipelines, and LLM models effortlessly with pre-built images. ","version":"Next","tagName":"h2"},{"title":"Setting up the environment​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#setting-up-the-environment","content":"We use OpenAI's adav2 for text embeddings and OpenAI's gpt-3.5-turbo as our LLM. OpenAI offers a range of embedding models and LLMs. The ones that we have chosen balance efficiency and cost-effectiveness, but depending on your needs, other models might be more suitable. ","version":"Next","tagName":"h2"},{"title":"ConfluenceQA initialize​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#confluenceqa-initialize","content":"This stage involves preparing the embedding model for text snippet processing and the LLM model for the final query response. Our go-to models are ada-v2 for embeddings and gpt-3.5-turbo for text generation, respectively. Learn more about embeddings from OpenAI Documentation. embedding = OpenAIEmbeddings()  After that, let’s initialize the LLM model to be used for the final LLM call to query with prompt: llm = ChatOpenAI(model_name=LLM_OPENAI_GPT35, temperature=0.)  The 'temperature' parameter in the LLM initialization impacts the randomness of the model's responses, with higher values producing more random responses and lower values producing more deterministic ones. Here, we've set it to 0, which makes the output entirely deterministic. ","version":"Next","tagName":"h3"},{"title":"OpenAI key setup​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#openai-key-setup","content":"To wrap up the environment setup, we specify the OpenAI API key, a prerequisite for LangChain's functionality. Make sure the API environment key is named OPENAI_API_KEY – it's a requirement for LangChain. import os os.environ[&quot;OPENAI_API_KEY&quot;] =&quot;sk-**&quot;  For security, we store the key in a .env file, and ensure the key is correctly recognized by our application. Never print or share your keys as this can expose them to potential security threats. With our environment set up, we are now ready to start building our Confluence Q&amp;A application.  ","version":"Next","tagName":"h3"},{"title":"Step 1: Creating an Embedding Store from the knowledge base:​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#step-1-creating-an-embedding-store-from-the-knowledge-base","content":"In this step, we will extract the documents from the Confluence knowledge base, transform these documents into text snippets, generate embeddings for these snippets, and store these embeddings in a Chroma store. ","version":"Next","tagName":"h2"},{"title":"Extract the documents with ConfluenceLoader​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#extract-the-documents-with-confluenceloader","content":"ConfluenceLoader is a powerful tool that allows us to extract documents from a Confluence site using login credentials. It currently supports username/api_key and OAuth2 authentication methods. Be careful when handling these credentials, as they are sensitive information. config = {&quot;persist_directory&quot;:&quot;./chroma_db/&quot;, &quot;confluence_url&quot;:&quot;https://templates.atlassian.net/wiki/&quot;, &quot;username&quot;:None, &quot;api_key&quot;:None, &quot;space_key&quot;:&quot;RD&quot; } persist_directory = config.get(&quot;persist_directory&quot;,None) confluence_url = config.get(&quot;confluence_url&quot;,None) username = config.get(&quot;username&quot;,None) api_key = config.get(&quot;api_key&quot;,None) space_key = config.get(&quot;space_key&quot;,None) ## 1. Extract the documents loader = ConfluenceLoader( url=confluence_url, username = username, api_key= api_key ) documents = loader.load( space_key=space_key, limit=100 )  ","version":"Next","tagName":"h3"},{"title":"Splitting Documents into Text Snippets:​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#splitting-documents-into-text-snippets","content":"Next, we split these documents into smaller, manageable text snippets. We employ CharacterTextSplitter and TokenTextSplitter from LangChain for this task. text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0) texts = text_splitter.split_documents(documents) text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name=&quot;cl100k_base&quot;) # This the encoding for text-embedding-ada-002 texts = text_splitter.split_documents(texts)  ","version":"Next","tagName":"h3"},{"title":"Generating Embeddings and Adding to Chroma Store:​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#generating-embeddings-and-adding-to-chroma-store","content":"Lastly, we generate embeddings for these text snippets and store them in a Chroma database. The Chroma class handles this with the help of an embedding function. if persist_directory and os.path.exists(persist_directory): vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding) else: vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)  We have now successfully transformed our knowledge base into a store of embedded text snippets, ready for efficient querying in the subsequent stages of our pipeline. For a dynamic confluence pages, the vector store creation process can be scheduled with the help of Shakudo jobs pipeline (Link Shakudo Jobs pipeline documentation here)  ","version":"Next","tagName":"h3"},{"title":"Step 2: Computing questions embeddings and finding relevant snippets​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#step-2-computing-questions-embeddings-and-finding-relevant-snippets","content":"ChromaDB is an advanced indexing system that accelerates retrieval by finding and matching things that have the same meaning. This makes our process quicker and more accurate. Next, we create &quot;questions embeddings&quot; to understand the meaning behind the questions. This is like converting the questions into a language that ChromaDB speaks fluently. Now, ChromaDB can pinpoint the most useful snippets of information. These snippets form the foundation of smart and detailed responses in our app. These steps enhance the effectiveness of our RetrievalQA chain. This ensures that our app delivers fast, accurate, and useful answers to the questions received. In the next step, we will show you how it works along with prompt engineering.  ","version":"Next","tagName":"h2"},{"title":"Step 3: Prompt engineering and LLM query​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#step-3-prompt-engineering-and-llm-query","content":"In this step, we construct a prompt for our LLM. A prompt is a message that sets the context and asks the question that we want the LLM to answer. To pass a custom prompt with context and question, you can define your own template as follows: custom_prompt_template = &quot;&quot;&quot;You are a Confluence chatbot answering questions. Use the following pieces of context to answer the question at the end. If you don't know the answer, say that you don't know, don't try to make up an answer. {context} Question: {question} Helpful Answer:&quot;&quot;&quot; CUSTOMPROMPT = PromptTemplate( template=custom_prompt_template, input_variables=[&quot;context&quot;, &quot;question&quot;] ) ## Inject custom prompt qa.combine_documents_chain.llm_chain.prompt = CUSTOMPROMPT retriever = vectordb.as_retriever(search_kwargs={&quot;k&quot;:4}) #Top4-Snippets qa = RetrievalQA.from_chain_type(llm=llm, chain_type=&quot;stuff&quot;,retriever=retriever) question = &quot;How to organize content in a space?&quot; answer = qa.run(question) print(answer) # Answer: To organize content in a space, you can create pages or blogs for different types of content. Pages can have child pages, which allows you to organize content into categories and subcategories. You can also use labels to categorize and identify content, and create a table of contents for your space using the Content Report Table Macro. Additionally, you can customize the sidebar to make it easier to navigate through your space and add a search box to find content within your space.  In the following class, ConfluenceQA, we package all the necessary steps that include initializing the models, embedding, and combining the retriever and answer generator into one organized module. This encapsulation improves code readability and reusability.  class ConfluenceQA: def __init__(self,config:dict = {}): self.config = config self.embedding = None self.vectordb = None self.llm = None self.qa = None self.retriever = None ... # You can see the full script on Github  Once the ConfluenceQA class is set up, you can initialize and run it as follows: # Configuration for ConfluenceQA config = {&quot;persist_directory&quot;:&quot;./chroma_db/&quot;, &quot;confluence_url&quot;:&quot;https://templates.atlassian.net/wiki/&quot;, &quot;username&quot;:None, &quot;api_key&quot;:None, &quot;space_key&quot;:&quot;RD&quot;} # Initialize ConfluenceQA confluenceQA = ConfluenceQA(config=config) confluenceQA.init_embeddings() confluenceQA.init_models() # Create Vector DB confluenceQA.vector_db_confluence_docs() # Set up Retrieval QA Chain confluenceQA.retreival_qa_chain() # Query the model question = &quot;How to organize content in a space?&quot; confluenceQA.answer_confluence(question)  Remember that the above approach is a structured way to access the Confluence knowledge base and get your desired information using a combination of embeddings, retrieval, and prompt engineering. However, the success of the approach would largely depend on the quality of the knowledge base and the prompt that is used to question the LLM. ","version":"Next","tagName":"h2"},{"title":"Step 4: Streamlit app​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#step-4-streamlit-app","content":"Let’s wrap our solution in a Streamlit app and deploy it as a service. This will make it accessible either locally or on a cloud-based cluster. ","version":"Next","tagName":"h2"},{"title":"Building a Streamlit App​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#building-a-streamlit-app","content":"To create an interactive web application around our ConfluenceQA class, we use Streamlit, a Python library that simplifies app creation. Below is the breakdown of the code: We start by importing necessary modules and initializing our ConfluenceQA instance: import streamlit as st from confluence_qa import ConfluenceQA st.set_page_config( page_title='Q&amp;A Bot for Confluence Page', page_icon='⚡', layout='wide', initial_sidebar_state='auto', ) st.session_state[&quot;config&quot;] = {} confluence_qa = None  We then define a sidebar form for user inputs: with st.sidebar.form(key ='Form1'): # Form fields and submit button go here  And finally, we provide a user interface for asking questions and getting answers: st.title(&quot;Confluence Q&amp;A Demo&quot;) question = st.text_input('Ask a question', &quot;How do I make a space public?&quot;) if st.button('Get Answer'): # Code to generate and display the answer  This Streamlit app can be launched locally or on a cluster and allows us to interact with our Confluence Q&amp;A system in a user-friendly manner. ","version":"Next","tagName":"h3"},{"title":"Deploying with Shakudo​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#deploying-with-shakudo","content":"Finally our app is ready, and we can deploy it as a service on Shakudo. The platform makes the process of deployment easier, allowing you to quickly put your app online. To deploy our app on Shakudo, we need two key files: pipeline.yaml, which describes our deployment pipeline, and run.sh, a bash script to set up and run our application. Here's what these files look like: ‘pipeline.yaml’: pipeline: name: &quot;QA demo&quot; tasks: - name: &quot;QA app&quot; type: &quot;bash script&quot; port: 8787 bash_script_path: &quot;LLM/confluence_app/run.sh&quot;  ‘run.sh’: PROJECT_DIR=&quot;$(cd -P &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)&quot; &amp;&amp; pwd)&quot; cd &quot;$PROJECT_DIR&quot; pip install -r requirements.txt export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python export STREAMLIT_RUNONSSAVE=True streamlit run app.py --server.port 8787 --browser.serverAddress localhost  In this script: Set the project directory and navigate into it.Install the necessary Python libraries from the requirements.txt file.Set two environment variables to: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION and STREAMLIT_RUNONSSAVE.Runthe Streamlit app on port 8787. After you have these files ready, navigate to the service tab and click the '+' button to create a new service. Fill in the details in the service creation panel and click 'CREATE'. On the services dashboard, click on the ‘Endpoint URL’ button to access the application. Check out the Shakudo docs for a more detailed explanation.  Now, your application is live! You can browse through the user interface to see how it works.  ","version":"Next","tagName":"h3"},{"title":"Adopting Shakudo into your workflow​","type":1,"pageTitle":"Confluence Q&A App on Shakudo with Langchain and ChatGPT","url":"/tutorials/confluenceapp#adopting-shakudo-into-your-workflow","content":"Shakudo is designed to facilitate the entire lifecycle of data and AI applications. Automating all stages of the development process, including the stack integrations, deployment, and the ongoing management of data-driven applications.  By using Shakudo, you’ll take advantage of: Integrated Environment: Shakudo is designed to provide compatibility across a diverse range of best-of-breed data tools. This unique feature facilitates experimentation and the creation of a data stack that's not only more reliable and performant but also cost-effective.Continuous Integration and Deployment (CI/CD): Shakudo automates the stages of application deployment with managed Kubernetes. This ensures rapid deployment and eliminates the complex task of self-managing a cluster, resulting in more reliable releases and data flows.Services Deployment and Pipelines Orchestration: With Shakudo you can deploy your app as a service that shows an endpoint, be it a dashboard, a website, or an API endpoint. It also facilitates the creation and orchestration of data pipelines, reducing setup time to only a few minutes. Conclusion This blog provides a comprehensive guide to developing a Confluence Q&amp;A application utilizing the power of Shakudo, Langchain, and ChatGPT, aiming to resolve the challenge posed by ChatGPT's token limit when extracting information from extensive text documents. We used a new method leveraging embeddings and similarity search, ensuring a more efficient process of retrieving accurate information from a knowledge base. Are you ready to start building more efficient data-driven applications? Check out our blog post on “How to Easily and Securely Integrate LLMs for Enterprise Data Initiatives”. Feel free to reach out to our team to check out how you can start using Shakudo to help your business. Thank you for reading and happy coding! References: This code is adapted based on the work in LLM-WikipediaQA, where the author compares FastChat-T5, Flan-T5 with ChatGPT running a Q&amp;A on Wikipedia Articles.Buster: Overview figure inspired from Buster’s demo. Buster is a QA bot that can be used to answer from any source of documentation.Claude model: 100K Context Window model from Anthropic AI ","version":"Next","tagName":"h2"},{"title":"Create a Flask App","type":0,"sectionRef":"#","url":"/tutorials/flaskapp","content":"","keywords":"","version":"Next"},{"title":"1. Prepare your Flask App​","type":1,"pageTitle":"Create a Flask App","url":"/tutorials/flaskapp#1-prepare-your-flask-app","content":"First and foremost, you need to have a working Flask API program. You can either develop it using Shakudo's Session or on your local machine. We have a simple working version that you could find here Flask Program Then push this code to a repository that are in-sync on Shakudo platform. ","version":"Next","tagName":"h2"},{"title":"2. Set up micro-service on Shakudo dashboard​","type":1,"pageTitle":"Create a Flask App","url":"/tutorials/flaskapp#2-set-up-micro-service-on-shakudo-dashboard","content":"From the sidebar, go to Microservices panel and click to Create Microservices button on the right hand side.  On the General tab, you have to choose the value for the following field: Name: The name of your service Endpoint: How you like the public URL would be. For example if you choose.flask_tut as endpoint, the public URL would be https://staging-aks.canopyhub.io/flask_tut, in which https://staging-aks.canopyhub.io is the url of the platform. Environment Config: This includes hardware requirement and base environment to run the micro-service. In the Pipeline section, you will need to point to the program's entrypoint.  On the Advance tab, choose the git repository in which the code is stored. Apart from that, we need to set up the value for port. In this example, I choose 8000 since this is the port being used by the micro-service in the example.  On the Parameter tab, you can set up some environment variables being used by the app.  Once the configuration is done, click to Create Microservice to set up the service. ","version":"Next","tagName":"h2"},{"title":"3. View micro-service status​","type":1,"pageTitle":"Create a Flask App","url":"/tutorials/flaskapp#3-view-micro-service-status","content":"Once the service is up, you can view its corresponding status or its endpoint on the dashboard. Moreover, we provide options for the user to interact with the microservice if necessary: Turn off: Scale down the service if it's not being userRestart: Redeploy the service with the latest code, in-cluster URL will be the samePublish: Allow other users to see the status of this serviceClone: Create a new service based on the current settings.  ","version":"Next","tagName":"h2"},{"title":"Build and Deploy with Streamlit on Shakudo","type":0,"sectionRef":"#","url":"/tutorials/streamlitapp","content":"","keywords":"","version":"Next"},{"title":"Building the App​","type":1,"pageTitle":"Build and Deploy with Streamlit on Shakudo","url":"/tutorials/streamlitapp#building-the-app","content":"Let's create a small dashboard for demonstration purposes. We will call it dashtest.py. Before we get started, let's create a script called run.sh so we can just execute the script to easily run our streamlit app. Its contents should be as follows: streamlit run streamlit_example/dashtest.py --server.port 8787 --browser.serverAddress localhost  Remember to adjust your paths! When developing on Shakudo, the localhost port will automatically be forwarded when using vscode or codeserver. In vscode, the following notification will pop up to access the app from the local browser:  We will also maintain a requirements.txt file to keep track of our dependencies (in this case, that will be streamlit alone). It is recommended to keep track of the exact version of your dependencies to ease reproducibility, but that is not necessary for this demo. streamlit  Now let's get started with the app proper. import streamlit as st  Let's set the page title and icon in our dashtest.py script: st.set_page_config( page_title=&quot;Shakudo Streamlit Example&quot;, page_icon=&quot;:shark:&quot; )  And run the startup script to make sure everything is working fine. If all goes well, you should be able to access the page on localhost:8787. While the page is blank, the title should look like in this picture:   Next, let's organize a basic layout and play with some simple widgets: st.title(&quot;This is an example Title&quot;) st.subheader(&quot;multiple columns&quot;) col1, col2, col3 = st.columns([2, 4, 5]) with col1: st.button(&quot;Click me&quot;, on_click=lambda: st.balloons()) with col2: st.text(&quot;this is a text place holder&quot;) with col3: st.table([[1, 2, 3], [4, 5, 6]])  Here we created 3 columns spanning 2, 4, and 5 units of space. We then filled the first column with a button, the second with a simple text, and the third with a table. If all went well, we can now run our start script and Streamlit will load the page in the default browser. The result should look like this:   col4, col5 = st.columns([5, 5]) with col4: st.subheader(&quot;json content&quot;) st.json(&quot;&quot;&quot;{ &quot;key&quot; : &quot;value&quot;, &quot;key2&quot; : 123, &quot;somelist&quot; : [1, 2, &quot;3&quot;] }&quot;&quot;&quot;) with col5: st.subheader('charts') st.bar_chart([1, 4, 5, 3, 2, 6])  Streamlit supports many kinds of elements, including json displays and various charts, which are displayed beautifully without any user styling or tweaking.   st.subheader(&quot;Progress bar&quot;) import time progressbar = st.empty() n = 0 with st.expander(&quot;expandable&quot;): st.text(&quot;&quot;&quot;This is some text. &quot;&quot;&quot;* 30) with st.sidebar: st.slider(&quot;slider&quot;, 0, 100, 50) st.select_slider(&quot;select slider&quot;, list(range(10))) st.selectbox('select box', ['apple', 'banana', 'pear']) st.checkbox('check box 1') st.checkbox('check box 2') st.checkbox('check box 3') st.checkbox('check box 4') st.time_input(&quot;time&quot;) st.date_input(&quot;Date&quot;) st.text_input(&quot;text&quot;) while 1: n += 1 with progressbar: st.progress(n) time.sleep(1) if n == 100: n = 0  Here we show how streamlit allows us to easily interact with controls such as progress bars: it's as simple as a st.progress after &quot;entering&quot; the progressbar in a with block. Streamlit also allows quickly creating common UI elements like sidebars and supports various user inputs, like dates, times, dropdown selectors, and more. The final result is shown below.   ","version":"Next","tagName":"h2"},{"title":"Deploying on Shakudo​","type":1,"pageTitle":"Build and Deploy with Streamlit on Shakudo","url":"/tutorials/streamlitapp#deploying-on-shakudo","content":"For long-running tasks, Shakudo provides Services, which are defined with a simple yaml file that described the set of steps to run. For our deployment, all we need is the following pipeline definition: pipeline: name: &quot;streamlit&quot; requirements: &quot;streamlit_example/requirements.txt&quot; tasks: - name: &quot;dashbaord example&quot; type: &quot;bash script&quot; port: 8787 bash_script_path: &quot;streamlit_example/run.sh&quot;  That's it! Now we can create a service, specify the repository where we pushed the code and yaml, the path from the repository's root, and run the service. Our app is now available on port 8787 at the endpoint we selected during service configuration on Shakudo. ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}